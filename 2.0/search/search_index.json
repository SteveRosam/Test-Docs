{"config":{"lang":["en"],"separator":"[\\s\\-\\.]","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Quix Documentation","text":"<p>Welcome to the Quix documentation! v2.0</p>"},{"location":"#get-started","title":"Get started","text":"<p>If you're new to Quix, here are some resources to help get you started quickly.</p> <ul> <li> <p>What is Quix?</p> <p>New to Quix? Find out more.</p> <p> What is Quix?</p> </li> <li> <p>Definitions</p> <p>Learn about common terms and definitions.</p> <p> Definitions</p> </li> <li> <p>Quickstart</p> <p>Start working with Quix Platform, the simple-to-use GUI for building real-time streaming applications.</p> <p> Quickstart</p> </li> <li> <p>Help</p> <p>If you need any help, please sign up to The Stream community, our free public Slack channel.</p> <p> Join The Stream community</p> </li> </ul>"},{"location":"#tutorials","title":"Tutorials","text":"<p>By following these tutorials, you can learn how to build data-driven apps, and integrate Quix with external systems.</p> <ul> <li> <p>Sentiment Analysis</p> <p>Stream data from Twitter and build a Sentiment analysis pipeline.</p> <p> Sentiment Analysis</p> </li> <li> <p>Image Processing</p> <p>Real time image processing using London's 'Jam Cams'.</p> <p> Image Processing</p> </li> </ul>"},{"location":"#core-resources","title":"Core resources","text":"<p>Read more about the Quix Streams Client Library and APIs.</p> <ul> <li> <p>Connect to Quix - Client Library</p> <p>Discover how to connect Quix and your application using Quix Streams.</p> <p> Learn more</p> </li> <li> <p>Publish data - Client Library</p> <p>Read how to publish real-time data to Kafka topics using Quix Streams.</p> <p> Learn more</p> </li> <li> <p>Subscribe to data - Client Library</p> <p>Learn how to subscribe to real-time data in your application using Quix Streams.</p> <p> Learn more</p> </li> <li> <p>Streaming Writer API</p> <p>Stream data to Quix Kafka topics via HTTP with this API.</p> <p> Learn more</p> </li> <li> <p>Streaming Reader API</p> <p>Work with this API to receive live data in your Web applications from Quix Kafka topics via Websockets.</p> <p> Learn more</p> </li> <li> <p>Data Catalogue API</p> <p>Query historic time-series data in Quix using HTTP interface.</p> <p> Learn more</p> </li> </ul>"},{"location":"#contribute","title":"Contribute","text":"<ul> <li> <p>Contribute</p> <p>We welcome contributions to the docs.</p> <p> Contribution guide</p> </li> </ul>"},{"location":"client-library-intro/","title":"Client library intro","text":"<p>This file is replaced with the Quix Streams README.md file during the build</p>"},{"location":"apis/","title":"Quix APIs","text":"<p>The Quix Platform provides the following APIs:</p>"},{"location":"apis/#data-catalogue","title":"Data Catalogue","text":"<p>The Data Catalogue HTTP API allows you to fetch data stored in the Quix platform. You can use it for exploring the platform, prototyping applications, or working with stored data in any language with HTTP capabilities.</p>"},{"location":"apis/#streaming-writer","title":"Streaming Writer","text":"<p>The Streaming Writer API allows you to stream data into the Quix platform via HTTP. It\u2019s an alternative to using our C# and Python client libraries. You can use the Streaming Writer API from any HTTP-capable language.</p>"},{"location":"apis/#streaming-reader","title":"Streaming Reader","text":"<p>As an alternative to the client library, the Quix platform supports real-time data streaming over WebSockets, via the Streaming Reader API. Clients can receive updates on data and definitions for parameters and events, as they happen. The examples use the Microsoft SignalR JavaScript client library.</p>"},{"location":"apis/#portal-api","title":"Portal API","text":"<p>The Portal API gives access to the Portal interface allowing you to automate access to data including Users, Workspaces, and Projects.</p>"},{"location":"apis/portal-api/","title":"Portal API","text":"<p>Portal API gives access to the Portal interface allowing you to automate access to data including Users, Workspaces, and Projects.</p> <p>Refer to Portal API Swagger for more information.</p>"},{"location":"apis/data-catalogue-api/aggregate-tags/","title":"Aggregate data by tags","text":"<p>If you need to compare data across different values for a given tag, you\u2019ll want to group results by that tag. You can do so via the <code>/parameters/data</code> endpoint.</p>"},{"location":"apis/data-catalogue-api/aggregate-tags/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up.</p> </li> <li> <p>Get a Personal Access Token     to authenticate each request.</p> </li> </ul>"},{"location":"apis/data-catalogue-api/aggregate-tags/#using-the-groupby-property","title":"Using the groupBy property","text":"<p>You can supply a list of Tags in the <code>groupBy</code> array to aggregate results by. For example, you could group a set of Speed readings by the LapNumber they occurred on using something like:</p> <pre><code>{\n\"from\": 1612191286000000000,\n\"to\":   1612191386000000000,\n\"numericParameters\": [{\n\"parameterName\": \"Speed\"\n}],\n\"groupBy\": [ \"LapNumber\" ]\n}\n</code></pre> <p>With these settings alone, we\u2019ll get the <code>LapNumber</code> tag included in our results, alongside the existing timestamps and requested parameters, e.g.</p> <pre><code>{\n\"timestamps\": [\n1612191286000000000,\n1612191287000000000,\n...\n],\n\"numericValues\": {\n\"Speed\": [\n307.8333333333333,\n313.8421052631579,\n...\n]\n},\n\"tagValues\": {\n\"LapNumber\": [\n\"3.0\",\n\"4.0\",\n...\n]\n}\n}\n</code></pre>"},{"location":"apis/data-catalogue-api/aggregate-tags/#using-aggregationtype","title":"Using aggregationType","text":"<p>For meaningful aggregations, you should specify a type of aggregation function for each parameter. When specifying the parameters to receive, include the <code>aggregationType</code> in each parameter object like so:</p> <pre><code>\"numericParameters\": [{\n\"parameterName\": \"Speed\",\n\"aggregationType\": \"mean\"\n}]\n</code></pre> <p>Ten standard aggregation functions are provided including <code>max</code>, <code>count</code>, and <code>spread</code>. When you group by a tag and specify how to aggregate parameter values, the result will represent that aggregation. For example, the following results demonstrate the average speed that was recorded against each lap:</p> <pre><code>{\n\"timestamps\": [\n1612191286000000000,\n1612191286000000000\n],\n\"numericValues\": {\n\"mean(Speed)\": [\n213.36765142704252,\n173.77710595934278\n]\n},\n\"stringValues\": {},\n\"binaryValues\": {},\n\"tagValues\": {\n\"LapNumber\": [\n\"3.0\",\n\"4.0\"\n]\n}\n}\n</code></pre>"},{"location":"apis/data-catalogue-api/aggregate-time/","title":"Aggregate data by time","text":"<p>You can downsample and upsample data from the catalogue using the <code>/parameters/data</code> endpoint.</p>"},{"location":"apis/data-catalogue-api/aggregate-time/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up.</p> </li> <li> <p>Get a Personal Access Token     to authenticate each request.</p> </li> </ul>"},{"location":"apis/data-catalogue-api/aggregate-time/#aggregating-and-interpolating","title":"Aggregating and interpolating","text":"<p>The JSON payload can include a <code>groupByTime</code> property, an object with the following members:</p> <ul> <li> <p><code>timeBucketDuration</code>     The duration, in nanoseconds, for one aggregated value.</p> </li> <li> <p><code>interpolationType</code>     Specify how additional values should be generated when     interpolating.</p> </li> </ul> <p>For example, imagine you have a set of speed data, with values recorded at 1-second intervals. You can group such data into 2-second intervals, aggregated by mean average, with the following:</p> <pre><code>{\n\"groupByTime\": {\n\"timeBucketDuration\": 2000000000,\n},\n\"numericParameters\": [{\n\"parameterName\": \"Speed\",\n\"aggregationType\": \"Mean\"\n}]\n}\n</code></pre> <p>You can specify an <code>interpolationType</code> to define how any missing values are generated. <code>Linear</code> will provide a value in linear proportion, whilst <code>Previous</code> will repeat the value before the one that was missing.</p> <pre><code>{\n\"from\": 1612191286000000000,\n\"to\":   1612191295000000000,\n\"numericParameters\": [{\n\"parameterName\": \"Speed\",\n\"aggregationType\": \"First\"\n}],\n\"groupByTime\": {\n\"timeBucketDuration\": 2000000000,\n\"interpolationType\": \"None\"\n},\n\"streamIds\": [ \"302b1de3-2338-43cb-8148-3f0d6e8c0b8a\" ]\n}\n</code></pre>"},{"location":"apis/data-catalogue-api/authenticate/","title":"Authenticate","text":""},{"location":"apis/data-catalogue-api/authenticate/#before-you-begin","title":"Before you begin","text":"<ul> <li>Sign up on the Quix Portal</li> </ul>"},{"location":"apis/data-catalogue-api/authenticate/#get-a-personal-access-token","title":"Get a Personal Access Token","text":"<p>You should authenticate requests to the Catalogue API using a Personal Access Token (PAT). This is a time-limited token which you can revoke if necessary.</p> <p>Follow these steps to generate a PAT:</p> <ol> <li> <p>Click the user icon in the top-right of the Portal and select the     Tokens menu.</p> </li> <li> <p>Click GENERATE TOKEN.</p> </li> <li> <p>Choose a name to describe the token\u2019s purpose, and an expiration     date, then click CREATE.</p> </li> <li> <p>Copy the token and store it in a secure place.</p> </li> </ol> <p>Warning</p> <p>You won\u2019t be able to retrieve your token from the Portal once you\u2019ve created it, so make sure to take a copy.</p> <p>Warning</p> <p>Treat your tokens like passwords and keep them secret. When working with the API, use tokens as environment variables instead of hardcoding them into your programs.</p>"},{"location":"apis/data-catalogue-api/authenticate/#sign-all-your-requests-using-this-token","title":"Sign all your requests using this token","text":"<p>Make sure you accompany each request to the API with an <code>Authorization</code> header using your PAT as a bearer token, as follows:</p> <pre><code>Authorization: bearer &lt;token&gt;\n</code></pre> <p>Replace <code>&lt;token&gt;</code> with your Personal Access Token. For example, if you\u2019re using curl on the command line, you can set the header using the <code>-H</code> flag:</p> <pre><code>curl -H \"Authorization: bearer &lt;token&gt;\" ...\n</code></pre> <p>Warning</p> <p>If you fail to send a valid Authorization header, the API will respond with a <code>401 UNAUTHORIZED</code> status code.</p>"},{"location":"apis/data-catalogue-api/filter-tags/","title":"Tag filtering","text":"<p>If you supply Tags with your parameter data, they will act as indexes, so they can be used to efficiently filter data.</p>"},{"location":"apis/data-catalogue-api/filter-tags/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up.</p> </li> <li> <p>Get a Personal Access Token     to authenticate each request.</p> </li> </ul>"},{"location":"apis/data-catalogue-api/filter-tags/#using-tag-filters","title":"Using tag filters","text":"<p>When calling the <code>/parameters/data</code> endpoint, you can include a <code>tagFilters</code> property in your payload. This property references an array of objects, each with the following structure:</p> <ul> <li> <p>tag     The name of the tag to filter by</p> </li> <li> <p>operator     A comparison operator</p> </li> <li> <p>value     The value to compare against</p> </li> </ul> <p>For example, to fetch only the data recorded on the second lap, we can filter on a <code>LapNumber</code> tag as follows:</p> <pre><code>{\n\"tagFilters\": [{\n\"tag\": \"LapNumber\",\n\"operator\": \"Equal\",\n\"value\": \"2.0\"\n}]\n}\n</code></pre> <p>Note that the value can also be an array, in which case data that matches the chosen operator for any value is returned:</p> <pre><code>{\n\"tagFilters\": [{\n\"tag\": \"LapNumber\",\n\"operator\": \"Equal\",\n\"value\": [ \"2.0\", \"4.0\" ]\n}]\n}\n</code></pre> <p>But also note that multiple filters for the same tag apply in combination, so:</p> <pre><code>{\n\"tagFilters\": [{\n\"tag\": \"LapNumber\",\n\"operator\": \"Equal\",\n\"value\": \"2.0\"\n},{\n\"tag\": \"LapNumber\",\n\"operator\": \"Equal\",\n\"value\": \"4.0\"\n}]\n}\n</code></pre> <p>Is useless because a LapNumber cannot be both \"2.0\" and \"4.0\".</p>"},{"location":"apis/data-catalogue-api/filter-tags/#supported-operators","title":"Supported operators","text":"<p>Each object in the <code>tagFilters</code> array can support the following <code>operator</code> values:</p> <ul> <li> <p>Equal</p> </li> <li> <p>NotEqual</p> </li> <li> <p>Like</p> </li> <li> <p>NotLike</p> </li> </ul> <p><code>Equal</code> and <code>NotEqual</code> test for true/false exact string matches.</p> <p><code>Like</code> and <code>NotLike</code> will perform a regular expression match, so you can search by pattern. For example, to get the Speed parameter values tagged with a LapNumber which is either 2 or 4, you can use the expression \"^[24]\\.\" to match values 2.0 and 4.0:</p> <pre><code>curl \"https://telemetry-query-testing-quickstart.platform.quix.ai/parameters/data\" \\\n-H \"Authorization: bearer ${token}\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n         \"tagFilters\": [{\n             \"tag\": \"LapNumber\",\n             \"operator\": \"Like\",\n             \"value\": \"^[24]\\\\.\"\n         }],\n         \"numericParameters\": [{\"parameterName\": \"Speed\"}],\n         \"from\": 1612191182000000000,\n         \"to\": 1612191189000000000\n     }'\n</code></pre>"},{"location":"apis/data-catalogue-api/get-swagger/","title":"Getting the Swagger documentation URL","text":"<p>You can access Swagger documentation and then use it to try out the Data Catalogue API. </p> <p>The URL is workspace-specific, and follows this pattern:</p> <pre><code>https://telemetry-query-${organization}-${workspace}.platform.quix.ai/swagger\n</code></pre> <p>The workspace ID is a combination based on your organization and workspace names. For example, for an <code>acme</code> organization with a <code>weather</code> workspace, the URL would have the following format:</p> <pre><code>https://telemetry-query-acme-weather.platform.quix.ai/swagger\n</code></pre> <p>To help determine the URL, you can find out how to get your workspace id.</p> <p>Tip</p> <p>Once you access the Swagger documentation, you can select the version of the API you require from the <code>Select a definition</code> dropdown list.</p>"},{"location":"apis/data-catalogue-api/intro/","title":"Introduction","text":"<p>The Data Catalogue HTTP API allows you to fetch data stored in the Quix platform. You can use it for exploring the platform, prototyping applications, or working with stored data in any language with HTTP capabilities.</p> <p>The API is fully described in our Swagger documentation. Read on for a guide to using the API, including real-world examples you can execute from your language of choice, or via the command line using <code>curl</code>.</p>"},{"location":"apis/data-catalogue-api/intro/#preparation","title":"Preparation","text":"<p>Before using any of the endpoints, you\u2019ll need to know how to authenticate your requests and how to form a typical request to the API.</p> <p>You\u2019ll also need to have some data stored in the Quix platform for API use to be meaningful. You can use any Source of our Quix Library to do this using the Quix portal.</p>"},{"location":"apis/data-catalogue-api/intro/#topics-covered","title":"Topics covered","text":"Topic Endpoint Examples Streams, paged <code>/streams</code> Get all streams in groups of ten per page Streams, filtered <code>/streams</code> Get a single stream, by ID Get only the streams with LapNumber data Streams &amp; models <code>/streams/models</code> Get stream hierarchy Raw data <code>/parameters/data</code> Get all the <code>Speed</code> readings Get <code>Speed</code> data between timestamps Aggregated data by time <code>/parameters/data</code> Downsample or upsample data Aggregated by tags <code>/parameters/data</code> Show average Speed by LapNumber Tag filtering <code>/parameters/data</code> Get data for just one Lap"},{"location":"apis/data-catalogue-api/raw-data/","title":"Raw data","text":"<p>Access persisted raw data by specifyng the parameters you\u2019re interested in. Add restrictions based on Stream or timings for finer-grained results.</p>"},{"location":"apis/data-catalogue-api/raw-data/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up.</p> </li> <li> <p>Get a Personal Access Token     to authenticate each request.</p> </li> </ul>"},{"location":"apis/data-catalogue-api/raw-data/#using-the-parametersdata-endpoint","title":"Using the /parameters/data endpoint","text":"<p>Raw telemetry data is available via the <code>/parameters/data</code> endpoint.</p>"},{"location":"apis/data-catalogue-api/raw-data/#fetching-specific-parameters","title":"Fetching specific parameters","text":""},{"location":"apis/data-catalogue-api/raw-data/#request","title":"Request","text":"<p>You can filter by a number of different factors but, at minimum, you\u2019ll need to supply one or more parameters to fetch:</p> <pre><code>{\n\"numericParameters\": [{\n\"parameterName\": \"Speed\"\n}]\n}\n</code></pre> <p>In this example, we\u2019re requesting a single numeric parameter, <code>Speed</code>. Each array of parameters is indexed based on parameter type, which can be <code>numericParameters</code>, <code>stringParameters</code> or <code>binaryParameters</code>. Parameters are returned in a union, so if you request several, you\u2019ll get back all parameters that match.</p>"},{"location":"apis/data-catalogue-api/raw-data/#example","title":"Example","text":"<pre><code>curl \"https://${domain}.platform.quix.ai/parameters/data\" \\\n-H \"accept: text/plain\" \\\n-H \"Authorization: bearer &lt;token&gt;\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"numericParameters\":[{\"parameterName\":\"Speed\"}]}'\n</code></pre> <p>If you just had a single parameter value in the catalogue, the response from the above call might look something like this:</p> <pre><code>{\n\"timestamps\": [\n1612191100000000000\n],\n\"numericValues\": {\n\"Speed\": [\n104.22222222222224\n]\n},\n\"stringValues\": {},\n\"binaryValues\": {},\n\"tagValues\": {},\n}\n</code></pre>"},{"location":"apis/data-catalogue-api/raw-data/#restricting-by-stream-or-time","title":"Restricting by Stream or time","text":"<p>In reality, you\u2019ll have far more data in the catalogue, so you\u2019ll want to filter it. Three remaining properties of the request object allow you to do so:</p> <ul> <li> <p><code>streamIds</code></p> </li> <li> <p><code>from</code></p> </li> <li> <p><code>to</code></p> </li> </ul> <p>Each stream you create has a unique ID. You can view the ID of a persisted via the Data section of the Quix Portal. Supply a list of stream IDs to restrict fetched data to just those streams:</p> <pre><code>{\n\"streamIds\": [\n\"302b1de3-2338-43cb-8148-3f0d6e8c0b8a\",\n\"9feb07ac-b0b2-4591-bc7f-8f0c1295ed7c\"\n]\n}\n</code></pre> <p>You can also restrict data to a certain time span using the <code>from</code> and <code>to</code> properties. These each expect a timestamp in nanoseconds, for example:</p> <pre><code>{\n\"from\": 1612191286000000000,\n\"to\":   1612191386000000000\n}\n</code></pre> <p>These timestamps cover a range of 100 seconds.</p>"},{"location":"apis/data-catalogue-api/request/","title":"Forming a request","text":"<p>How you send requests to the Data Catalogue API will vary depending on the client or language you\u2019re using. But the API still has behavior and expectations that is common across all clients.</p> <p>Tip</p> <p>The examples in this section show how to use the popular <code>curl</code> command line tool.</p>"},{"location":"apis/data-catalogue-api/request/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>Sign up on the Quix Portal</p> </li> <li> <p>Read about Authenticating with the Data Catalogue     API</p> </li> </ul>"},{"location":"apis/data-catalogue-api/request/#endpoint-urls","title":"Endpoint URLs","text":"<p>The Data Catalogue API is available on a per-workspace basis, so the subdomain is based on a combination of your organization and workspace names. See How to get a workspace ID to find out how to get the exact hostname required. It will be in this format:</p> <pre><code>https://telemetry-query-${organization}-${workspace}.platform.quix.ai/\n</code></pre> <p>So your final endpoint URL will look something like:</p> <pre><code>https://telemetry-query-acme-weather.platform.quix.ai/\n</code></pre>"},{"location":"apis/data-catalogue-api/request/#method","title":"Method","text":"<p>Most endpoints use the <code>POST</code> method, even those that just fetch data. Ensure your HTTP client sends <code>POST</code> requests as appropriate.</p> <p>Using <code>curl</code>, the <code>-X POST</code> flag specifies a POST request. Note that this is optional if you\u2019re using the <code>-d</code> flag to send a payload (see below).</p> <pre><code>curl -X POST ...\n</code></pre>"},{"location":"apis/data-catalogue-api/request/#payload","title":"Payload","text":"<p>For most methods, you\u2019ll need to send a JSON object containing supported parameters. You\u2019ll also need to set the appropriate content type for the payload you\u2019re sending:</p> <pre><code>curl -H \"Content-Type: application/json\" ...\n</code></pre> <p>Warning</p> <p>You must specify the content type of your payload. Failing to include this header will result in a <code>415 UNSUPPORTED MEDIA TYPE</code> status code.</p> <p>You can send data via a POST request using the <code>curl</code> flag <code>-d</code>. This should be followed by either a string of JSON data, or a string starting with the @ symbol, followed by a filename containing the JSON data.</p> <pre><code>curl -d '{\"key\": \"value\"}' ...\ncurl -d \"@data.json\" ...\n</code></pre>"},{"location":"apis/data-catalogue-api/request/#complete-curl-example","title":"Complete curl example","text":"<p>You should structure most of your requests to the API around this pattern:</p> <pre><code>curl -H \"Authorization: ${token}\" \\\n-H \"Content-Type: application/json\" \\\n-d \"@data.json\" \\\nhttps://${domain}.platform.quix.ai/${endpoint}\n</code></pre>"},{"location":"apis/data-catalogue-api/streams-filtered/","title":"Filtered streams","text":"<p>To fetch specific streams, you can include various filters with your request to the <code>/streams</code> endpoint.</p>"},{"location":"apis/data-catalogue-api/streams-filtered/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up.</p> </li> <li> <p>Get a Personal Access Token     to authenticate each request.</p> </li> </ul>"},{"location":"apis/data-catalogue-api/streams-filtered/#fetch-a-single-stream-via-id","title":"Fetch a single stream via ID","text":"<p>The most basic filter matches against a stream\u2019s ID.</p> <pre><code>curl \"https://${domain}.platform.quix.ai/streams\" \\\n-H \"Authorization: bearer ${token}\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"streamIds\": [\"302b1de3-2338-43cb-8148-3f0d6e8c0b8a\"]}'\n</code></pre> <p>Note that you can supply multiple IDs in the <code>streamIds</code> array to match multiple streams.</p>"},{"location":"apis/data-catalogue-api/streams-filtered/#filtering-streams-on-basic-properties","title":"Filtering streams on basic properties","text":"<p>The location of a stream defines its position in a hierarchy. A stream location looks just like a filesystem path. You can filter streams based on the start of this path, so you can easily find streams contained within any point in the hierarchy. For example, this query will find streams with a location of <code>/one</code> but it will also find streams with a <code>/one/two</code> location:</p> <pre><code>curl \"https://${domain}.platform.quix.ai/streams\" \\\n-H \"Authorization: bearer ${token}\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"location\": \"/one\"}'\n</code></pre> <p>Warning</p> <p>Since this is just a basic prefix match, filtering on a location named <code>/one</code> will also bring back matches for the location <code>/one111</code> as well as the location <code>/one/111</code>. If you want to strictly filter on an exact directory (and below), make sure to  include a trailing slash, e.g. <code>/one/</code>.</p> <p>Note</p> <p>Filtering on topic uses a case insensitive Equals match. Filtering on a topic named \"MyTopic\" will match \"mytopic\" but will not match \"MyTopic123\"</p> <p>You can filter streams based on their use of a given parameter with the <code>parameterIds</code> property. For example, to find all streams that contain at least one single occurence of <code>Gear</code> data:</p> <pre><code>curl \"https://${domain}.platform.quix.ai/streams\" \\\n-H \"Authorization: bearer ${token}\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"parameterIds\": [ \"Gear\"] }'\n</code></pre> <p>You can filter based on the presence or absence of a certain stream status, for example, if the stream is <code>Open</code> or was <code>Interrupted</code>. The <code>includeStatuses</code> and <code>excludeStatuses</code> properties each take an array of values to act on. So to get all streams that aren\u2019t Interrupted or Closed, use this query:</p> <pre><code>curl \"https://${domain}.platform.quix.ai/streams\" \\\n-H \"Authorization: bearer ${token}\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"excludeStatuses\": [ \"Interrupted\", \"Closed\" ]}'\n</code></pre>"},{"location":"apis/data-catalogue-api/streams-filtered/#filtering-streams-on-metadata","title":"Filtering streams on metadata","text":"<p>You can associate metadata with your streams. This can be used, for example, to store the circuit a car has travelled around, or the player of a particular run of a game.</p> <p>To filter on metadata, include the <code>metadata</code> property in the JSON object in your request body. This property\u2019s value is an array of objects, each of which has two properties, <code>key</code> and <code>value</code>:</p> <ul> <li> <p><code>key</code>     The exact, case-sensitive key of the metadata you\u2019re interested in.</p> </li> <li> <p><code>value</code>     The exact, case-sensitive value of the metadata to match on</p> </li> </ul> <p>If you have a metadata entry keyed as \"circuit\", you can match against it for an example value with this payload:</p> <pre><code>\"metadata\": [{\n\"key\": \"circuit\",\n\"value\": \"Sakhir Short\"\n}]\n</code></pre> <p>As before, the response is an array of Stream objects:</p> <pre><code>[{\n\"streamId\":\"e6545c18-d20d-47bd-8997-f3f825c1a45c\",\n\"name\":\"cardata\",\n\"topic\":\"cardata\",\n\"createdAt\":\"2021-03-31T13:04:43.368Z\",\n\"lastUpdate\":\"2021-03-31T13:04:44.53Z\",\n\"dataStart\":1612191099000000000,\n\"dataEnd\":1612191371000000000,\n\"status\":\"Closed\",\n\"metadata\":{\n\"circuit\":\"Sakhir Short\",\n\"player\":\"Swal\",\n\"game\":\"Codemasters F1 2019\"\n},\n\"parents\":[],\n\"location\":\"/static data/\"\n}]\n</code></pre>"},{"location":"apis/data-catalogue-api/streams-filtered/#ordering-results","title":"Ordering results","text":"<p>Calls to the <code>/streams</code> endpoint can include an <code>ordering</code> property in the payload. This references an array of properties to sort on, each one an object with the following properties:</p> <ul> <li> <p>by     A string representing the property to order by.</p> </li> <li> <p>direction     A string, either \"Asc\" or \"Desc\", to define the sort direction.</p> </li> </ul> <p>For example, to sort all streams in ascending order by topic:</p> <pre><code>curl \"https://${domain}.platform.quix.ai/streams\" \\\n-H \"Authorization: bearer ${token}\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"ordering\": [{ \"by\": \"topic\", \"direction\": \"asc\" }]}'\n</code></pre>"},{"location":"apis/data-catalogue-api/streams-models/","title":"Streams with models","text":"<p>One stream can derive from another, for example, acting as a model in a pipeline. This relationship can be inspected using the <code>/streams/models</code> endpoint.</p>"},{"location":"apis/data-catalogue-api/streams-models/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up.</p> </li> <li> <p>Get a Personal Access Token     to authenticate each request.</p> </li> </ul>"},{"location":"apis/data-catalogue-api/streams-models/#fetching-model-data","title":"Fetching model data","text":"<p>The hierarchy is represented as a parent/child structure where a stream can have an optional parent and any number of children.</p> <p>The <code>/streams/models</code> endpoint will return data in the same structure as the <code>/streams</code> endpoint, with an additional property for each stream: <code>children</code>. This is an array of stream objects which may have their own children.</p> <p>The payload requirements are the same as those for <code>/streams</code>. You can fetch model information across all streams with an empty payload:</p> <pre><code>curl \"https://${domain}.platform.quix.ai/streams/models\" \\\n-H \"Authorization: bearer ${token}\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{}\"\n</code></pre> <p>Here\u2019s an example result for a stream with two children:</p> <pre><code>[{\n\"children\": [{\n\"children\": [],\n\"streamId\": \"79bbed17-5c71-4b0e-99f6-3596577b46d8\",\n\"name\": \"new-child\",\n\"topic\": \"cars\",\n\"createdAt\": \"2021-04-08T15:27:09.19Z\",\n\"lastUpdate\": \"2021-04-13T10:21:52.572Z\",\n\"status\": \"Open\",\n\"metadata\": {},\n\"parents\": [\n\"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\"\n],\n\"location\": \"/\"\n},{\n\"children\": [],\n\"streamId\": \"f003c1dd-9abe-49dd-afd2-f194d3d96035\",\n\"name\": \"example1\",\n\"topic\": \"cars\",\n\"createdAt\": \"2021-04-12T11:50:38.504Z\",\n\"lastUpdate\": \"2021-04-12T12:00:40.482Z\",\n\"status\": \"Interrupted\",\n\"metadata\": {\n\"rain\": \"light\"\n},\n\"parents\": [\n\"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\"\n],\n\"location\": \"/examples/first/\"\n}],\n\"streamId\": \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\",\n\"topic\": \"cars\",\n\"createdAt\": \"2021-04-08T14:12:29.807Z\",\n\"lastUpdate\": \"2021-04-12T13:45:08.377Z\",\n\"timeOfRecording\": \"2021-04-12T00:00:00Z\",\n\"dataStart\": 0,\n\"dataEnd\": 1618233869000000000,\n\"status\": \"Interrupted\",\n\"metadata\": {},\n\"parents\": [],\n\"location\": \"/\"\n}]\n</code></pre> <p>And here\u2019s an example with a child and a grandchild:</p> <pre><code>[{\n\"children\": [{\n\"children\": [{\n\"children\": [],\n\"streamId\": \"79bbed17-5c71-4b0e-99f6-3596577b46d8\",\n\"name\": \"new-child\",\n\"topic\": \"cars\",\n\"createdAt\": \"2021-04-08T15:27:09.19Z\",\n\"lastUpdate\": \"2021-04-13T10:30:11.495Z\",\n\"status\": \"Open\",\n\"metadata\": {},\n\"parents\": [\n\"f003c1dd-9abe-49dd-afd2-f194d3d96035\"\n],\n\"location\": \"/\"\n}\n],\n\"streamId\": \"f003c1dd-9abe-49dd-afd2-f194d3d96035\",\n\"name\": \"example1\",\n\"topic\": \"cars\",\n\"createdAt\": \"2021-04-12T11:50:38.504Z\",\n\"lastUpdate\": \"2021-04-12T12:00:40.482Z\",\n\"status\": \"Interrupted\",\n\"metadata\": {\n\"rain\": \"light\"\n},\n\"parents\": [\n\"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\"\n],\n\"location\": \"/examples/first/\"\n}\n],\n\"streamId\": \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\",\n\"topic\": \"cars\",\n\"createdAt\": \"2021-04-08T14:12:29.807Z\",\n\"lastUpdate\": \"2021-04-12T13:45:08.377Z\",\n\"timeOfRecording\": \"2021-04-12T00:00:00Z\",\n\"dataStart\": 0,\n\"dataEnd\": 1618233869000000000,\n\"status\": \"Interrupted\",\n\"metadata\": {},\n\"parents\": [],\n\"location\": \"/\"\n}]\n</code></pre>"},{"location":"apis/data-catalogue-api/streams-paged/","title":"Paged streams","text":"<p>You can fetch all streams within a workspace, across topics and locations, with a single call. If you\u2019re working with a large number of streams, you can use pagination parameters to group the results into smaller pages.</p>"},{"location":"apis/data-catalogue-api/streams-paged/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>If you don\u2019t already have any Stream data in your workspace, you can use any Source of our Quix Library to set some up.</p> </li> <li> <p>Get a Personal Access Token     to authenticate each request.</p> </li> </ul>"},{"location":"apis/data-catalogue-api/streams-paged/#fetching-all-streams","title":"Fetching all streams","text":"<p>The <code>/streams</code> endpoint provides read access to all streams within the workspace. Sending an empty JSON object in your request body will return all streams.</p> <p>Warning</p> <p>Even if you\u2019re not supplying any parameters, you must still send a valid empty object as JSON data in the body of your request.</p>"},{"location":"apis/data-catalogue-api/streams-paged/#example-request","title":"Example request","text":"<pre><code>curl \"https://${domain}.platform.quix.ai/streams\" \\\n-H \"Authorization: bearer ${token}\" \\\n-H \"Content-Type: application/json\" \\\n-d \"{}\"\n</code></pre>"},{"location":"apis/data-catalogue-api/streams-paged/#example-response","title":"Example response","text":"<p>The JSON returned consists of an array of Stream objects:</p> <pre><code>[{\n\"streamId\":\"e6545c18-d20d-47bd-8997-f3f825c1a45c\",\n\"name\":\"cardata\",\n\"topic\":\"cardata\",\n\"createdAt\":\"2021-03-31T13:04:43.368Z\",\n\"lastUpdate\":\"2021-03-31T13:04:44.53Z\",\n\"dataStart\":1612191099000000000,\n\"dataEnd\":1612191371000000000,\n\"status\":\"Closed\",\n\"metadata\":{},\n\"parents\":[],\n\"location\":\"/static data/\"\n}]\n</code></pre>"},{"location":"apis/data-catalogue-api/streams-paged/#fetching-streams-page-by-page","title":"Fetching streams page by page","text":"<p>To reduce the size of the response, you should page these results with the <code>paging</code> property. Include this in the JSON object you send in the body of your request. The value of this property is an object with two members, <code>index</code> and <code>length</code>:</p> <ul> <li> <p><code>index</code>     The index of the page you want returned.</p> </li> <li> <p><code>length</code>     The number of items (i.e. streams) per page.</p> </li> </ul> <p>For example, to group all streams in pages of 10 and receive the 2nd page, use this value:</p> <pre><code>\"paging\": {\n\"index\": 1,\n\"length\": 10\n}\n</code></pre>"},{"location":"apis/data-catalogue-api/streams-paged/#example-request_1","title":"Example request","text":"<pre><code>curl \"https://${domain}.platform.quix.ai/streams\" \\\n-H \"Authorization: bearer ${token}\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"paging\":{\"index\": 1,\"length\": 10}}'\n</code></pre>"},{"location":"apis/streaming-reader-api/authenticate/","title":"Authenticate","text":""},{"location":"apis/streaming-reader-api/authenticate/#before-you-begin","title":"Before you begin","text":"<ul> <li>Sign up on the Quix Portal</li> </ul>"},{"location":"apis/streaming-reader-api/authenticate/#get-a-personal-access-token","title":"Get a Personal Access Token","text":"<p>You should authenticate requests to the Streaming Reader API using a Personal Access Token (PAT). This is a time-limited token which you can revoke if necessary.</p> <p>Follow these steps to generate a PAT:</p> <ol> <li> <p>Click the user icon in the top-right of the Portal and select the     Tokens menu.</p> </li> <li> <p>Click GENERATE TOKEN.</p> </li> <li> <p>Choose a name to describe the token\u2019s purpose, and an expiration     date, then click CREATE.</p> </li> <li> <p>Copy the token and store it in a secure place.</p> </li> </ol> <p>Warning</p> <p>You won\u2019t be able to retrieve your token from the Portal once you\u2019ve created it, so make sure to take a copy.</p> <p>Warning</p> <p>Treat your tokens like passwords and keep them secret. When working with the API, use tokens as environment variables instead of hardcoding them into your programs.</p>"},{"location":"apis/streaming-reader-api/intro/","title":"Introduction","text":"<p>As an alternative to Quix Streams, the Quix platform supports real-time data streaming over WebSockets. Clients can receive updates on data and definitions for parameters and events, as they happen. The examples shown use the Microsoft SignalR JavaScript client library.</p>"},{"location":"apis/streaming-reader-api/intro/#topics","title":"Topics","text":"<ul> <li> <p>Set up SignalR</p> </li> <li> <p>Authenticate</p> </li> <li> <p>Reading data</p> </li> <li> <p>Subscription &amp; Event     reference</p> </li> </ul>"},{"location":"apis/streaming-reader-api/reading-data/","title":"Reading data","text":"<p>Before you can read data from a stream, you need to subscribe to an event of the Streaming Reader service like ParameterData or EventData.</p> <p>You can get a full list of Subscriptions and Events here .</p>"},{"location":"apis/streaming-reader-api/reading-data/#example","title":"Example","text":"<p>The following code sample shows how to use the SignalR client library to:</p> <ol> <li> <p>Establish a connection to Quix</p> </li> <li> <p>Subscribe to a parameter data stream</p> </li> <li> <p>Receive data from that stream</p> </li> <li> <p>Unsubscribe from the event</p> </li> </ol> <pre><code>var signalR = require(\"@microsoft/signalr\");\nconst options = {\naccessTokenFactory: () =&gt; 'YOUR_ACCESS_TOKEN'\n};\nconst connection = new signalR.HubConnectionBuilder()\n.withUrl(\"https://reader-YOUR_WORKSPACE_ID.platform.quix.ai/hub\", options)\n.build();\n// Establish connection\nconnection.start().then(() =&gt; {\nconsole.log(\"Connected to Quix.\");\n// Subscribe to parameter data stream.\nconnection.invoke(\"SubscribeToParameter\", \"your-topic-name\", \"your-stream-id\", \"your-parameter-id\");\n// Read data from the stream.\nconnection.on(\"ParameterDataReceived\", data =&gt; {\nlet model = JSON.parse(data);\nconsole.log(\"Received data from stream: \" + model.streamId);\n// Unsubscribe from stream.\nconnection.invoke(\"UnsubscribeFromParameter\", \"your-topic-name\", \"your-stream-id\", \"your-parameter-id\");\n});\n});\n</code></pre>"},{"location":"apis/streaming-reader-api/signalr/","title":"Set up SignalR","text":""},{"location":"apis/streaming-reader-api/signalr/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>Get a PAT for     Authentication</p> </li> <li> <p>Ensure you know your workspace ID</p> </li> </ul>"},{"location":"apis/streaming-reader-api/signalr/#installation","title":"Installation","text":"<p>If you are using a package manager like npm, you can install SignalR using <code>npm install @microsoft/signalr</code>. For other installation options that don\u2019t depend on a platform like Node.js, such as consuming SignalR from a CDN, please refer to SignalR documentation.</p>"},{"location":"apis/streaming-reader-api/signalr/#testing-the-connection","title":"Testing the connection","text":"<p>Once you\u2019ve installed the SignalR library, you can test it\u2019s set up correctly with the following code snippet. This opens a connection to the hub running on your custom subdomain, and checks authentication.</p> <p>You should replace the text <code>YOUR_ACCESS_TOKEN</code> with the PAT obtained from Authenticating with the Streaming Reader API.</p> <p>You should also replace <code>YOUR_WORKSPACE_ID</code> with the appropriate identifier, a combination of your organization and workspace names. This can be located in one of the following ways:</p> <ul> <li> <p>Portal URL   Look in the browsers URL when you are logged into the Portal and   inside the Workspace you want to work with. The URL contains the   workspace id. e.g everything after \"workspace=\" till the next &amp;</p> </li> <li> <p>Topics Page   In the Portal, inside the Workspace you want to work with, click the   Topics menu    and then   click the expand icon    on any   topic. Here you will see a Username under the Broker Settings.     This Username is also the Workspace Id.</p> </li> </ul> <pre><code>var signalR = require(\"@microsoft/signalr\");\nconst options = {\naccessTokenFactory: () =&gt; 'YOUR_ACCESS_TOKEN'\n};\nconst connection = new signalR.HubConnectionBuilder()\n.withUrl(\"https://reader-YOUR_WORKSPACE_ID.platform.quix.ai/hub\", options)\n.build();\nconnection.start().then(() =&gt; console.log(\"SignalR connected.\"));\n</code></pre> <p>If the connection is successful, you should see the console log \u201cSignalR connected\u201d.</p>"},{"location":"apis/streaming-reader-api/subscriptions/","title":"Subscription &amp; Event reference","text":"<p>The Quix SignalR hub provides the following subscriptions and events.</p>"},{"location":"apis/streaming-reader-api/subscriptions/#subscriptions","title":"Subscriptions","text":"<p>You can subscribe to the following hub methods via the <code>invoke</code> method of a <code>HubConnection</code>:</p> <ul> <li> <p><code>SubscribeToParameter(topicName, streamId, parameterId)</code>: Subscribe     to a parameter data stream.</p> </li> <li> <p><code>SubscribeToEvent(topicName, streamId, eventId)</code>: Subscribes to an     event data stream.</p> </li> <li> <p><code>IList&lt;ActiveStream&gt; SubscribeToActiveStreams(topicName)</code>: Subscribe     to Active Streams List changes. The subscription method returns an     initial list of the active streams existing in the topic.</p> </li> <li> <p><code>IList&lt;TopicMetrics&gt; SubscribeToTopicMetrics(topicName)</code>: Subscribe     to Topic metrics updates. The subscription method returns an initial     list of the last 5 minutes of topic metrics.</p> </li> <li> <p><code>SubscribeToPackages(string topicName)</code>: Subscribe to Topic     packages. A package is an abstraction for any message received in     the topic.</p> </li> </ul> <p>Each Subscribe method has its own Unsubscribe. Use them once you don\u2019t need the subscriptions anymore to avoid receiving data unnecessarily:</p> <ul> <li> <p><code>UnsubscribeFromParameter(topicName, streamId, parameterId)</code>:     Unsubscribe from a parameter data stream.</p> </li> <li> <p><code>UnsubscribeFromEvent(topicName, streamId, eventId)</code> Unsubscribe     from an event data stream.</p> </li> <li> <p><code>UnsubscribeFromActiveStreams(string topicName)</code>: Unsubscribe from     Streams List changes.</p> </li> <li> <p><code>UnsubscribeFromTopicMetrics(topicName)</code>: Unsubscribe from Topic     metrics updates.</p> </li> <li> <p><code>UnsubscribeFromPackages(string topicName)</code>: Unsubscribe from Topic     packages.</p> </li> <li> <p><code>UnsubscribeFromStream(topicName, streamId)</code>: Unsubscribes from all     subscriptions of the specified stream.</p> </li> </ul> <p>Tip</p> <p>You should pass the method\u2019s name as the first argument to <code>invoke</code>, followed by the method-specific arguments. For example, to call:</p> <p><code>SubscribeToParameter(topicName, streamId, parameterId)</code></p> <p>Use the following:</p> <pre><code>connection.invoke(\"SubscribeToParameter\", \"your-topic-name\", \"your-stream-id\", \"your-parameter-id\");\n</code></pre>"},{"location":"apis/streaming-reader-api/subscriptions/#signalr-events","title":"SignalR events","text":"<p>You can register a handler for SignalR events using the <code>on</code> method of a <code>HubConnection</code>. The following events are available:</p> <ul> <li> <p><code>ParameterDataReceived(parameterData)</code></p> </li> <li> <p><code>EventDataReceived(eventData)</code></p> </li> <li> <p><code>ActiveStreamsChanged(stream, action)</code></p> </li> <li> <p><code>TopicMetricsUpdated(metrics)</code></p> </li> <li> <p><code>PackageReceived(package)</code></p> </li> </ul> <p>Tip</p> <p>You should pass the event\u2019s name as the first argument to <code>on</code>, followed by a function callback. For example, to react to the <code>ParameterDataReceived</code> event, use the following:</p> <pre><code>connection.on(\"ParameterDataReceived\", data =&gt; {\n// process payload data\n});\n</code></pre>"},{"location":"apis/streaming-reader-api/subscriptions/#parameterdatareceived","title":"ParameterDataReceived","text":"<p>Add a listener to <code>ParameterDataReceived</code> event to receive data from a <code>SubscribeToParameter</code> subscription.</p> <p>One event is generated each time a ParameterData package is received in the Topic and the data contains the Parameter the user has subscribed for.</p> <p>Example payload:</p> <pre><code>{\ntopicName: 'topic-1',\nstreamId: 'b45969d2-4624-4ab7-9779-c8f90ce79420',\ntimestamps: [ 1591733989000000000, 1591733990000000000, 1591733991000000000 ],\nnumericValues: { ParameterA: [ 1, 2, 3 ] },\nstringValues: {},\ntagValues: { ParameterA: [ null, null, 'tag-1' ] }\n}\n</code></pre>"},{"location":"apis/streaming-reader-api/subscriptions/#eventdatareceived","title":"EventDataReceived","text":"<p>Add a listener to <code>EventDataReceived</code> event to receive data from a <code>SubscribeToEvent</code> subscription.</p> <p>One event is generated each time a EventData package is received in the Topic and the data contains the Event the user has subscribed for.</p> <p>Example payload:</p> <pre><code>{\ntopicName: 'topic-1',\nstreamId: 'b45969d2-4624-4ab7-9779-c8f90ce79420'\nid: 'EventA',\ntimestamp: 1591733990000000000,\nvalue: 'val-a',\ntags: {\ntag1: 'val1'\n}\n}\n</code></pre>"},{"location":"apis/streaming-reader-api/subscriptions/#activestreamchanged","title":"ActiveStreamChanged","text":"<p>This event is generated each time a change has been produced in the list of Active streams of a Topic.</p> <p>Add a listener to <code>ActiveStreamChanged</code> event to receive data from a <code>SubscribeToActiveStreams</code> subscription. This SignalR event contains 2 arguments on it:</p> <ul> <li> <p><code>stream</code>: Payload of the stream that has been changed.</p> </li> <li> <p><code>action</code>: It describes the type of operation has been applied to the     list of active streams:</p> <ul> <li> <p><code>AddUpdate</code>: Stream added or updated</p> </li> <li> <p><code>Remove</code>: Stream removed</p> </li> </ul> </li> </ul> <p>Stream payload example:</p> <pre><code>{\n\"streamId\": \"5ecfc7ce-906c-4d3a-811c-a85ea75a24b3\",\n\"topicName\": \"f1-data\",\n\"name\": \"F1 Game - Swal - Sakhir Short 2022-04-08-09:00:39\",\n\"location\": \"/Game/Codemasters/F1-2019/Sakhir Short\",\n\"metadata\": {\n\"GameVersion\": \"1.22\",\n\"PacketFormat\": \"2019\",\n\"Track\": \"Sakhir Short\",\n\"SessionId\": \"237322236454500810\",\n\"Player0_Name\": \"Swal\",\n\"Player0_Id\": \"100\"\n},\n\"parents\": [],\n\"timeOfRecording\": \"2022-04-08T09:00:39.3971666Z\",\n\"parameters\": {\n\"EngineRPM\": {\n\"dataType\": \"Numeric\",\n\"minimumValue\": 0,\n\"maximumValue\": 20000,\n\"location\": \"/Player/Telemetry/Engine\"\n},\n\"LapDistance\": {\n\"dataType\": \"Numeric\",\n\"unit\": \"m\",\n\"location\": \"/Player/Telemetry/Misc\"\n},\n\"Brake\": {\n\"description\": \"Amount of brake applied\",\n\"dataType\": \"Numeric\",\n\"minimumValue\": 0,\n\"maximumValue\": 1,\n\"location\": \"/Player/Telemetry/Input\"\n},\n\"Throttle\": {\n\"dataType\": \"Unknown\",\n\"minimumValue\": 0,\n\"maximumValue\": 1,\n\"location\": \"/Player/Telemetry/Input\"\n},\n\"Gear\": {\n\"dataType\": \"Numeric\",\n\"minimumValue\": -1,\n\"maximumValue\": 8,\n\"location\": \"/Player/Telemetry/Engine\"\n},\n\"Speed\": {\n\"dataType\": \"Numeric\",\n\"minimumValue\": 0,\n\"maximumValue\": 400,\n\"location\": \"/Player/Telemetry/Engine\"\n},\n\"Steer\": {\n\"dataType\": \"Numeric\",\n\"minimumValue\": -1,\n\"maximumValue\": 1,\n\"location\": \"/Player/Telemetry/Input\"\n},\n},\n\"events\": {\n\"Player_NewLap\": {\n\"name\": \"Player NewLap\",\n\"level\": \"Information\",\n\"location\": \"\"\n},\n\"Player_Position_Changed\": {\n\"name\": \"Player Position Changed\",\n\"level\": \"Critical\",\n\"location\": \"\"\n},\n\"RaceWinner\": {\n\"name\": \"Race Winner\",\n\"level\": \"Critical\",\n\"location\": \"\"\n},\n},\n\"firstSeen\": \"2022-04-08T08:57:40.3406586Z\",\n\"lastSeen\": \"2022-04-08T09:00:39.6308255Z\",\n\"status\": \"Receiving\",\n\"lastData\": \"2022-04-08T09:00:39.6237312Z\"\n}\n</code></pre>"},{"location":"apis/streaming-reader-api/subscriptions/#topicmetricsupdated","title":"TopicMetricsUpdated","text":"<p>This event is generated periodically by the service to provide basic metrics about a Topic, like \"Bytes per Second\" or \"Number of Active Streams\".</p> <p>Add a listener to <code>TopicMetricsUpdated</code> event to receive data from a <code>SubscribeToTopicMetrics</code> subscription.</p> <p>Topic Metrics payload example:</p> <pre><code>{\n\"timestamp\": \"2022-04-10T19:26:49.1417825Z\",\n\"topicName\": \"f1-data\",\n\"bytesPerSecond\": 14877,\n\"activeStreams\": 1\n}\n</code></pre>"},{"location":"apis/streaming-reader-api/subscriptions/#packagereceived","title":"PackageReceived","text":"<p>Add a listener to <code>PackageReceived</code> event to receive data from a <code>SubscribeToPackages</code> subscription.</p> <p>One event is generated each time a package is received in the topic.</p> <ul> <li> <p>Type: Indicates the Quix Sdk model used to deserialize the package.</p> </li> <li> <p>Value: Deserialized package object represented as a Json string     format.</p> </li> </ul> <p>Package payload example:</p> <pre><code>{\n\"topicName\": \"f1-data\",\n\"streamId\": \"dec481d7-7ae4-403a-9d20-a1cabdcd3275\",\n\"type\": \"Quix.Sdk.Process.Models.ParameterDataRaw\",\n\"value\": \"{\\\"Epoch\\\":0,\\\"Timestamps\\\":[1649623155716050700],\\\"NumericValues\\\":{\\\"LapDistance\\\":[542.504638671875],\\\"TotalLapDistance\\\":[4368.53271484375]},\\\"StringValues\\\":{},\\\"BinaryValues\\\":{},\\\"TagValues\\\":{\\\"LapValidity\\\":[\\\"Valid\\\"],\\\"LapNumber\\\":[\\\"2\\\"],\\\"PitStatus\\\":[\\\"None\\\"],\\\"Sector\\\":[\\\"0\\\"],\\\"DriverStatus\\\":[\\\"Flying_lap\\\"]}}\",\n\"dateTime\": \"2022-04-10T20:39:16.63Z\"\n}\n</code></pre>"},{"location":"apis/streaming-writer-api/authenticate/","title":"Authenticate","text":""},{"location":"apis/streaming-writer-api/authenticate/#before-you-begin","title":"Before you begin","text":"<ul> <li>Sign up on the Quix Portal</li> </ul>"},{"location":"apis/streaming-writer-api/authenticate/#get-a-personal-access-token","title":"Get a Personal Access Token","text":"<p>You should authenticate requests to the Streaming Writer API using a Personal Access Token (PAT). This is a time-limited token which you can revoke if necessary.</p> <p>Follow these steps to generate a PAT:</p> <ol> <li> <p>Click the user icon in the top-right of the Portal and select the     Tokens menu.</p> </li> <li> <p>Click GENERATE TOKEN.</p> </li> <li> <p>Choose a name to describe the token\u2019s purpose, and an expiration     date, then click CREATE.</p> </li> <li> <p>Copy the token and store it in a secure place.</p> </li> </ol> <p>Warning</p> <p>You won\u2019t be able to retrieve your token from the Portal once you\u2019ve created it, so make sure to take a copy.</p> <p>Warning</p> <p>Treat your tokens like passwords and keep them secret. When working with the API, use tokens as environment variables instead of hardcoding them into your programs.</p>"},{"location":"apis/streaming-writer-api/authenticate/#sign-all-your-requests-using-this-token","title":"Sign all your requests using this token","text":"<p>Make sure you accompany each request to the API with an <code>Authorization</code> header using your PAT as a bearer token, as follows:</p> <pre><code>Authorization: bearer &lt;token&gt;\n</code></pre> <p>Replace <code>&lt;token&gt;</code> with your Personal Access Token. For example, if you\u2019re using curl on the command line, you can set the header using the <code>-H</code> flag:</p> <pre><code>curl -H \"Authorization: bearer &lt;token&gt;\" ...\n</code></pre> <p>Warning</p> <p>If you fail to send a valid Authorization header, the API will respond with a <code>401 UNAUTHORIZED</code> status code.</p>"},{"location":"apis/streaming-writer-api/create-stream/","title":"Create a new Stream","text":"<p>You can create a new stream by specifying a topic to create it in, and supplying any other additional properties required.</p> <p>Tip</p> <p>This method is optional. You can also create a stream implicitly by sending data to a stream that doesn\u2019t already exist. But creating a stream using the method on this page avoids having to determine a unique stream id yourself.</p>"},{"location":"apis/streaming-writer-api/create-stream/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>You should have a Workspace set up with at least one Topic.</p> </li> <li> <p>Get a Personal Access Token to authenticate each     request.</p> </li> </ul>"},{"location":"apis/streaming-writer-api/create-stream/#using-the-streams-endpoint","title":"Using the /streams endpoint","text":"<p>To create a new stream, send a <code>POST</code> request to:</p> <pre><code>/topics/${topicName}/streams\n</code></pre> <p>You should replace <code>$\\{topicName}</code> in the endpoint URL with the name of the Topic you wish to create the stream in. For example, if your topic is named \u201ccars\u201d, your endpoint url will be <code>/topics/cars/streams</code>.</p>"},{"location":"apis/streaming-writer-api/create-stream/#example-request","title":"Example request","text":"<p>You can create a new Stream with an absolute minimum of effort by passing an empty JSON object in the payload:</p> <ul> <li> <p>curl</p> <pre><code>curl \"https://${domain}.platform.quix.ai/topics/${topicName}/streams\" \\\n-H \"Authorization: bearer ${token}\" \\\n-H \"Content-Type: application/json\" \\\n-d '{}'\n</code></pre> </li> <li> <p>Node.js</p> <pre><code>const https = require('https');\nconst data = \"{}\";\nconst options = {\nhostname: domain + '.platform.quix.ai',\npath: '/topics/' + topicName + '/streams',\nmethod: 'POST',\nheaders: {\n'Authorization': 'Bearer ' + token,\n'Content-Type': 'application/json'\n}\n};\nconst req = https.request(options, res =&gt; {\nres.on('data', d =&gt; {\nlet streamId = JSON.parse(d).streamId;\nconsole.log(streamId);\n});\n});\nreq.write(data);\nreq.end();\n</code></pre> </li> </ul> <p>For most real-world cases, you\u2019ll also want to provide some or all of the following:</p> <ul> <li> <p><code>name</code></p> </li> <li> <p><code>location</code></p> </li> <li> <p><code>metadata</code></p> </li> <li> <p><code>parents</code></p> </li> <li> <p><code>timeOfRecording</code></p> </li> </ul> <p>For example, here\u2019s a more useful payload:</p> <pre><code>{\n\"name\": \"cardata\",\n\"location\": \"simulations/trials\",\n\"metadata\": {\n\"rain\": \"light\"\n}\n}\n</code></pre>"},{"location":"apis/streaming-writer-api/create-stream/#example-response","title":"Example response","text":"<p>The JSON returned is an object with a single property, <code>streamId</code>. This contains the unique identifier of your newly created stream, and will look something like this:</p> <pre><code>{\n\"streamId\": \"66fb0a2f-eb70-494e-9df7-c06d275aeb7c\"\n}\n</code></pre> <p>Tip</p> <p>If you\u2019re following these guides in order, you\u2019ll want to take note of that stream id. For curl examples, it\u2019s convenient to keep it in an environment variable, e.g.</p> <pre><code>$ streamId=66fb0a2f-eb70-494e-9df7-c06d275aeb7c\n</code></pre>"},{"location":"apis/streaming-writer-api/create-stream/#using-signalr","title":"Using SignalR","text":"<pre><code>var signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_TOKEN\"\nconst workspaceId = \"YOUR_WORKSPACE_ID\"\nconst topic = \"YOUR_TOPIC_NAME\"\nconst options = {\naccessTokenFactory: () =&gt; token\n};\nconst connection = new signalR.HubConnectionBuilder()\n.withUrl(\"https://writer-\" + workspaceId + \".platform.quix.ai/hub\", options)\n.build();\n// Establish connection\nconnection.start().then(async () =&gt; {\nconsole.log(\"Connected to Quix.\");\n// Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model.\nlet streamDetails = {\n\"name\": \"cardata\",\n\"location\": \"simulations/trials\",\n\"metadata\": {\n\"rain\": \"light\"\n}\n}\n// Send create details\nconsole.log(\"Creating stream\");\nlet createdDetails = await connection.invoke(\"CreateStream\", topic, streamDetails);\nlet streamId = createdDetails.streamId\nconsole.log(\"Created stream \" + streamId);\n});\n</code></pre> <p>Tip</p> <p>Also available as JsFiddle at https://jsfiddle.net/QuixAI/cLno68fs/</p>"},{"location":"apis/streaming-writer-api/get-swagger/","title":"Getting the Swagger documentation URL","text":"<p>You can access Swagger documentation and then use it to try out the Streaming Writer API. </p> <p>The URL is workspace-specific, and follows this pattern:</p> <pre><code>https://writer-${organization}-${workspace}.platform.quix.ai/swagger\n</code></pre> <p>The workspace ID is a combination based on your organization and workspace names. For example, for an <code>acme</code> organization with a <code>weather</code> workspace, the URL would have the following format:</p> <pre><code>https://writer-acme-weather.platform.quix.ai/swagger\n</code></pre> <p>To help determine the URL, you can find out how to get your workspace id.</p> <p>Tip</p> <p>Once you access the Swagger documentation, you can select the version of the API you require from the <code>Select a definition</code> dropdown list.</p>"},{"location":"apis/streaming-writer-api/intro/","title":"Introduction","text":"<p>The Streaming Writer API allows you to stream data into the Quix platform via HTTP endpoints or SignalR. It\u2019s an alternative to using our C# and Python client libraries. You can use the Streaming Writer API from any HTTP-capable language.</p> <p>The API is fully documented in our Swagger documentation. Read on for a guide to using the API, including real-world examples you can execute from your language of choice, or via the command line using curl.</p>"},{"location":"apis/streaming-writer-api/intro/#preparation","title":"Preparation","text":"<p>If you plan on using the HTTP endpoints, then you\u2019ll need to know how to authenticate your requests and how to form a typical request to the API.</p> <p>If you would rather use the SignalR api, which is suggested for high frequency data streaming, then see SignalR setup.</p>"},{"location":"apis/streaming-writer-api/intro/#topics-covered","title":"Topics covered","text":"<ul> <li> <p>Stream</p> <ul> <li> <p>Create a new Stream</p> </li> <li> <p>Add Stream metadata</p> </li> </ul> </li> <li> <p>Parameters</p> <ul> <li>Send Parameter data</li> </ul> </li> <li> <p>Events</p> <ul> <li>Send an Event</li> </ul> </li> </ul>"},{"location":"apis/streaming-writer-api/request/","title":"Forming a request","text":"<p>How you send requests to the Streaming Writer API will vary depending on the client or language you\u2019re using. But the API still has behavior and expectations that is common across all clients.</p> <p>Tip</p> <p>The examples in this section show how to use the popular <code>curl</code> command line tool.</p>"},{"location":"apis/streaming-writer-api/request/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>Sign up on the Quix Portal</p> </li> <li> <p>Read about Authenticating with the Streaming Writer     API</p> </li> </ul>"},{"location":"apis/streaming-writer-api/request/#endpoint-urls","title":"Endpoint URLs","text":"<p>The Streaming Writer API is available on a per-workspace basis, so the subdomain is based on a combination of your organization and workspace names. See the Swagger documentation to find out how to get the exact hostname required. It will be in this format:</p> <p>https://writer-${organization}-${workspace}.platform.quix.ai</p> <p>So your final endpoint URL will look something like:</p> <p>https://writer-acme-weather.platform.quix.ai/</p>"},{"location":"apis/streaming-writer-api/request/#method","title":"Method","text":"<p>Endpoints in this API use the <code>POST</code> and <code>PUT</code> methods. Ensure your HTTP client sends the correct request method.</p> <p>Using <code>curl</code>, you can specify the request method with the <code>-X &lt;POST|PUT&gt;</code> flag, for example:</p> <pre><code>curl -X PUT ...\n</code></pre>"},{"location":"apis/streaming-writer-api/request/#payload","title":"Payload","text":"<p>For most methods, you\u2019ll need to send a JSON object containing supported parameters. You\u2019ll also need to set the appropriate content type for the payload you\u2019re sending:</p> <pre><code>curl -H \"Content-Type: application/json\" ...\n</code></pre> <p>Warning</p> <p>You must specify the content type of your payload. Failing to include this header will result in a <code>415 UNSUPPORTED MEDIA TYPE</code> status code.</p> <p>You can send data using the <code>curl</code> flag <code>-d</code>. This should be followed by either a string of JSON data, or a string starting with the @ symbol, followed by a filename containing the JSON data.</p> <pre><code>curl -d '{\"key\": \"value\"}' ...\ncurl -d \"@data.json\" ...\n</code></pre> <p>Tip</p> <p>By default, <code>-d</code> will send a <code>POST</code> request, so <code>-X POST</code> becomes unnecessary.</p>"},{"location":"apis/streaming-writer-api/request/#complete-curl-example","title":"Complete curl example","text":"<p>You should structure most of your requests to the API around this pattern:</p> <pre><code>curl -H \"Authorization: ${token}\" \\\n-H \"Content-Type: application/json\" \\\n-d \"@data.json\" \\\nhttps://${domain}.platform.quix.ai/${endpoint}\n</code></pre>"},{"location":"apis/streaming-writer-api/send-data/","title":"Send Parameter data","text":"<p>You can send telemetry data using the Streaming Writer API. Select a topic and a stream to send the data to. In your payload, you can include numeric, string, or binary parameter data, with nanosecond-level timestamps.</p>"},{"location":"apis/streaming-writer-api/send-data/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>You should have a Workspace set up with at least one Topic.</p> </li> <li> <p>Get a Personal Access     Token to authenticate each     request.</p> </li> </ul>"},{"location":"apis/streaming-writer-api/send-data/#sending-structured-data-to-the-endpoint","title":"Sending structured data to the endpoint","text":"<p>Send a POST request together with a JSON payload representing the data you\u2019re sending to:</p> <pre><code>/topics/${topicName}/streams/${streamId}/parameters/data\n</code></pre> <p>You should replace <code>$\\{topicName}</code> with the name of the topic your stream belongs to, and <code>$\\{streamId}</code> with the id of the stream you wish to send data to. For example:</p> <pre><code>/topics/cars/streams/66fb0a2f-eb70-494e-9df7-c06d275aeb7c/parameters/data\n</code></pre> <p>Tip</p> <p>You can create a new stream by supplying a <code>$\\{streamId}</code> that doesn\u2019t already exist. This avoids the need to call the create stream endpoint separately.</p>"},{"location":"apis/streaming-writer-api/send-data/#example-request","title":"Example request","text":"<p>Your payload should include an array of <code>timestamps</code> with one timestamp for each item of data you\u2019re sending. Actual data values should be keyed on their name, in the object that corresponds to their type, one of <code>numericValues</code>, <code>stringValues</code>, or <code>binaryValues</code>. The payload is in this structure:</p> <pre><code>{\n\"timestamps\": [...],\n\"numericValues\": {...},\n\"stringValues\": {...},\n\"binaryValues\": {...},\n\"tagValues\": {...}\n}\n</code></pre> <p>Any data types that are unused can be omitted. So a final request using curl might look something like this:</p> curlNode.js <pre><code>curl -X POST \"https://${domain}.platform.quix.ai/topics/${topicName}/streams/${streamId}/parameters/data\" \\\n-H \"Authorization: Bearer ${token}\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n             \"timestamps\": [1591733989000000000, 1591733990000000000, 1591733991000000000],\n             \"numericValues\": {\n                 \"SomeParameter1\": [10.01, 202.02, 303.03],\n                 \"SomeParameter2\": [400.04, 50.05, 60.06]\n             }\n        }'\n</code></pre> <pre><code>const https = require('https');\nconst data = JSON.stringify({\n\"timestamps\": [1591733989000000000, 1591733990000000000, 1591733991000000000],\n\"numericValues\": {\n\"SomeParameter1\": [10.01, 202.02, 303.03],\n\"SomeParameter2\": [400.04, 50.05, 60.06]\n}\n});\nconst options = {\nhostname: domain + '.platform.quix.ai',\npath: '/topics/' + topicName + '/streams/' + streamId + '/parameters/data',\nmethod: 'POST',\nheaders: {\n'Authorization': 'Bearer ' + token,\n'Content-Type': 'application/json'\n}\n};\nconst req = https.request(options);\nreq.write(data);\nreq.end();\n</code></pre>"},{"location":"apis/streaming-writer-api/send-data/#response","title":"Response","text":"<p>No payload is returned from this call. A 200 HTTP response code indicates success. If the call fails, you should see either a 4xx or 5xx response code indicating what went wrong.</p>"},{"location":"apis/streaming-writer-api/send-data/#using-signalr","title":"Using SignalR","text":"<pre><code>var signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_TOKEN\"\nconst workspaceId = \"YOUR_WORKSPACE_ID\"\nconst topic = \"YOUR_TOPIC_NAME\"\nconst streamId = \"ID_OF_STREAM_TO_WRITE_TO\"\nconst options = {\naccessTokenFactory: () =&gt; token\n};\nconst connection = new signalR.HubConnectionBuilder()\n.withUrl(\"https://writer-\" + workspaceId + \".platform.quix.ai/hub\", options)\n.build();\n// Establish connection\nconnection.start().then(async () =&gt; {\nconsole.log(\"Connected to Quix.\");\n// Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model.\nlet parameterData = {\n\"epoch\": Date.now() * 1000000, // set now as time starting point, in nanoseconds\n\"timestamps\": [\n0,\n5000000000, // 5 seconds from now (see epoch)\n8000000000\n],\n\"numericValues\": {\n\"NumericParameter1\": [\n13.37,\n42,\n24.72\n]\n},\n\"stringValues\": {\n\"StringParameter1\": [\n\"Hello\",\n\"World\",\n\"!\"\n]\n},\n\"binaryValues\": {\n\"BinaryParameter1\": [\nbtoa(\"Hello\"), // send binary array as base64\nbtoa(\"World\"),\nbtoa(\"!\")\n]\n},\n\"tagValues\": {\n\"Tag1\": [\n\"A\",\n\"B\",\nnull\n]\n}\n}\n// Send stream update details\nconsole.log(\"Sending parameter data\");\nawait connection.invoke(\"SendParameterData\", topic, streamId, parameterData);\nconsole.log(\"Sent parameter data\");\n});\n</code></pre> <p>Tip</p> <p>Also available as JsFiddle at https://jsfiddle.net/QuixAI/a41b8x0t/</p>"},{"location":"apis/streaming-writer-api/send-event/","title":"Send an Event","text":"<p>You can add Events to your stream data to record discrete actions for future reference.</p>"},{"location":"apis/streaming-writer-api/send-event/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>Get a Personal Access Token to authenticate each     request.</p> </li> <li> <p>If you don\u2019t already have a Stream in your workspace, add one using     the API.</p> </li> </ul>"},{"location":"apis/streaming-writer-api/send-event/#sending-event-data","title":"Sending event data","text":"<p>To send event data to a stream, use the <code>POST</code> method with this endpoint:</p> <pre><code>/topics/${topicName}/streams/${streamId}/events/data\n</code></pre> <p>You should replace <code>${topicName}</code> with the name of the topic your stream belongs to, and <code>${streamId}</code> with the id of the stream you wish to send data to. For example:</p> <pre><code>/topics/cars/streams/66fb0a2f-eb70-494e-9df7-c06d275aeb7c/events/data\n</code></pre> <p>Tip</p> <p>You can create a new stream by supplying a <code>$\\{streamId}</code> that doesn\u2019t already exist. This avoids the need to call the create stream endpoint separately.</p> <p>Your payload should be an array of events. Each event is an object containing the following properties:</p> <ul> <li> <p>id     a unique identifier for the event</p> </li> <li> <p>timestamp     the nanosecond-precise timestamp at which the event occurred</p> </li> <li> <p>tags     a object containing key-value string pairs representing tag values</p> </li> <li> <p>value     a string value associated with the event</p> </li> </ul>"},{"location":"apis/streaming-writer-api/send-event/#example-request","title":"Example request","text":"<p>This example call adds a single event to a stream. The event has an example value and demonstrates use of a tag to include additional information.</p> curlNode.js <pre><code>curl -i \"https://${domain}.platform.quix.ai/topics/${topicName}/streams/${streamId}/events/data\" \\\n-H \"Authorization: bearer ${token}\" \\\n-H \"Content-Type: application/json\" \\\n-d '[{\n             \"id\": \"Alert\",\n             \"timestamp\": 1618133869000000000,\n             \"tags\": {\n                 \"capacity\": \"over\"\n             },\n             \"value\": \"Help\"\n     }]'\n</code></pre> <pre><code>const https = require('https');\nconst data = JSON.stringify({\n\"id\": \"Alert\",\n\"timestamp\": 1618133869000000000,\n\"tags\": {\n\"capacity\": \"over\"\n},\n\"value\": \"Help\"\n});\nconst options = {\nhostname: domain + '.platform.quix.ai',\npath: '/topics/' + topicName + '/streams/' + streamId + '/events/data',\nmethod: 'POST',\nheaders: {\n'Authorization': 'Bearer ' + token,\n'Content-Type': 'application/json'\n}\n};\nconst req = https.request(options);\nreq.write(data);\nreq.end();\n</code></pre>"},{"location":"apis/streaming-writer-api/send-event/#response","title":"Response","text":"<p>No payload is returned from this call. A 200 HTTP response code indicates success. If the call fails, you should see either a 4xx or 5xx response code indicating what went wrong.</p>"},{"location":"apis/streaming-writer-api/send-event/#using-signalr","title":"Using SignalR","text":"<pre><code>var signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_TOKEN\"\nconst workspaceId = \"YOUR_WORKSPACE_ID\"\nconst topic = \"YOUR_TOPIC_NAME\"\nconst streamId = \"ID_OF_STREAM_TO_WRITE_TO\"\nconst options = {\naccessTokenFactory: () =&gt; token\n};\nconst connection = new signalR.HubConnectionBuilder()\n.withUrl(\"https://writer-\" + workspaceId + \".platform.quix.ai/hub\", options)\n.build();\n// Establish connection\nconnection.start().then(async () =&gt; {\nconsole.log(\"Connected to Quix.\");\n// Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model.\nlet eventData = [\n{\n\"timestamp\": Date.now() * 1000000, // set now in nanoseconds,\n\"tags\": {\n\"capacity\": \"over\"\n},\n\"id\": \"Alert\",\n\"value\": \"Successful sample run\"\n}\n]\n// Send stream update details\nconsole.log(\"Sending event data\");\nawait connection.invoke(\"SendEventData\", topic, streamId, eventData);\nconsole.log(\"Sent event data\");\n});\n</code></pre> <p>Tip</p> <p>Also available as JsFiddle at https://jsfiddle.net/QuixAI/h4fztrns/</p>"},{"location":"apis/streaming-writer-api/signalr/","title":"Set up SignalR","text":""},{"location":"apis/streaming-writer-api/signalr/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>Get a PAT for     Authentication</p> </li> <li> <p>Ensure you know your workspace ID</p> </li> </ul>"},{"location":"apis/streaming-writer-api/signalr/#installation","title":"Installation","text":"<p>If you are using a package manager like npm, you can install SignalR using <code>npm install @microsoft/signalr</code>. For other installation options that don\u2019t depend on a platform like Node.js, such as consuming SignalR from a CDN, please refer to SignalR documentation.</p>"},{"location":"apis/streaming-writer-api/signalr/#testing-the-connection","title":"Testing the connection","text":"<p>Once you\u2019ve installed the SignalR library, you can test it\u2019s set up correctly with the following code snippet. This opens a connection to the hub running on your custom subdomain, and checks authentication.</p> <p>You should replace the text <code>YOUR_ACCESS_TOKEN</code> with the PAT obtained from Authenticating with the Streaming Writer API.</p> <p>You should also replace <code>YOUR_WORKSPACE_ID</code> with the appropriate identifier, a combination of your organization and workspace names. This can be located in one of the following ways:</p> <ul> <li> <p>Portal URL     Look in the browsers URL when you are logged into the Portal and     inside the Workspace you want to work with. The URL contains the     workspace id. e.g everything after \"workspace=\" till the next &amp;</p> </li> <li> <p>Topics Page     In the Portal, inside the Workspace you want to work with, click the     Topics menu      and then     click the expand icon      on any     topic. Here you will see a Username under the Broker Settings.     This Username is also the Workspace Id.</p> </li> </ul> <pre><code>var signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_TOKEN\"\nconst workspaceId = \"YOUR_WORKSPACE_ID\"\nconst options = {\naccessTokenFactory: () =&gt; token\n};\nconst connection = new signalR.HubConnectionBuilder()\n.withUrl(\"https://writer-\" + workspaceId + \".platform.quix.ai/hub\", options)\n.build();\nconnection.start().then(() =&gt; console.log(\"SignalR connected.\"));\n</code></pre> <p>If the connection is successful, you should see the console log \u201cSignalR connected\u201d.</p> <p>Tip</p> <p>Also available as JsFiddle at https://jsfiddle.net/QuixAI/L9ha4p5j/</p>"},{"location":"apis/streaming-writer-api/stream-metadata/","title":"Add Stream metadata","text":"<p>You can add arbitrary string metadata to any stream. You can also create a new stream by sending metadata using a stream id that does not already exist.</p>"},{"location":"apis/streaming-writer-api/stream-metadata/#before-you-begin","title":"Before you begin","text":"<ul> <li> <p>You should have a Workspace set up with at least one Topic.</p> </li> <li> <p>Get a Personal Access     Token to authenticate each     request.</p> </li> </ul>"},{"location":"apis/streaming-writer-api/stream-metadata/#how-to-add-metadata-to-a-stream","title":"How to add metadata to a stream","text":"<p>Send a <code>PUT</code> request to the following endpoint to update a stream with the given properties:</p> <pre><code>/topics/${topicName}/streams/${streamId}\n</code></pre> <p>You should replace <code>$\\{topicName}</code> with the name of the topic your stream belongs to, and <code>$\\{streamId}</code> with the id of the stream you wish to update. For example:</p> <pre><code>/topics/cars/streams/66fb0a2f-eb70-494e-9df7-c06d275aeb7c\n</code></pre> <p>Tip</p> <p>You can create a new stream by supplying a <code>$\\{streamId}</code> that doesn\u2019t already exist. It will be initialized with the data you provide in the payload, and the id you use in the endpoint. This avoids the need to call the create stream endpoint separately.</p> <p>Your request should contain a payload consisting of JSON data containing the desired metadata.</p>"},{"location":"apis/streaming-writer-api/stream-metadata/#example-request","title":"Example request","text":"<p>Below is an example payload demonstrating how to set a single item of metadata. Note that the <code>metadata</code> property references an object which contains key/value string-based metadata.</p> <ul> <li> <p>curl</p> <pre><code>curl \"https://${domain}.platform.quix.ai/topics/${topicName}/streams/${streamId}\" \\\n-X PUT \\\n-H \"Authorization: bearer ${token}\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\"metadata\":{\"fruit\":\"apple\"}}'\n</code></pre> </li> <li> <p>Node.js</p> <pre><code>const https = require('https');\nconst data = JSON.stringify({ metadata: { fruit: \"apple\" }});\nconst options = {\nhostname: domain + '.platform.quix.ai',\npath: '/topics/' + topicName + '/streams/' + streamId,\nmethod: 'PUT',\nheaders: {\n'Authorization': 'Bearer ' + token,\n'Content-Type': 'application/json'\n}\n};\nconst req = https.request(options);\nreq.write(data);\nreq.end();\n</code></pre> </li> </ul> <p>Since this is a PUT request, it will replace all the stream data with the payload contents. To maintain existing data, you should include it in the payload alongside your metadata, e.g.</p> <pre><code>{\n\"name\": \"Example stream\",\n\"location\": \"/sub/dir\",\n\"metadata\": {\n\"fruit\": \"apple\"\n}\n}\n</code></pre>"},{"location":"apis/streaming-writer-api/stream-metadata/#response","title":"Response","text":"<p>No payload is returned from this call. A 200 HTTP response code indicates success. If the call fails, you should see either a 4xx or 5xx response code indicating what went wrong. For example, you\u2019ll see a 405 code if you forget to specify the correct <code>PUT</code> method.</p>"},{"location":"apis/streaming-writer-api/stream-metadata/#using-signalr","title":"Using SignalR","text":"<pre><code>var signalR = require(\"@microsoft/signalr\");\nconst token = \"YOUR_TOKEN\"\nconst workspaceId = \"YOUR_WORKSPACE_ID\"\nconst topic = \"YOUR_TOPIC_NAME\"\nconst streamId = \"ID_OF_STREAM_TO_UPDATE\"\nconst options = {\naccessTokenFactory: () =&gt; token\n};\nconst connection = new signalR.HubConnectionBuilder()\n.withUrl(\"https://writer-\" + workspaceId + \".platform.quix.ai/hub\", options)\n.build();\n// Establish connection\nconnection.start().then(async () =&gt; {\nconsole.log(\"Connected to Quix.\");\n// Note, SignalR uses the same models as the HTTP endpoints, so if in doubt, check HTTP endpoint samples or Swagger for model.\nlet streamDetails = {\n\"name\": \"Example stream\",\n\"location\": \"/sub/dir\",\n\"metadata\": {\n\"fruit\": \"apple\"\n}\n}\n// Send stream update details\nconsole.log(\"Updating stream\");\nawait connection.invoke(\"UpdateStream\", topic, streamDetails);\nconsole.log(\"Updated stream\");\n});\n</code></pre> <p>Tip</p> <p>Also available as JsFiddle at https://jsfiddle.net/QuixAI/ruywnz28/</p>"},{"location":"platform/MLOps/","title":"MLOps","text":""},{"location":"platform/MLOps/#mlops","title":"MLOPs","text":"<p>There are a number of barriers that prevent companies from successfully implementing data and ML projects. It\u2019s generally considered to be significantly harder than implementing software projects due to the cross functional complexity of data and ML pipelines. As a result, even if you hire the best data scientists, their work often fails and many companies give-up on their projects before they\u2019ve begun to see the value of the technologies.</p> <p>Solving these challenges is a new field of expertise called MLOps</p> <p>We are working to incorporate MLOps into Quix so that your data team has a seamless journey from concept to production. The key steps are:</p>"},{"location":"platform/MLOps/#discover-and-access-data","title":"Discover and access data","text":"<p>Any member of any team can quickly access data in the Catalogue without support from software or regulatory teams.</p>"},{"location":"platform/MLOps/#develop-features-in-historic-data","title":"Develop features in historic data","text":"<p>Use Visualise to discover, segment, label and store significant features in the catalogue.</p>"},{"location":"platform/MLOps/#build-train-models-on-historic-data","title":"Build &amp; train models on historic data","text":"<p>Use Develop and Deploy to:</p> <ul> <li> <p>Write model code in Python using their favourite IDE.</p> </li> <li> <p>Train models on historic data.</p> </li> <li> <p>Evaluate results against raw data and results from other models.</p> </li> <li> <p>Rapidly iterate models with GIT version control.</p> </li> </ul>"},{"location":"platform/MLOps/#test-models-on-live-data","title":"Test models on live data","text":"<p>Connect models to live input topics to test them against live data sources. Review the results in Visualise.</p>"},{"location":"platform/MLOps/#build-a-production-pipeline","title":"Build a production pipeline","text":"<p>Use Develop and Deploy to:</p> <ul> <li> <p>Connect validated models to live output topics.</p> </li> <li> <p>Daisy-chain models using input and output topics.</p> </li> <li> <p>Work seamlessly with engineers to hook-up software services.</p> </li> </ul>"},{"location":"platform/MLOps/#deploy-production-models","title":"Deploy production models","text":"<p>With one click, data engineers can deploy their Python models to production without support from software engineering or DevOps teams.</p>"},{"location":"platform/MLOps/#monitor-production-models","title":"Monitor production models","text":"<p>Data teams can:</p> <ul> <li> <p>Ensure that components in a production pipeline operate correctly     through the product lifecycle.</p> </li> <li> <p>Build and deploy services that detect data drift or unexpected     results.</p> </li> </ul>"},{"location":"platform/definitions/","title":"Definitions","text":"<p>The following is a list of definitions to aid understanding of how to work with Quix and streaming data.</p>"},{"location":"platform/definitions/#workspace","title":"Workspace","text":"<p>A Workspace is an instance of a complete streaming infrastructure isolated from the rest of your Organization in terms of performance and security. It contains his own dedicated API instances and Quix internal services.</p> <p>You can imagine a Workspace as the streaming infrastructure of your company or your team, where you don\u2019t want other operations except the ones being developed in that workspace affecting the performance or the stability of your application.</p> <p>You can also have different workspaces to separate different stages of your development process like Development, Staging, and Production.</p>"},{"location":"platform/definitions/#topics","title":"Topics","text":"<p>A Topic is a channel of real-time data. You can imagine a topic as the pipe we use to interconnect our streaming applications.</p> <p>It is highly recommended to organize the data of a topic with some kind of grouping context for the telemetry data coming from a single source. Very simplified, a topic is similar to a folder in a filesystem, the streams are the files in that folder, and your data is the contents of each file.</p> <p>For example:</p> <ul> <li> <p>Car engine data</p> </li> <li> <p>Game data</p> </li> <li> <p>Telemetry from one ECU on a Boeing 737</p> </li> </ul> <p>Topics are key for scalability and good data governance. Use them to organize your data by:</p> <ul> <li> <p>Grouping incoming data by type or source</p> </li> <li> <p>Maintaining separate topics for raw, clean or processed data</p> </li> </ul>"},{"location":"platform/definitions/#stream","title":"Stream","text":"<p>A stream is a collection of data (parameters, events, binary blobs and metadata) that belong to a single session of a single source. For example:</p> <ul> <li> <p>One journey for one car.</p> </li> <li> <p>One game session for one player.</p> </li> <li> <p>One flight for one aeroplane.</p> </li> </ul>"},{"location":"platform/definitions/#timestamp","title":"Timestamp","text":"<p>A timestamp is the primary key for all data in a stream.</p> <p>We support nanosecond precision; that\u2019s 1 x 10-9 seconds or one-billionth of a second!</p> <p>Nanosecond precision is at the bleeding edge of real-time computing and is primarily driven by innovation with hardware and networking technology; kudos to you if you have an application for it!</p>"},{"location":"platform/definitions/#data-types","title":"Data Types","text":"<p>We currently support any parameter, event, metadata or blob that consist of numeric (double precision), string (UTF-8) and binary data (blobs).</p>"},{"location":"platform/definitions/#parameters","title":"Parameters","text":"<p>Parameters are values that develop over time. Quix Streams supports numeric and string values.</p> <p>For example:</p> <ul> <li> <p>Crank revolution and oil temperature are two discrete engine     parameters that begin to define the engine system.</p> </li> <li> <p>Player position in X, Y and Z are three discreet parameters that     begin to define the player location in a game.</p> </li> <li> <p>Altitude, GPS LAT, GPS LONG and Speed are four parameters that begin     to define the location and velocity of a plane in the sky.</p> </li> <li> <p>Referring back to topics as a grouping context: we would recommend     that each of these examples would be grouped into a single topic to     maintain context.</p> </li> </ul>"},{"location":"platform/definitions/#events","title":"Events","text":"<p>Events are a discrete occurrence of a thing that happens or takes place.</p> <p>For example:</p> <ul> <li> <p>Engine start, engine stop, warning light activated.</p> </li> <li> <p>Game started, match made, kill made, player won the race, lap     completed, track limits exceeded, task completed.</p> </li> <li> <p>Takeoff, landing, missile launched, fuel low, autopilot engaged,     pilot ejected.</p> </li> </ul> <p>Events are typically things that occur less frequently. They are streamed into the same topics as their respective parameters and act to provide some context to what is happening.</p> <p>Start and stop events mark the beginning and end of data streams.</p>"},{"location":"platform/definitions/#metadata","title":"Metadata","text":"<p>Metadata describes additional information or context about a stream.</p> <p>For example:</p> <ul> <li> <p>License plate number, car manufacturer, car model, car engine type,     driver ID,</p> </li> <li> <p>Game version, player name, game session type, game session settings,     race car set-up</p> </li> <li> <p>Flight number, destination, airport of origin, pilot ID, airplane     type</p> </li> </ul> <p>Metadata typically has no time context, rather it exists as a constant throughout one or more streams. For example, your metadata could be the configuration of a car that is sold from a dealership (such as engine size, transmission type, wheel size, tyre model etc); you could create a stream every time that car is driven by the owner, but the engine size and transmission type won\u2019t change.</p> <p>Metadata is key to data governance and becomes very useful in down-stream data processing and analytics.</p>"},{"location":"platform/definitions/#binary-data","title":"Binary data","text":"<p>Quix also supports any binary blob data.</p> <p>With this data you can stream, process and store any type of audio, image, video or lidar data, or anything that isn\u2019t supported with our parameter, event or metadata types.</p>"},{"location":"platform/definitions/#project","title":"Project","text":"<p>A set of code which can be edited, compiled, executed and deployed as one Docker image.</p>"},{"location":"platform/definitions/#online-ide","title":"Online IDE","text":"<p>We provide an online integrated development environment for python projects. When you open any python project, you will see the Run button and a console during runtime in addition to the intellisense for python files.</p>"},{"location":"platform/definitions/#deployment","title":"Deployment","text":"<p>An instance of a Project running in the serverless environment.</p>"},{"location":"platform/definitions/#service","title":"Service","text":"<p>Any application code that is continuously running in the serverless environment. For example, a bridge, a function, a backend operation, or an integration to a third party service like Twilio.</p>"},{"location":"platform/definitions/#job","title":"Job","text":"<p>Any application code that is run once. For example, use a job to run a batch import of data from an existing data store (CSV, DB or DataLake etc).</p>"},{"location":"platform/definitions/#quix-streams","title":"Quix Streams","text":"<p>Quix Streams is the main client library we use to send and receive real-time data in our streaming applications.</p>"},{"location":"platform/definitions/#apis","title":"APIs","text":""},{"location":"platform/definitions/#streaming-writer-api","title":"Streaming Writer API","text":"<p>A HTTP API used to send telemetry data from any source to a topic in the Quix platform. It should be used when it is not possible to use directly our client library.</p>"},{"location":"platform/definitions/#streaming-reader-api","title":"Streaming Reader API","text":"<p>A WebSockets API used to stream any data directly from a topic to an external application. Most commonly used to read the results of a model or service to a real-time web application.</p>"},{"location":"platform/definitions/#data-catalogue-api","title":"Data Catalogue API","text":"<p>An HTTP API used to query historic data in the Data Catalogue. Most commonly used for dashboards, analytics and training ML models. Also useful to call historic data when running an ML model or to call historic data from an external application.</p>"},{"location":"platform/definitions/#portal-api","title":"Portal API","text":"<p>An HTTP API used to interact with most portal-related features such as creation of Workspaces, Users, Deployments, etc.</p>"},{"location":"platform/intro/","title":"What is Quix?","text":"<p>Quix is a platform for developing and deploying applications with streaming data.</p> <p>We architected Quix natively around a message broker (specifically Kafka) because we know databases are in the way of building low-latency applications that scale cost-effectively. Instead of working with data on a disk, developers could work with live data in-memory, if broker technologies were easier to use.</p> <p>But they are not easy to use, especially for Python developers who are at the forefront of data science but cannot easily work with streaming data.</p> <p>Quix provides everything a developer needs to build applications with streaming data. By using Quix you can build new products faster whilst keeping your data in-memory, helping to achieve lower latencies and lower operating costs.</p> <p>From the top-down, our stack provides a Web UI, APIs and Quix Streams that abstract developers off our underlying infrastructure, including fully-managed Kafka topics, serverless compute environment and a metadata-driven data catalogue (time-series database with steroids).</p> <p></p>"},{"location":"platform/intro/#web-ui","title":"Web UI","text":"<p>With the Quix Portal we are striving to make a beautiful software experience that facilitates DevOps/MLOps best-practices for less-experienced development teams. Our goals are to:</p> <ol> <li> <p>Help less expert people access live data</p> </li> <li> <p>Help them create and manage complex infrastructure and write     application code without support from expert engineering teams, and</p> </li> <li> <p>Help to accelerate the development lifecycle by enabling developers     to test and iterate code in an always-live environment.</p> </li> </ol> <p>To achieve these goals Quix Portal includes the following features:</p> <ul> <li> <p>Online IDE: Develop and Run your streaming applications directly     on the browser without setting up a local environment.</p> </li> <li> <p>Library: Choose between hundreds of autogenerated code examples     ready to run and deploy from our Online IDE.</p> </li> <li> <p>One click deployments: Deploy and manage your streaming     applications on production with a simple user interface.</p> </li> <li> <p>Monitoring tools: Monitor in real-time the status and the data     flow of your streaming applications.</p> </li> <li> <p>Broker management: Create, Delete, Explore or Configure your     message broker infrastructure with just a click of a button.</p> </li> <li> <p>Pipeline view: Visualize your pipeline architecture with the     information provided from the deployment variables.</p> </li> <li> <p>Data Explorer: Explore Live and Historical data of your     applications to test that your code is working as expected.</p> </li> </ul>"},{"location":"platform/intro/#apis","title":"APIs","text":"<p>We have provided four APIs to help you work with streaming data. These include:</p> <p>Stream Writer API: helps you send any data to a Kafka topic in Quix using HTTP. This API handles encryption, serialization and conversion to the Quix Streams format ensuring efficiency and performance of down-stream processing regardless of the data source.</p> <p>Stream Reader API: helps you push live data from a Quix topic to your application ensuring super low latency by avoiding any disk operations.</p> <p>Data Catalogue API: lets you query historic data streams in the data catalogue to train ML models, build dashboards and export data to other systems.</p> <p>Portal API: lets you automate Portal tasks like creating workspaces, topics and deployments.</p>"},{"location":"platform/intro/#quix-streams","title":"Quix Streams","text":"<p>Python is the dominant language for data science and machine learning, but it is quite incompatible with streaming technologies (like Kafka) which are predominantly written in Java and Scala.</p> <p>Our Quix Streams is a client library that abstracts Python developers off streaming-centric complexities like learning Java or dealing with buffering, serialization and encryption.</p> <p>Instead, Quix Streams serves you streaming data in a data frame so you can write any simple or complex data processing logic and connect it directly to the broker. There are just a few key streaming concepts that you must learn. You can read about them here.</p>"},{"location":"platform/intro/#serverless-compute","title":"Serverless compute","text":"<p>Quix provides an easy way to run code in an elastic serverless compute environment. It automatically builds code in GIT into a docker image and deploys containers to Kubernetes. This otherwise very complicated procedure is done by a couple of clicks in the Quix web portal.</p>"},{"location":"platform/intro/#architecture","title":"Architecture","text":""},{"location":"platform/intro/#git-integration","title":"Git integration","text":"<p>Source code for workspace projects (models, connectors and services) is hosted in GIT repositories. Developers can check out repositories and develop locally and collaborate using GIT protocol. Code is deployed to the Quix serverless environment using tags in GIT. Quix builds service will build selected GIT commit into a docker image.</p>"},{"location":"platform/intro/#docker-integration","title":"Docker integration","text":"<p>Each code example generated using the Quix library is shipped with a <code>Dockerfile</code> that is designed to work in the Quix serverless compute environment powered by Kubernetes. You can alter this file if necessary. When you deploy a service with Quix, a code reference to GIT with a build request is sent to the build queue. The build service will build a docker image and save it in the docker registry. In the next step, this image is deployed to Kubernetes.</p> <p>Tip</p> <p>If there is any problem with the docker build process, you can check the build logs.</p> <p>Tip</p> <p>Hover over the deployment in the deployments page to download the docker image of the deployed service for local testing or custom deployment.</p>"},{"location":"platform/intro/#kubernetes-integration","title":"Kubernetes integration","text":"<p>Quix manages an elastic compute environment so you don\u2019t need to worry about servers, nodes, memory, CPU, etc. Quix will make sure that your container is deployed to the right server in the cluster.</p> <p>We provide the following integrations with Kubernetes:</p> <ul> <li> <p>Logs from container accessible in the portal or via portal API.</p> </li> <li> <p>Environment variables allows passing variables into the docker     image deployment. So code can be parameterized.</p> </li> <li> <p>Replica number for horizontal scale.</p> </li> <li> <p>CPU limit.</p> </li> <li> <p>Memory limit.</p> </li> <li> <p>Deployment type - Options of one-time job or continuously     running service,</p> </li> <li> <p>Ingress - Optional ingress mapped to port 80.</p> </li> </ul> <p>Tip</p> <p>If a deployment reference is already built and deployed to a service, the build process is skipped and the docker image from the container registry is used instead.</p>"},{"location":"platform/intro/#dns-integration","title":"DNS integration","text":"<p>The Quix serverless environment offers DNS routing for services on port 80. That means that any API or frontend can be hosted in Quix with no extra complexity. Load balancing is provided out of the box, just increase the replica count to provide resiliency to your deployed API or frontend.</p> <p>Warning</p> <p>A newly deployed service with DNS routing takes up to 10 minutes to propagate to all DNS servers in the network.</p>"},{"location":"platform/intro/#managed-kafka-topics","title":"Managed Kafka topics","text":"<p>Quix provides fully managed Kafka topics which are used to stream data and build data processing pipelines by daisy-chaining models together.</p> <p>Our topics are multi-tenant which means you don\u2019t have to build and maintain an entire cluster to stream a few bytes of data. Instead, you can start quickly and cheaply by creating one topic for your application and only pay for the resources consumed when streaming that data. When your solution grows in data volume or complexity you can just add more topics without concern for the underlying infrastructure which is handled by us.</p> <p>Together with our client library and serverless compute, you can connect your models directly to our topics to read and write data using the pub/sub pattern. This keeps the data in-memory to deliver low-latency and cost effective stream processing capabilities.</p> <p>Note</p> <p>Quix also provides the ability to connect external infrastructure components like your own message broker infrastructure.</p>"},{"location":"platform/intro/#data-catalogue","title":"Data Catalogue","text":"<p>We provide a data catalogue for long-term storage, analytics and data science activities.</p> <p>We have combined what we know to be the best database technologies for each data type into a unified catalogue. There\u2019s a timeseries database for recording your events and parameter values, blob storage for your binary data, and a NoSQL DB for recording your metadata.</p> <p>Our data catalogue technology has two advantages:</p> <ol> <li> <p>It allocates each data type to the optimal database technology for     that type. This increases read/write and query performance which     reduces operating costs.</p> </li> <li> <p>It uses your metadata to record your context. This makes your data     more usable for more people across your organization who only need     to know your business context to navigate vast quantities of data.</p> </li> </ol>"},{"location":"platform/intro/#in-memory-processing","title":"In-memory processing","text":"<p>Traditional architectures for applications that need to process data have always been very database-centric. This means that, when you needed to process data and get some value out of it, everything had to pass through a database several times. This approach worked when the amount of data to process was relatively low, and the latency needed was on the scale of \"days\". But with a world changing to more real-time use cases where you need results on the scale of seconds or nanoseconds, and where you can get millions of IoT devices sending data to process at the same time, traditional database-centric architectures don't scale.</p> <p></p> <p>Quix uses a message broker and it puts it at the very center of the application, enabling a new approach for processing data without the need to save and pass all the information through a database. By using in-memory processing, you can persist only the data you're really interested in keeping.</p> <p></p> <p>This approach lowers the complexity and cost of real-time data processing by several orders of magnitude and, in fact, it is the only possible approach when you need to process a huge amount of data per second with low latency requirements.</p>"},{"location":"platform/connectors/","title":"Connectors","text":"<p>Connectors are part of our open source repository of samples, examples and integrations.</p> <p>Connectors help our users connect with other vendors such as AWS and Kafka.</p> <p>You can explore the connector README files here in Quix Docs. When you are ready to start using them, head over to the Quix Library GitHub repository, or sign up and login to the platform.</p>"},{"location":"platform/how-to/create-dlq/","title":"Create a Dead Letter Queue","text":"<p>When your code identifies a dead letter, or unprocessable message, simply send the message to the dead letter topic and continue processing other messages.</p> <p>What you do with messages in the dead letter topic is up to you.</p>"},{"location":"platform/how-to/create-dlq/#example","title":"Example","text":"<p>Take a look at the short example. It describes how to open topics, create streams and also send messages to a dedicated dead letter topic or queue.</p> PythonC# <pre><code># open topics\ninput_topic = client.open_input_topic('INPUT_DATA')\noutput_topic = client.open_output_topic('OUTPUT_DATA')\ndead_letter_topic = client.open_output_topic('UNPROCESSABLE')\n# create streams\noutput_stream = output_topic.create_stream()\ndlq_stream = dead_letter_topic.create_stream()\n# open the input stream\n# and start handling messages\ndef on_stream_received_handler(new_stream: StreamReader):\ndef on_parameter_data_handler(data_in: ParameterData):\ntry:\n# get a data value\ndata_to_process = data_in.timestamps[0].parameters['ParameterA'].numeric_value\n# prepare the data packet for onward processing\ndata_out = ParameterData()\ndata_out.add_timestamp_nanoseconds(1) \\\n                                .add_value(\"Speed\", data_to_process)\n# it was ok so pass to the output stream\n# to be processed by the next stage in the pipeline\noutput_stream.parameters.write(data_out)\nexcept Exception:\n# There was an error during processing.\n# Print the error and forward data into dead letter queue.\nprint(traceback.format_exc())\ndlq_stream.parameters.write(data_in)\n# hook up on read handler\nnew_stream.on_read += on_parameter_data_handler\ninput_topic.on_stream_received += on_stream_received_handler\ninput_topic.start_reading()\n</code></pre> <pre><code>var inputTopic = client.OpenInputTopic(\"INPUT_DATA\");\nvar outputTopic = client.OpenOutputTopic(\"OUTPUT_DATA\");\nvar deadLetterTopic = client.OpenOutputTopic(\"UNPROCESSABLE\");\n// create streams\nvar outputStream = outputTopic.CreateStream();\nvar dlqStream = deadLetterTopic.CreateStream();\n// open the input stream\ninputTopic.OnStreamReceived += (s, streamReader) =&gt;\n{\nstreamReader.Parameters.OnRead += dataIn =&gt;\n{\ntry\n{\n// get the data to process\nvar numValue = dataIn.Timestamps[0].Parameters['ParameterA'].NumericValue;\n// create the data for onward processing\nvar data = new ParameterData();\ndata.AddTimestampNanoseconds(1)\n.AddValue(\"Speed\", numValue);\n// it was ok so pass to the output stream\n// to be processed by the next stage in the pipeline\noutputStream.Parameters.Write(data);\n}\ncatch (Exception e)\n{\n// There was an error during processing.\n// Print the error and forward data into dead letter queue.\nSystem.Console.WriteLine(e);\ndlqStream.Parameters.Write(dataIn);\n}\n};\n};\ninputTopic.StartReading();\n</code></pre>"},{"location":"platform/how-to/deploy-public-page/","title":"How to deploy a public service","text":"<p>The Quix SaaS platform allows you to deploy public-facing web pages and APIs.</p> <p>This how-to will help to explain the features and options and ensure projects containing public facing web pages and APIs are successful.</p>"},{"location":"platform/how-to/deploy-public-page/#library-samples","title":"Library samples","text":"<p>In our Library you can find our <code>Web API Template</code> which demonstrates how to create a API using <code>Node</code>.</p> <p>There are also examples dashboard and web/UI examples using <code>Dash</code>, <code>Streamlit</code> and <code>Angular</code>.</p> <p></p>"},{"location":"platform/how-to/deploy-public-page/#the-code","title":"The code","text":"<p>When running locally it is usual to see local addresses like <code>127.0.0.1</code> and <code>localhost</code>, however, these cannot be used and will result in routing errors.</p> <p>Please ensure your service is hosted on <code>0.0.0.0</code> and uses port <code>80</code>.</p> <p>Examples of this for various languages are:</p> <ul> <li> <p>Node.js which can be used to host Dash/Plotly apps and any JavaScript-based websites:     <pre><code>app.run_server(debug=False, host=\"0.0.0.0\", port=80)\n</code></pre></p> </li> <li> <p>For Streamlit (just set the port in <code>main.py</code>)     <pre><code>sys.argv = [\"streamlit\", \"run\", \"streamlit_file.py\", \"--server.port=80\"]\n</code></pre></p> </li> </ul>"},{"location":"platform/how-to/deploy-public-page/#deploy-a-public-service","title":"Deploy a public service","text":"<p>To access your public facing service or web site you must enable <code>Public Access</code> at deployment time.</p> <ol> <li> <p>Switch on <code>Public Access</code>.</p> </li> <li> <p>If needed adjust the <code>URL prefix</code> to suit your needs.</p> </li> </ol> <p></p>"},{"location":"platform/how-to/deploy-public-page/#security","title":"Security","text":"<p>Please note that the basic examples, included in our library, do not include any security features and come with no warranty.</p> <p>Quix advises you to build in a security layer to ensure your data is secure and only the intended recipients have access to it.</p> <p>Read more about Python API security here and here</p> <p>Find out more here, if you're interested in securing an Angular application.</p>"},{"location":"platform/how-to/get-workspace-id/","title":"Get Workspace ID","text":"<p>Occasionally, you\u2019ll need to obtain an ID based on a specific workspace. For example, endpoints for the Data Catalogue API use a domain with the following pattern:</p> <pre><code>https://telemetry-query-${workspace-id}.platform.quix.ai/\n</code></pre> <p>The workspace ID is a combination of your organization and workspace names, converted to URL friendly values. The easiest way to get hold of it is as follows:</p> <ol> <li> <p>Go to the Portal home.</p> </li> <li> <p>Locate the workspace you\u2019re interested in and click OPEN.</p> </li> <li> <p>At this point, take note of the URL. It should be in the form:</p> </li> </ol> <pre><code>https://portal.platform.quix.ai/home?workspace=**{workspace-id}**\n</code></pre> <p>Copy that value and use it wherever you need a workspace ID.</p>"},{"location":"platform/how-to/jupyter-nb/","title":"Use Jupyter notebooks","text":"<p>In this article, you will learn how to use Jupyter Notebook to analyse data persisted in the Quix platform</p>"},{"location":"platform/how-to/jupyter-nb/#why-this-is-important","title":"Why this is important","text":"<p>Although Quix is a realtime platform, to build realtime in-memory models and data processing pipelines, we need to understand data first. To do that, Quix offers a Data catalogue that makes data discovery and analysis so much easier.</p>"},{"location":"platform/how-to/jupyter-nb/#preparation","title":"Preparation","text":"<p>You\u2019ll need some data stored in the Quix platform. You can use any of our Data Sources available in the samples Library, or just follow the onboarding process when you sign-up to Quix.</p> <p>You will also need Python 3 environment set up in your local environment.</p> <p>Install Jupyter notebooks as directed here.</p>"},{"location":"platform/how-to/jupyter-nb/#create-a-new-notebook-file","title":"Create a new notebook file","text":"<p>You can now run jupyter from the Windows start menu or with the following command in an Anaconda Powershell Prompt, or the equivalent for your operating system.</p> <pre><code>jupyter notebook\n</code></pre> <p>Then create a new Python3 notebook</p> <p></p>"},{"location":"platform/how-to/jupyter-nb/#connecting-jupyter-notebook-to-data-catalogue","title":"Connecting Jupyter notebook to Data Catalogue","text":"<p>The Quix web application has a python code generator to help you connect your Jupyter notebook with Quix.</p> <p>You need to be logged into the platform for this:</p> <ol> <li> <p>Select workspace (you likely only have one)</p> </li> <li> <p>Go to the Data Explorer</p> </li> <li> <p>Add a query to visualize some data. Select parameters, events, aggregation and time range</p> </li> <li> <p>Select the Code tab</p> </li> <li> <p>Ensure Python is the selected language</p> </li> </ol> <p></p> <p>Copy the Python code to your Jupyter notebook and execute.</p> <p></p> <p>Tip</p> <p>If you want to use this generated code for a long time, replace the temporary token with PAT token. See authenticate your requests how to do that.</p>"},{"location":"platform/how-to/jupyter-nb/#too-much-data","title":"Too much data","text":"<p>If you find that the query results in more data than can be handled by Jupyter Notebooks try using the aggregation feature to reduce the amount of data returned.</p> <p>For more info on aggregation check out this short video.</p>"},{"location":"platform/how-to/use-sdk-token/","title":"Using an SDK token","text":"<p>SDK token is a type of bearer token that can be used to authenticate against some of our APIs to access functionality necessary for streaming actions. Think of SDK tokens like a token you use to access portal but very limited in scope.</p> <p>Each workspace comes with two of these tokens, limited in use for that specific workspace. We call them <code>Token 1</code> and <code>Token 2</code>, also known as <code>Current</code> and <code>Next</code> token.</p>"},{"location":"platform/how-to/use-sdk-token/#how-to-find","title":"How to find","text":"<p>You can access these tokens by going to your topics and clicking on broker settings.</p>"},{"location":"platform/how-to/use-sdk-token/#how-to-use","title":"How to use","text":"<p>These tokens can be used to authenticate against the API, but their primary intended use is to be used with the client library. When using it with QuixStreamingClient, you no longer need to provide all broker credentials manually, they\u2019ll be acquired when needed and set up automatically.</p> <p>When deploying or running an online IDE, among other environment variables <code>Quix__Sdk__Token</code> is injected with value of <code>Token 1</code>.</p> <p>You should always use <code>Token 1</code>, unless you\u2019re rotating.</p> <p>Caution</p> <p>Your tokens do not have an expiration date. Treat them as you would a password. If you think they\u2019re exposed, rotate them.</p>"},{"location":"platform/how-to/use-sdk-token/#rotating","title":"Rotating","text":"<p>Having two keys lets you update your services without interruption, as both <code>Token 1</code> and <code>Token 2</code> are always valid. Rotating deactivates <code>Token 1</code>, <code>Token 2</code> takes its place and a new <code>Token 2</code> will be generated.</p> <p>You have two main options regarding how you rotate.</p> <p>The easiest way to rotate comes with some service downtime. This assumes you do not directly set the token for your QuixStreamingClient, instead you let the platform take care of it for you by using the default <code>Quix__Sdk__Token</code> environment variable. In this scenario all you have to do is rotate keys, stop and start all your deployments. Until a service is restarted it\u2019ll try to communicate with the platform using the deactivated token. If you\u2019re using local environments, those need updating manually.</p> <p>The alternative option is a bit more labour intense, but you can achieve no downtime. This requires you to set a new environment variable you control. This should point to the token to be used. Provide the value of this environment variable to QuixStreamingClient by passing it as an argument. Once you have that, set the value of this environment variable to <code>Token 2</code> and start your services. When you\u2019re sure you replaced the tokens for all services, rotate your keys.</p> <p>Note</p> <p>Only users with Admin role can rotate.</p>"},{"location":"platform/how-to/webapps/","title":"Develop web applications with Quix","text":"<p>Quix can bring real-time web functionality to you client applications. Following types of applications are good candidates to use Quix as their data plane.</p> <ul> <li> <p>Dashboard and real-time monitoring applications that show updates as     they happen to users like cloud/edge monitoring tools.</p> </li> <li> <p>Applications that require data to be pushed from a backend at high     frequency like games and simulations.</p> </li> <li> <p>Social networking applications that require broadcasting updates to     many users at high frequency like live sharing of Strava data.</p> </li> </ul>"},{"location":"platform/how-to/webapps/#nodejs","title":"NodeJs","text":"<p>NodeJs applications can update parameter and event definitions and write data to streams using RESTful APIs. Quix supports WebSockets for clients that want to receive telemetry data and parameters/events updates in real-time. NodeJs clients must authenticate with Quix using personal access tokens.</p>"},{"location":"platform/how-to/webapps/read/","title":"Read from Quix with NodeJs","text":"<p>Quix supports real-time data streaming over WebSockets. JavaScript clients can receive updates on parameter and event definition updates, parameter data and event data as they happen. Following examples use SignalR client library to connect to Quix over WebSockets.</p>"},{"location":"platform/how-to/webapps/read/#setting-up-signalr","title":"Setting up SignalR","text":"<p>If you are using a package manager like npm, you can install SignalR using <code>npm install @microsoft/signalr</code>. For other installation options that don\u2019t depend on a platform like Node.js such as consuming SignalR from a CDN please refer to SignalR documentation.</p> <p>Following code snippet shows how you can connect to Quix after SignalR has been setup.</p> <pre><code>var signalR = require(\"@microsoft/signalr\");\nconst options = {\naccessTokenFactory: () =&gt; 'your_access_token'\n};\nconst connection = new signalR.HubConnectionBuilder()\n.withUrl(\"https://reader-your-workspace-id.portal.quix.ai/hub\", options)\n.build();\nconnection.start().then(() =&gt; console.log(\"SignalR connected.\"));\n</code></pre> <p>If the connection is successful, you should see the console log \"SignalR connected\".</p>"},{"location":"platform/how-to/webapps/read/#reading-data-from-a-stream","title":"Reading data from a stream","text":"<p>Before you can read data from a stream, you need to subscribe to an event like parameter definition, event definition, parameter data or event data.</p> <p>Following is an example of establishing a connection to Quix, subscribing to a parameter data stream, reading data from that stream, and unsubscribing from the event using a SignalR client.</p> <pre><code>var signalR = require(\"@microsoft/signalr\");\nconst options = {\naccessTokenFactory: () =&gt; 'your_access_token'\n};\nconst connection = new signalR.HubConnectionBuilder()\n.withUrl(\"https://reader-your-workspace-id.portal.quix.ai/hub\", options)\n.build();\n// Establish connection\nconnection.start().then(() =&gt; {\nconsole.log(\"Connected to Quix.\");\n// Subscribe to parameter data stream.\nconnection.invoke(\"SubscribeToParameter\", \"your-topic-name\", \"your-stream-id\", \"your-parameter-id\");\n// Read data from the stream.\nconnection.on(\"ParameterDataReceived\", data =&gt; {\nlet model = JSON.stringify(data);\nconsole.log(\"Received data from stream: \" + model);\n// Unsubscribe from stream.\nconnection.invoke(\"UnsubscribeFromParameter\", \"your-topic-name\", \"your-stream-id\", \"your-parameter-id\");\n});\n});\n</code></pre> <p>Following is a list of subscriptions available for SignalR clients.</p> <ul> <li> <p><code>SubscribeToParameter(topicName, streamId, parameterId)</code>: Subscribes     to a parameter data stream. Use <code>UnsubscribeFromParameter(topicname,     streamId, parameterId)</code> to unsubscribe.</p> </li> <li> <p><code>SubscribeToParameterDefinitions(topicName, streamId)</code>: Subscribes     to parameter definition updates.</p> </li> <li> <p><code>SubscribeToEvent(topicName, streamId, eventId)</code>: Subscribes to an     event data stream. Use <code>UnsubscribeFromEvent(topicName, streamId,     eventId)</code> to unsubscribe.</p> </li> <li> <p><code>SubscribeToEventDefinitions(topicName, streamId)</code>: Subscribes to     event definition updates.</p> </li> <li> <p><code>UnsubscribeFromStream(topicName, streamId)</code>: Unsubscribes from all     subscriptions of the specified stream.</p> </li> </ul> <p>Following is a list of SignalR events supported by Quix and their payloads.</p> <ul> <li><code>ParameterDataReceived</code>: Add a listener to this event to receive     parameter data from a stream. Following is a sample payload for this     event.</li> </ul> <pre><code>{\ntopicName: 'topic-1',\nstreamId: 'b45969d2-4624-4ab7-9779-c8f90ce79420',\ntimestamps: [ 1591733989000000000, 1591733990000000000, 1591733991000000000 ],\nnumericValues: { ParameterA: [ 1, 2, 3 ] },\nstringValues: {},\ntagValues: { ParameterA: [ null, null, 'tag-1' ] }\n}\n</code></pre> <ul> <li><code>ParameterDefinitionsUpdated</code>: Add a listener to this event to     receive data from <code>SubscribeToParameterDefinitions</code> subscription.     Following is a sample payload of this event.</li> </ul> <pre><code>{\ntopicName: 'topic-1',\nstreamId: 'b45969d2-4624-4ab7-9779-c8f90ce79420',\ndefinitions: [\n{\nId: 'ParameterA',\nName: 'Parameter A',\nDescription: 'Description of parameter A',\nMinimumValue: null,\nMaximumValue: 100.0,\nUnit: 'kmh',\nCustomProperties: null,\nLocaltion: '/car/general'\n}\n]\n}\n</code></pre> <ul> <li><code>EventDataReceived</code>: Add a listener to this event to receive data     from <code>SubscribeToEvent</code> subscription. Following is a sample payload     of this event.</li> </ul> <pre><code>{\ntopicName: 'topic-1',\nstreamId: 'b45969d2-4624-4ab7-9779-c8f90ce79420'\nid: 'EventA',\ntimestamp: 1591733990000000000,\nvalue: 'val-a',\ntags: {\ntag1: 'val1'\n}\n}\n</code></pre>"},{"location":"platform/how-to/webapps/write/","title":"Write to Quix with NodeJs","text":"<p>Clients write data to Quix using streams opened on existing topics. Therefore, you need to first create a topic in the Portal to hold your data streams.</p> <p>Once you have a topic, your clients can start writing data to Quix by</p> <ul> <li> <p>creating a stream in your topic</p> </li> <li> <p>sending data to that stream</p> </li> <li> <p>closing the stream</p> </li> </ul>"},{"location":"platform/how-to/webapps/write/#creating-a-stream","title":"Creating a stream","text":"<p>To write data to Quix, you need to open a stream to your topic. Following is an example of creating a stream using JavaScript and Node.js.</p> <pre><code>const https = require(https);\nconst data = JSON.stringify({\nName: \"Your Stream Name\",\nLocation: \"your/location\",\nMetadata: {\nProperty1: \"Value 1\",\nProperty2: \"Value 2\"\n},\nParents = [\"parent-stream-1\", \"parent-stream-2\"],\nTimeOfRecording: \"2021-02-06T00:15:15Z\"\n});\nconst options = {\nhostname: 'your-workspace-id.portal.quix.ai',\npath: '/topics/your-topic-name/streams',\nmethod: 'POST',\nheaders: {\n'Authorization': 'Bearer your_access_token',\n'Content-Type': 'application/json'\n}\n};\nconst req = https.request(options, res =&gt; {\nres.on('data', d =&gt; {\nlet json = JSON.parse(d);\nlet streamId = json.streamId;\n});\n});\nreq.write(data);\nreq.end();\n</code></pre> <p>Upon completing the request successfully, you will receive the stream id in the response body. You are going to need this stream id when you are writing data to the stream.</p> <p>In the request data, <code>Location</code> is also an optional, but an important property. Location allows you to organize your streams under directories in the Data Catalogue.</p> <p>When you are creating the stream, you can add optional metadata about the stream to the stream definition like <code>Property1</code> and <code>Property2</code> in the preceding example.</p> <p>Field <code>Parents</code> is also optional. If the current stream is derived from one or more streams (e.g. by transforming data from one stream using an analytics model), you can reference the original streams using this field.</p> <p><code>TimeOfRecording</code> is an optional field that allows you to specify the actual time the data was recorded. This field is useful if you are streaming data that was recorded in the past.</p>"},{"location":"platform/how-to/webapps/write/#writing-parameter-data-to-a-stream","title":"Writing parameter data to a stream","text":"<p>After you have created the stream, you can start writing data to that stream using the following HTTP request.</p> <pre><code>const https = require(https);\nconst data = JSON.stringify({\nTimestamps: [1591733989000000000, 1591733990000000000, 1591733991000000000],\nNumericValues: {\n\"ParameterA\": [1, 2, 3],\n\"ParameterB\": [5, 8, 9]\n},\nStringValues: {\n\"ParameterC\": [\"hello\", \"world\", \"!\"]\n},\nBinaryValues: {\n\"ParameterD\": [\nBuffer.from(\"hello\").toString('base64'),\nBuffer.from(\" Quix\").toString('base64'),\nBuffer.from(\"!\").toString('base64')\n]\n}\nTagValues: {\n\"ParameterA\": [null, null, \"tag-1\"]\n}\n});\nconst options = {\nhostname: 'your-workspace-id.portal.quix.ai',\npath: '/topics/your-topic-name/streams/your-stream-id/parameters/data',\nmethod: 'POST',\nheaders: {\n'Authorization': 'Bearer your_access_token',\n'Content-Type': 'application/json'\n}\n};\nconst req = https.request(options, res =&gt; {\nconsole.log(`Status Code: ${res.statusCode}`);\n});\nreq.write(data);\nreq.end();\n</code></pre> <p>In the preceding example, <code>data</code> has two different parameter types, numeric and strings. If your data only contains numeric data, you do not need to include the <code>StringValues</code> property. In the case of binary values, the items in the array must be a base64 encoded string.</p> <p><code>TagValues</code> is another optional field in the data request that allows you to add context to data points by means of tagging them. Index of the <code>Timestamps</code> array is used when matching the parameter data values as well as tag values. Therefore, the order of the arrays is important.</p>"},{"location":"platform/how-to/webapps/write/#defining-parameters","title":"Defining parameters","text":"<p>In the above examples, parameters are created in Quix as you write data to the stream. However, what if you would like to add more information like acceptable value ranges, measurement units, etc. to your parameters? You can use the following HTTP request to update your parameter definitions.</p> <pre><code>const https = require(https);\nconst data = JSON.stringify([\n{\nId: \"ParameterA\",\nName: \"Parameter A\",\nDescription: \"Temperature measurements from unit 1234A\",\nMinimumValue: 0.0,\nMaximumValue: 100.0,\nUnit: \"\u00b0C\",\nCustomProperties: \"{\\\"OptimalMinimum\\\": 30.0, \\\"OptimalMaximum\\\": 50.0}\",\nLocation: \"/chassis/engine\"\n}\n]);\nconst options = {\nhostname: 'your-workspace-id.portal.quix.ai',\npath: '/topics/your-topic-name/streams/your-stream-id/parameters',\nmethod: 'PUT',\nheaders: {\n'Authorization': 'Bearer your_access_token',\n'Content-Type': 'application/json'\n}\n};\nconst req = https.request(options, res =&gt; {\nconsole.log(`Status Code: ${res.statusCode}`);\n});\nreq.write(data);\nreq.end();\n</code></pre> <p>In the preceding request, the <code>Id</code> must match the parameter id you set when writing data to the stream. <code>Name</code> allows you to set a more readable name for the parameter. You can also add a description, minimum and maximum values, unit of measurement to your parameter. <code>Location</code> allows you to organize/group your parameters in a hierarchical manner like with the streams. If you have a custom parameter definition that is not covered by the primary fields of the request, you can use <code>CustomProperties</code> field to add your custom definition as a string.</p>"},{"location":"platform/how-to/webapps/write/#writing-event-data-to-a-stream","title":"Writing event data to a stream","text":"<p>Writing event data to a stream is similar to writing parameter data using the web api. The main difference in the two requests is in the request body.</p> <pre><code>const data = JSON.stringify([\n{\nId: \"EventA\",\nTimestamp: 1591733989000000000,\nValue: \"Lap1\",\nTags: {\nTagA: \"val1\",\nTagB: \"val2\"\n}\n},\n{\nId: \"EventA\",\nTimestamp: 1591734989000000000,\nValue: \"Lap2\",\nTags: {\nTagA: \"val1\",\nTagB: \"val2\"\n}\n},\n{\nId: \"EventA\",\nTimestamp: 1591735989000000000,\nValue: \"Lap3\",\nTags: {\nTagA: \"val1\"\n}\n},\n]);\nconst options = {\nhostname: 'your-workspace-id.portal.quix.ai',\npath: '/topics/your-topic-name/streams/your-stream-id/events/data',\nmethod: 'POST',\nheaders: {\n'Authorization': 'Bearer your_access_token',\n'Content-Type': 'application/json'\n}\n};\nconst req = https.request(options, res =&gt; {\nconsole.log(`Status Code: ${res.statusCode}`);\n});\nreq.write(data);\nreq.end();\n</code></pre> <p>In the preceding example, tags in the event data request are optional. Tags add context to your data points and help you to execute efficient queries over them on your data like using indexes in traditional databases.</p>"},{"location":"platform/how-to/webapps/write/#defining-events","title":"Defining events","text":"<p>In the above examples, events are created in Quix as you write data to the stream. If you want to add more descriptions to your events, you can use event definitions api similar to parameter definitions to update your events.</p> <pre><code>const https = require(https);\nconst data = JSON.stringify([\n{\nId: \"EventA\",\nName: \"Event A\",\nDescription: \"New lap event\",\nCustomProperties: \"{\\\"Tarmac\\\": \\\"Open-graded\\\"}\",\nLocation: \"/drive/lap\",\nLevel: \"Information\"\n}\n]);\nconst options = {\nhostname: 'your-workspace-id.portal.quix.ai',\npath: '/topics/your-topic-name/streams/your-stream-id/events',\nmethod: 'PUT',\nheaders: {\n'Authorization': 'Bearer your_access_token',\n'Content-Type': 'application/json'\n}\n};\nconst req = https.request(options, res =&gt; {\nconsole.log(`Status Code: ${res.statusCode}`);\n});\nreq.write(data);\nreq.end();\n</code></pre> <p>In the preceding request, the <code>Id</code> must match the event id you set when writing events to the stream. <code>Name</code> allows you to set a more readable name for the event. <code>Location</code> allows you to organize/group your events in a hierarchy like with the parameters. If you have a custom event definition that is not covered by the primary fields of the request, you can use <code>CustomProperties</code> field to add your custom definition as a string. You can also set an optional event <code>Level</code>. Accepted event levels are Trace, Debug, Information, Warning, Error and Critical. Event level defaults to Information if not specified.</p>"},{"location":"platform/how-to/webapps/write/#closing-a-stream","title":"Closing a stream","text":"<p>After finishing sending data, you can proceed to close the stream using the request below.</p> <pre><code>const https = require(https);\nconst options = {\nhostname: 'your-workspace-id.portal.quix.ai',\npath: '/topics/your-topic-name/streams/your-stream-id/close',\nmethod: 'POST',\nheaders: {\n'Authorization': 'Bearer your_access_token',\n'Content-Type': 'application/json'\n}\n};\nconst req = https.request(options, res =&gt; {\nconsole.log(`Status Code: ${res.statusCode}`);\n});\nreq.end();\n</code></pre>"},{"location":"platform/samples/samples/","title":"Code Samples","text":"<p>The Quix Portal includes a Library of templates and sample projects that you can use to start working with the platform.</p> <p>Quix allows explore the samples and save them as a new Project and immediately Run or Deploy them. If you don\u2019t have a Quix account yet, go sign-up to Quix and create one.</p> <p>The backend of the Quix Library is handled by a public Open source repository on GitHub, so you can become a contributor of our Library generating new samples or updating existing ones.</p> <p></p>"},{"location":"platform/security/security/","title":"Security","text":"<p>This section describes the basic security features of Quix.</p>"},{"location":"platform/security/security/#data-in-flight","title":"Data in flight","text":""},{"location":"platform/security/security/#authentication","title":"Authentication","text":"<ul> <li> <p>Our APIs are authenticated using     OAuth 2.0 token. We     are using Auth0     as our provider.</p> </li> <li> <p>Each Kafka server is authenticated using certificate, which is     provided for each project created and can also be downloaded from     topics view. The client is authenticated using SASL (username,     password).</p> </li> </ul>"},{"location":"platform/security/security/#authorization","title":"Authorization","text":"<ul> <li> <p>The APIs is using RBAC. You are limited in what you can do based on     your token and the role configured for your user.</p> </li> <li> <p>Each kafka client is authrozied to only read and write to the topics     or query consumer group information regarding topics owned by the     organization the client belongs to.</p> </li> </ul>"},{"location":"platform/security/security/#encryption","title":"Encryption","text":"<ul> <li>All our APIs communicate with TLS 1.2</li> </ul>"},{"location":"platform/security/security/#data-at-rest","title":"Data at rest","text":"<ul> <li> <p>Your data is encrypted at rest using cloud provider (Azure) managed     keys.</p> </li> <li> <p>Your data is phyisically protected at our cloud provider\u2019s location.</p> </li> </ul>"},{"location":"platform/troubleshooting/troubleshooting/","title":"Troubleshooting","text":"<p>This section contains solutions, fixes, hints and tips to help you solve the most common issues encountered when using Quix.</p>"},{"location":"platform/troubleshooting/troubleshooting/#data-is-not-being-received-into-a-topic","title":"Data is not being received into a Topic","text":"<ul> <li> <p>Ensure the Topic Name or Id is correct in Topics option of Quix     Portal.</p> </li> <li> <p>You can check the data in / out rates on the Topics tab.</p> </li> <li> <p>If you want to see the data in the Data Catalogue please make sure     you are persisting the data to the Topic otherwise it may appear     that there is no data.</p> </li> <li> <p>If you are using a consumer group, check that no other services are     using the same group. If you run your code locally and deployed     somewhere and they are both using the same consumer group one of     them may consume all of the data.</p> </li> </ul>"},{"location":"platform/troubleshooting/troubleshooting/#topic-authentication-error","title":"Topic Authentication Error","text":"<p>If you see errors like these in your service or job logs then you may have used the wrong credentials or it could be that you have specified the wrong Topic Id.</p> <p>Authentication failed during authentication due to invalid credentials with SASL mechanism SCRAM-SHA-256 Exception receiving package from Kafka 3/3 brokers are down Broker: Topic authorization failed</p> <p>Check very carefully each of the details.</p> <p>The following must be correct:</p> <ul> <li> <p>TopicId or TopicName</p> </li> <li> <p>Sdk Token</p> </li> </ul> <p>These can all be found in Topics option of Quix Portal.</p>"},{"location":"platform/troubleshooting/troubleshooting/#broker-transport-failure","title":"Broker Transport Failure","text":"<p>If you have deployed a service or job and the logs mention broker transport failure then check the workspace name and password in the SecurityOptions.</p> <p>Also check the broker address list. You should have these by default:</p> <p>kafka-k1.quix.ai:9093,kafka-k2.quix.ai:9093,kafka-k3.quix.ai:9093</p>"},{"location":"platform/troubleshooting/troubleshooting/#401-error","title":"401 Error","text":"<p>When attempting to access the web APIs you may encounter a 401 error. Check that the bearer token is correct and has not expired. If necessary generate a new bearer token.</p> <p>Example of the error received when trying to connect to the Streaming Reader API with an expired bearer token</p> <p>signalrcore.hub.errors.UnAuthorizedHubError</p> <p>The APIs that require a valid bearer token are:</p> <ol> <li> <p>Portal API</p> <ul> <li>https://portal-api.platform.quix.ai/swagger/index.html</li> </ul> </li> <li> <p>Streaming Writer API</p> <ul> <li>https://writer-[YOUR_ORGANIZATION_ID]-[YOUR_WORKSPACE_ID].platform.quix.ai/index.html</li> </ul> </li> <li> <p>Telemetry Query API</p> <ul> <li>https://telemetry-query-[YOUR_ORGANIZATION_ID]-[YOUR_WORKSPACE_ID].platform.quix.ai/swagger/index.html</li> </ul> </li> </ol>"},{"location":"platform/troubleshooting/troubleshooting/#error-handling-in-the-client-library-callbacks","title":"Error Handling in the client library callbacks","text":"<p>Errors generated in the client library callback can be swallowed or hard to read. To prevent this and make it easier to determine the root cause you should use a traceback</p> <p>Begin by importing traceback</p> <pre><code>import traceback\n</code></pre> <p>Then, inside the client library callback where you might have an issue place code similar to this:</p> <pre><code>def read_stream(new_stream: StreamReader):\ndef on_parameter_data_handler(data: ParameterData):\ntry:\ndata.timestamps[19191919] # this does not exist\nexcept Exception:\nprint(traceback.format_exc())\nnew_stream.parameters.create_buffer().on_read += on_parameter_data_handler\ninput_topic.on_stream_received += read_stream\n</code></pre> <p>Notice that the try clause is within the handler and the except clause prints a formatted exception (below)</p> <pre><code>Traceback (most recent call last):\nFile \"main.py\", line 20, in on_parameter_data_handler\ndata.timestamps[19191919]\nFile \"/usr/local/lib/python3.8/dist-packages/quixstreaming/models/netlist.py\", line 22, in __getitem__\nitem = self.__wrapped[key]\nIndexError: list index out of range\n</code></pre>"},{"location":"platform/troubleshooting/troubleshooting/#service-keeps-failing-and-restarting","title":"Service keeps failing and restarting","text":"<p>If your service continually fails and restarts you will not be able to view the logs. Redeploy your service as a job instead. This will allow you to inspect the logs and get a better idea about what is happening.</p>"},{"location":"platform/troubleshooting/troubleshooting/#possible-dns-propagation-errors","title":"Possible DNS Propagation Errors","text":"<p>There are currently 2 scenarios in which you might encounter an issue caused by DNS propagation.</p> <ul> <li>1. Data catalogue has been deployed but DNS entries have not fully     propagated. In this scenario you might see a banner when accessing     the data catalogue.</li> </ul> <p></p> <ul> <li>2. A dashboard or other publicly visible deployment is not yet     accessible, again due to DNS propagation.</li> </ul> <p></p> <p>Tip</p> <p>In these scenarios simply wait while the DNS records propagate. It can take up to 10 minutes for DNS to records to propagate fully.</p>"},{"location":"platform/troubleshooting/troubleshooting/#python-version","title":"Python Version","text":"<p>If you get strange errors when trying to compile your Python code locally please check that you are using Python version 3.8</p> <p>For example you may encounter a ModuleNotFoundError</p> <pre><code>ModuleNotFoundError: No module named 'quixstreaming'\n</code></pre> <p>For information on how to setup your IDE for working with Quix please check out this section in the client library documentation.</p>"},{"location":"platform/troubleshooting/troubleshooting/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<p>If you are having trouble with Jupyter Notebooks or another consumer of Quix data try using aggregation to reduce the number of records returned.</p> <p>For more info on aggregation check out this short video.</p>"},{"location":"platform/troubleshooting/troubleshooting/#process-killed-or-out-of-memory","title":"Process Killed or Out of memory","text":"<p>If your deployment\u2019s logs report \"Killed\" or \"Out of memory\" then you may need to increase the amount of memory assigned to the deployment.</p> <p>You may experience this:</p> <ul> <li> <p>At build time if you want to load large third party packages into     your code</p> </li> <li> <p>At runtime if you are storing large datasets in memory.</p> </li> </ul>"},{"location":"platform/troubleshooting/troubleshooting/#missing-dependency-in-online-ide","title":"Missing Dependency in online IDE","text":"<p>Currently the online IDE does not use the same docker image as the one used for deployment due to time it would take to build it and make it available to you. (Likely feature for future however) Because of this you might have some OS level dependencies that you need to install from within your python code to be able to make use of the Run feature in the IDE. The section below should give you guidance how to achieve this.</p> <p>In your <code>main.py</code> (or similar) file, add as the first line: <code>import preinstall</code>. Now create the file <code>preinstall.py</code> and add content based on example below:</p> <ul> <li> <p>TA-Lib     This script will check if TA-Lib is already installed (like from     docker deployment). If not then installs it.</p> <pre><code>import os\nimport sys\nta_lib_pip_details = os.system(\"python3 -m pip show TA-Lib\")\nif ta_lib_pip_details == 0:\nprint(\"TA-Lib already installed\")\nelse:\nif os.system(\"apt-get update\") != 0:\nprint(\"Failed apt-get update\")\nsys.exit(1)\nif os.popen(\"if [ -e ta-lib-0.4.0-src.tar.gz ]; then echo \\\"ok\\\"; else echo \\\"nok\\\"; fi\").read().strip() == \"ok\":\nprint(\"TA-Lib already downloaded\")\nelse:\nprint(\"Downloading ta-lib\")\nif os.system(\"apt-get install curl -y\") != 0:\nprint(\"Failed apt-get install curl -y\")\nsys.exit(1)\nif os.system(\"curl https://jztkft.dl.sourceforge.net/project/ta-lib/ta-lib/0.4.0/ta-lib-0.4.0-src.tar.gz -O\") != 0:\nprint(\"Failed to download ta-lib\")\nsys.exit(1)\nzipmdsum = os.popen(\"md5sum ta-lib-0.4.0-src.tar.gz | cut -d ' ' -f 1\").read().strip()\nif zipmdsum == \"308e53b9644213fc29262f36b9d3d9b9\":\nprint(\"TA-Lib validated\")\nelse:\nprint(\"TA-Lib has incorrect hash value, can't trust it. Found hash: '\" + str(zipmdsum) + \"'\")\nsys.exit(1)\nif os.system(\"tar -xzf ta-lib-0.4.0-src.tar.gz\") != 0:\nprint(\"Failed to extract TA-Lib zip\")\nsys.exit(1)\nif os.system(\"apt-get install build-essential -y\") != 0:\nprint(\"Failed apt-get install build-essential -y\")\nsys.exit(1)\nos.chdir(os.path.abspath(\".\") + \"/ta-lib\")\nif os.system(\"./configure --prefix=/usr\") != 0:\nprint(\"Failed to configure TA-Lib for build\")\nsys.exit(1)\nif os.system(\"make\") != 0:\nprint(\"Failed to make TA-Lib\")\nsys.exit(1)\nif os.system(\"make install\") != 0:\nprint(\"Failed to make install TA-Lib\")\nsys.exit(1)\nprint(\"Installed dependencies for TA-Lib pip package\")\nif os.system(\"python3 -m pip install TA-Lib\") != 0:\nprint(\"Failed to pip install TA-Lib\")\nsys.exit(1)\nprint(\"Installed TA-Lib pip package\")\n</code></pre> </li> </ul> <p>With this, the first time you press Run, the dependency should install. Any subsequent run should already work without having to install.</p>"},{"location":"platform/tutorials/currency-alerting/currency-alerting/","title":"Currency alerting","text":"<p>In this tutorial you will learn how to build a real-time streaming pipeline that sends push notifications to your phone when the Bitcoin price reaches a certain threshold.</p>"},{"location":"platform/tutorials/currency-alerting/currency-alerting/#learning-objectives","title":"Learning objectives","text":"<p>In this tutorial you will learn:</p> <ul> <li>How to use an existing library item to interface Quix to a real-time event stream. In this tutorial this is a stream of trading data from CoinAPI.</li> <li>How to create a transformation. </li> <li>How to use an existing library item to interface Quix to a mobile device.</li> </ul>"},{"location":"platform/tutorials/currency-alerting/currency-alerting/#getting-help","title":"Getting help","text":"<p>If you need assistance with this tutorial, or have any general questions, please reach out to Quix, we\u2019ll be more than happy to help. We can be found on our public Slack channel, The Stream. Please sign up, and introduce yourself!</p>"},{"location":"platform/tutorials/currency-alerting/currency-alerting/#prerequisites","title":"Prerequisites","text":"<p>To complete this tutorial you will need the following accounts:</p> <ul> <li>Quix - You can sign up for a free account here. This enables you to create the real-time stream processing pipeline.</li> <li>CoinAPI - You can sign up for a free API key here. On the free tier, click the <code>GET A FREE API KEY</code> button, enter the requested information, and you will receive an email containing your API key. This enables you to access a stream of constantly updated BTC/USD exchange rate data.</li> <li>Pushover - You can sign up for a free account here. Enter your details, and click <code>Signup</code>. You will receive a welcome email, and you can log in to Pushover to retrieve your user key and generate an API token. This enables you to send notifications to your phone. Install the Pushover mobile app from the Apple App store or Google Play.</li> </ul>"},{"location":"platform/tutorials/currency-alerting/currency-alerting/#the-pipeline-you-will-create","title":"The pipeline you will create","text":"<p>The objective of this tutorial is to create a pipeline that resembles the following example:</p> <p></p> <p>The colors describe the role of the microservice that is being deployed. The possible roles are as follows:</p> Source \u2014 enables streaming of data into the Quix platform from any external source, such as an API or websocket. Transformation \u2014 implements the processing of data, for example, cleaning data or implementing a Machine Learning (ML) model.  Destination \u2014 enables streaming of processed data to an external destination, such as a database or dashboard."},{"location":"platform/tutorials/currency-alerting/currency-alerting/#setting-up-the-coinapi-source","title":"Setting up the CoinAPI source","text":"<p>In this section you will learn how to set up the source library item and deploy it in your pipeline as a microservice.</p> <p>This library item, when deployed as a microservice in the Quix pipeline, connects a live stream of updates for the currency pair: <code>BTC/USD</code>. This real-time exchange rate data is streamed in from the CoinAPI through its Websocket interface. The free sandbox version is used for the purposes of this tutorial. </p> <p>To summarize this functionality:</p> <ul> <li>The microservice streams the exchange rate data to a topic called <code>currency-exchange-rates</code>. </li> <li>Downstream microservices in the pipeline can then read fom the <code>currency-exchange-rates</code> topic and process it in different ways. In this tutorial, you will check the current Bitcoin price against a threshold.</li> </ul> <p>To set up the CoinAPI source, follow these steps:</p> <ol> <li> <p>In the Quix Portal, click the <code>Library</code> icon in the main left-hand navigation.</p> </li> <li> <p>In the search box on the library page, enter \"CoinAPI - Exchange Rate Feed\".</p> </li> </ol> <p>You will see the Coin API library item appear in the search results: </p> <ol> <li> <p>Click the <code>Preview code</code> button, and on the page that appears, click the <code>Edit code</code> button. When you choose to edit a library item, Quix prompts you to create a copy of it as a project, as library items are read-only.</p> <p>Optionally, you could have clicked the <code>Setup &amp; deploy</code> button, which would have deployed the microservice directly. However, in this tutorial, you are given the opportunity to first look at the code, and modify it if necessary.</p> </li> <li> <p>In the <code>Setup project</code> form, configure the following environment variables:</p> Field Value <code>Name</code> Enter a project name or keep the default suggestion. <code>output</code> Select the output topic. In this case, select <code>currency-exchange-rates</code> from the list. <code>coin_api_key</code> The API key that you use to access CoinAPI. <code>asset_id_base</code> The short code for the base currency that you want to track, for example BTC. <code>asset_id_quote</code> The short code for the target currency in which prices will be quoted, for example, USD. </li> <li> <p>Click <code>Save as project</code>. You now have a copy of the CoinAPI library item in your workspace.</p> </li> <li> <p>Click the <code>Deploy</code> button. The library item is deployed as a service and automatically started.</p> <p>Once the library item has been deployed, you\u2019ll be redirected to the workspace home page, where you can see the service in the pipeline context, as was illustrated previously.</p> </li> <li> <p>Click the CoinAPI service card to inspect the logs:</p> </li> </ol> <p></p> <p>A successful deployment will resemble the following example:</p> <p></p> <p>If there is an issue with the service, you can also inspect the <code>build logs</code> in the <code>Lineage</code> panel to check for any traces of a syntax error or other build issues.</p>"},{"location":"platform/tutorials/currency-alerting/currency-alerting/#setting-up-the-threshold-alert-transformation","title":"Setting up the Threshold Alert transformation","text":"<p>In this section you will learn how to implement a microservice that will compare data in the BTC/USD exchange rate stream against a specific price threshold. </p> <p>This microservice contains a simple algorithm that checks to see if a value has crossed a threshold in either direction.</p> <p>The data stream is transformed from a series of real-time price points into a series of price alerts.</p> <p>To summarize this functionality:</p> <ul> <li>When the threshold criteria are met, the microservice writes an alert message to a topic called <code>currency-rate-alerts</code>. </li> <li>Downstream services can then read fom this topic and send alerts and notifications whenever they detect a new message.</li> </ul> <p>To set up the Threshold Alert item, follow these steps:</p> <ol> <li>Click on the Library icon in the left-hand navigation.</li> <li> <p>In the search box on the library page, enter \"Threshold Alerts\".</p> <p>You will see the <code>Threshold Alert</code> library item appear in the search results: </p> <p></p> </li> <li> <p>Click the <code>Preview code</code> button, and on the page that appears, click the <code>Edit code</code> button.</p> </li> <li> <p>In the <code>Setup project</code> form, set the following environment variables:</p> Field Value <code>Name</code> As usual, enter a project name or keep the default suggestion. <code>input</code> Select the input topic. In this case, select <code>currency-exchange-rates</code> from the list. <code>output</code> Select the output topic. In this case, select <code>currency-rate-alerts</code> from the list. <code>parameterName</code> Set this to <code>PRICE</code>. <code>thresholdValue</code> The price in USD that you'd like to get alerted about. For example, on the day that this tutorial was written, BTC was hovering around $16,300 so we entered <code>16300</code>. This increases the likelihood that some alerts are generated soon after deploying (otherwise it's hard to tell if it's working). <code>msecs_before_recheck</code> Enter the minimum delay in milliseconds between alerts. The default is 300 milliseconds (5 minutes), as this prevents numerous alerts when the price hovers around the threshold. </li> <li> <p>Click <code>Save as project</code>. </p> <p>You now have a copy of the Threshold Alert library item in your workspace.</p> </li> <li> <p>Click the <code>Deploy</code> button. </p> <p>The library item is deployed as a service and automatically started. Once the microservice has been deployed, you'll be redirected to the pipeline view. </p> </li> <li> <p>Click the Threshold Alert service card to inspect the logs.</p> </li> </ol> <p>A successful deployment will resemble the following screenshot:</p> <p></p> <p>In the <code>Lineage</code> panel, you will notice that the two services are connected by a line, which indicates that they're both using the same topic, <code>currency-exchange-rates</code>. The CoinAPI service is writing to <code>currency-exchange-rates</code>, and the Threshold Alert service is reading from it.</p> <p>If there is an issue with the service, you can inspect the <code>build logs</code> in the <code>Lineage</code> panel, to check for any traces of a syntax error or other build issues.</p>"},{"location":"platform/tutorials/currency-alerting/currency-alerting/#setting-up-the-pushover-destination","title":"Setting up the Pushover destination","text":"<p>In this section you create a destination microservice that sends a push notification when a threshold alert condition occurs.</p> <p>This microservice reads from the <code>currency-rate-alerts</code> topic and whenever a new message arrives, it sends a push notification to the Pushover app on your mobile phone.</p> <p>It also reads the contents of the message and enriches the notification with details on how the threshold was crossed, that is, whether the price is moving up or down.</p> <p>To set up the push nonfiction microservice, follow these steps:</p> <ol> <li> <p>Click on the Library icon in the left-hand navigation.</p> </li> <li> <p>In the search box on the library page, enter \"Pushover\".</p> <p>You will see the <code>Threshold Alert</code> library item appear in the search results: </p> <p></p> </li> <li> <p>Click the <code>Preview code</code> button, and on the page that appears, click the <code>Edit code</code> button.</p> </li> <li> <p>On the <code>Project Creation</code> page, complete the following fields:</p> Field Value <code>Name</code> Enter a project name or keep the default suggestion. <code>Input</code> Select the input topic. In this case, select <code>currency-rate-alerts</code> from the list.Every message will be read from this topic, and turned into a push notification. <code>base_url</code> Leave the default value, <code>https://api.pushover.net/1/messages.json?</code>. If you decide to use another push notification app, your can always update this value. <code>api_token</code> Enter the API token that you generated for this application in your Pushover dashboard. For example: <code>azovmnbxxdxkj7j4g4wxxxdwf12xx4</code>. <code>user_key</code> Enter the user key that you received when you signed up with Pushover. For example: <code>u721txxxgmvuy5dxaxxxpzx5xxxx9e</code>) </li> <li> <p>Click the <code>Save as project</code>. You now have a copy of the Pushover notification library item in your workspace.</p> </li> <li> <p>Click the <code>Deploy</code> button.</p> </li> </ol> <p>You will now start receiving Pushover notifications on your phone, as shown here:</p> <p></p> <p>Depending on your threshold value and the price fluctuations, it might take a few minutes for you to get a notification. While you are waiting to receive a notification, you can inspect the logs, as shown previously.</p> <p></p> <ul> <li>Don't worry if the logs only show \"Listening to Stream\" initially \u2014 remember that the Threshold service only writes a message to the <code>currency-rate-alerts</code> topic when the threshold has been crossed.</li> <li>This means that the <code>currency-rate-alerts</code> stream might be empty for a short while.</li> <li>Depending on your threshold, it might take a couple of minutes for messages to start arriving.</li> </ul> <p>You've now completed the tutorial. </p>"},{"location":"platform/tutorials/currency-alerting/currency-alerting/#summary","title":"Summary","text":"<p>Here's what you accomplished in this tutorial:</p> <p>\u2705 You created a real-time web app, hosted on the Quix serverless compute environment.</p> <p>\u2705 You deployed three real-time serverless microservices: the CoinAPI source to read data from another platform, the Threshold Alert transformation to make decisions based on that data, and the Pushover destination to send push notifications to your phone.</p> <p>\u2705 You gained some experience in navigating the Quix platform, and learned how to deploy microservices without needing to write any code.</p>"},{"location":"platform/tutorials/currency-alerting/currency-alerting/#next-steps","title":"Next steps","text":"<p>To learn more, try one of these tutorials:</p> <ul> <li>Build a live video processing pipeline using the Transport for London (TfL) traffic cameras and the YOLO ML model for object detection</li> <li>Perform sentiment analysis on a stream of Tweets about a given subject</li> <li>Gather and processes data from an RSS feed and get an alert when specific criteria are met</li> <li>Stream and visualize real-time telemetry data with an Android app and Streamlit</li> </ul> <p>Getting Help</p> <p>If you need assistance with this tutorial, or have any general questions, please reach out to Quix, we\u2019ll be more than happy to help. We can be found on our public Slack channel, The Stream. Please sign up, and introduce yourself!</p>"},{"location":"platform/tutorials/data-science/data-science/","title":"Data Science with Quix: NY Bikes","text":"<p>Throughout this tutorial you will learn how to deploy a real-time data science project from scratch and into a scalable self-maintained solution. We will predict bike availability in New York by building the raw data ingestion pipelines, ETL and predictions. All in real time!</p>"},{"location":"platform/tutorials/data-science/data-science/#introduction","title":"Introduction","text":""},{"location":"platform/tutorials/data-science/data-science/#aim","title":"Aim","text":"<p>Quix allows you to create complex and efficient real time infrastructure in a simple and quick way. To show you that, you are going to build an application that uses real time New York bikes and weather data to predict the future availability of bikes in New York.</p> <p>In other words, you will complete all the typical phases of a data science project by yourself:</p> <ul> <li> <p>Build pipelines to gather bikes and weather data in real time</p> </li> <li> <p>Store the data efficiently</p> </li> <li> <p>Train some ML models with historic data</p> </li> <li> <p>Deploy the ML models into production in real time</p> </li> </ul> <p>This will typically take several people (Data Engineers, Data Scientists) and weeks of work, however you will complete this tutorial in under 90 minutes using Quix.</p>"},{"location":"platform/tutorials/data-science/data-science/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>You will need to know how to train an ML model.</p> Want to learn it? <p>If you don't already know how to train an ML model, follow our \"How to train an ML model\" tutorial here.</p> <p>We walk you through the process of getting the code to access the data, running the code in a Jupyter notebook, training the model and uploading your pickle file to Quix.</p> </li> <li> <p>You will need a Quix account and be logged into the Portal. </p> <p>Tip</p> <p>Go here to sign up if you need a free account.</p> </li> </ol>"},{"location":"platform/tutorials/data-science/data-science/#overview","title":"Overview","text":"<p>This walk through covers the following steps:</p> <ol> <li> <p>Create OpenWeather account (third party)</p> </li> <li> <p>Create a bikes data real time stream</p> </li> <li> <p>Create a weather forecast data real time stream</p> </li> <li> <p>Visualize the data</p> </li> <li> <p>Get data to train a model</p> </li> <li> <p>Deploy pre-trained ML models and produce predictions in real time</p> </li> <li> <p>See the models output</p> </li> </ol>"},{"location":"platform/tutorials/data-science/data-science/#1-create-openweather-account-free","title":"1. Create OpenWeather account (free)","text":"<p>Info</p> <p>OpenWeather is a team of IT experts and data scientists that provides historical, current and forecasted weather data via light-speed APIs.</p> <ol> <li> <p>Go to the OpenWeather Sign Up page.</p> </li> <li> <p>Click the \"Sign Up\" button and complete the dialog. Do the email and text message verifications.</p> </li> <li> <p>Then, go to the OpenWeather API keys page to copy your key. Keep it safe for later.</p> </li> </ol>"},{"location":"platform/tutorials/data-science/data-science/#2-bikes-real-time-stream","title":"2. Bikes real time stream","text":"<p>Start by getting the real time bikes stream. Use CityBikes to get real time bikes data (it doesn\u2019t require a sign up or any keys).</p> <p>Instead of writing a lot of code you will use the Library to deploy a pre-built service that streams data from the New York CitiBikes api.</p> <ol> <li> <p>Search the library for <code>New York</code> and select the <code>New York Bikes - Source</code> tile.</p> <p></p> <p>Tip</p> <p>The Library is on the left hand menu</p> </li> <li> <p>Click <code>Setup and deploy</code></p> <p>a. Leave the Name as it is</p> <p>b. Ensure the <code>output</code> is set to <code>bikes-topic</code></p> </li> <li> <p>Click <code>Deploy</code></p> <p>The pre-compiled service will be deployed to your workspace and will begin running immediately.</p> </li> </ol>"},{"location":"platform/tutorials/data-science/data-science/#3-weather-real-time-stream","title":"3. Weather real time stream","text":"<p>You now have a working real time stream of bike data. Now use the OpenWeather account to create a real time weather stream. The procedure is almost the same, so you should have no problems!</p> <ol> <li> <p>Search the library for <code>weather</code> and select the <code>Open Weather API</code> tile.</p> </li> <li> <p>Click <code>Setup and deploy</code></p> <p>a. Leave the Name as it is</p> <p>b. Ensure the <code>output</code> is set to <code>weather-topic</code></p> <p>c. Paste the key from your OpenWeather API keys page (here)</p> <p></p> </li> <li> <p>Click <code>Deploy</code></p> <p>The pre-compiled service will be deployed to your workspace and will begin running immediately.</p> <p>OpenWeather limitation</p> <p>The OpenWeather API has limits on how much data we can access for free and the real weather only changes in real-time (this means slowly). </p> <p>In order to prevent your account from being rate limited, we consume updated data every 30 minutes. (see more about this limitation later on in training data)</p> </li> </ol> <p>Success</p> <p>At this stage you should have two services running</p> <p></p> <p>One service publishing New York CitiBike data to a topic and another publishing OpenWeather data.</p>"},{"location":"platform/tutorials/data-science/data-science/#4-view-and-store-the-data","title":"4. View and store the data","text":"<p>With Quix it's easy to visualize your data in a powerful and flexible way, you can see the real-time data and view historic data.</p> <p>At it's heart Quix is a real-time data platform, so if you want to see data-at-rest for a topic, you must turn on data persistence for that topic (You'll do this below).</p>"},{"location":"platform/tutorials/data-science/data-science/#real-time","title":"Real-time","text":"<ol> <li> <p>View the real-time data by clicking on the green arrow coming out of the <code>New York Bikes</code> service on the home page.</p> </li> <li> <p>Click <code>Explore live data</code> on the context menu</p> </li> <li> <p>Select a stream from the <code>select streams</code> list</p> </li> <li> <p>Select the parameters in the <code>select parameters or events</code> list</p> </li> <li> <p>Select the <code>Table</code> tab in the top middle of the screen</p> </li> </ol> <p>Be patient</p> <p>If you don't see any streams or parameters, just wait a moment or two. The next time data arrives these lists will be automatically populated.</p>"},{"location":"platform/tutorials/data-science/data-science/#historic","title":"Historic","text":"<p>In order to train a machine learning model we will need to store the data we are ingesting so that we start building a historic dataset. However topics are real time infrastructures, not designed for data storage. To solve this, Quix allows you to send the data going through a topic to an efficient real time database if you need it:</p> <ol> <li> <p>Navigate to the topics page using the left hand navigation</p> </li> <li> <p>Locate the topic(s) you want to store data for (in this case <code>bikes-topic</code> and <code>weather-topic</code>)</p> </li> <li> <p>Click the toggle in the Persistence column to <code>on</code></p> </li> <li> <p>Finally, go back to the homepage. </p> <p>Now stop and then start the OpenWeather API service. This will collect and publish fresh data to the weather-topic.</p> </li> </ol>"},{"location":"platform/tutorials/data-science/data-science/#5-train-your-model","title":"5. Train your model","text":"<p>Quix gives you the freedom to train the ML model your way. If you already have tools and processes for doing that then great, you can train the model and import it into Quix so that you can run it in real-time. </p> <p>Follow the along and we'll show you how to get data out of Quix so you can train the model.</p>"},{"location":"platform/tutorials/data-science/data-science/#training-data","title":"Training data","text":"<p>We mentioned earlier in Weather real time stream that free access to the OpenWeather API only allows us to consume new data every 30 minutes therefore, at this point you will have a limited data set.</p> <p>You can leave the data consumption process running overnight or for a few days to gather more data, but for the time being there's no problem in continuing with the tutorial with your limited historic data.</p>"},{"location":"platform/tutorials/data-science/data-science/#get-the-data","title":"Get the data","text":"<ol> <li> <p>Click <code>Persisted data</code> in the left hand navigation</p> </li> <li> <p>Select the <code>bikes-topic</code> in the left hand panel</p> </li> <li> <p>Mouse over a stream name in the table and click the <code>Visualize stream</code> button</p> <p>The page will navigate to the <code>Data explorer</code></p> </li> <li> <p>In the query builder on the left hand side click the <code>+</code> under <code>SELECT (Parameters &amp; Events)</code></p> </li> <li> <p>Select <code>total_num_bikes_available</code></p> </li> <li> <p>Click <code>+</code> again</p> </li> <li> <p>Select <code>num_docks_available</code></p> </li> </ol> <p>Success</p> <p>You should be looking at a visualization of the two selected parameters</p> <p></p> <ol> <li> <p>Switch off <code>aggregation</code> to see all of the data</p> </li> <li> <p>Select the <code>Code</code> tab to view the code to access this data set from outside of Quix</p> </li> </ol>"},{"location":"platform/tutorials/data-science/data-science/#train-the-model","title":"Train the model","text":"<p>At this point, you are generating historic data and know how to query it. You can train your ML models as soon as you've gathered enough data.</p> <p>Need help?</p> <p>Follow our \"How to train an ML model\" tutorial here</p> <p>We walk you through the process of getting the code to access the data (as described above), running the code in a Jupyter notebook, training the model and uploading your pickle file to Quix.</p> <p>However, it would take several weeks to accumulate enough historic data to train a model, so let's continue the tutorial with some pre-trained models we have provided. We've done it using the very same data flow you've just built, and can find the Jupyter notebook code we used here.</p>"},{"location":"platform/tutorials/data-science/data-science/#6-run-the-model","title":"6. Run the model","text":"<p>We have included our trained model artifacts as pickle files in the prediction code project and uploaded it to the open source library, so let's use them.</p>"},{"location":"platform/tutorials/data-science/data-science/#prediction-service-code","title":"Prediction service code","text":"<p>Get the code for the prediction service.</p> <ol> <li> <p>Click on the Library</p> </li> <li> <p>Search for <code>New York</code> and select the <code>New york Bikes - Prediction</code> tile</p> </li> <li> <p>Click <code>Edit code</code></p> <p>a. Leave the Name as it is</p> <p>b. Ensure the <code>bike-input</code> is set to <code>bike-topic</code></p> <p>c. Ensure the <code>weather-input</code> is set to <code>weather-topic</code></p> <p>d. Ensure the <code>output</code> is set to <code>NY-bikes-prediction</code></p> </li> <li> <p>Click <code>Save as project</code></p> <p>This will save the code for this service to your workspace</p> </li> </ol> <p>Free Models</p> <p>Look in the <code>MLModels</code> folder. We've stashed the pre-trained ML models here for you. You can upload your own and compare them to ours. (Let us know how they compare)</p>"},{"location":"platform/tutorials/data-science/data-science/#run","title":"Run","text":"<p>You can now run the prediction model from this 'dev' environment to make sure it's working before deploying it to an always ready, production environment.</p> <ol> <li> <p>Click <code>run</code> in the top right hand corner.</p> </li> <li> <p>Observe the <code>console</code> tab at the bottom of the screen.</p> <p>Any packages that are needed will be installed.</p> <p>Any topics that didn't previously exist will be created.</p> <p>Then the code will run.</p> <p>You will see a line similar to this in the console output.</p> <pre><code>Current n bikes: 23742 Forecast 1h: 23663 Forecast 1 day: 22831\n</code></pre> <p>Data</p> <p>For a new prediction to be generated, the service has to receive data from both bikes (updated often) and weather feeds (only updated every 30 mins).</p> <p>When you test the model, you may want to force the weather service to produce some new data (to avoid waiting for 30 mins) by restarting the service: stop it and then re-deploy it. By doing this it will start generating predictions sooner.</p> </li> </ol>"},{"location":"platform/tutorials/data-science/data-science/#deploy","title":"Deploy","text":"<pre><code>With the code running we can deploy it to the Quix serverless environment. Here, it will run continuously, gathering data from the sources and producing predictions.\n</code></pre> <ol> <li> <p>Click stop if you haven't already done so.</p> </li> <li> <p>Click <code>Deploy</code> in the top right hand corner near <code>run</code></p> </li> <li> <p>On the <code>Deployment settings</code>, increase the memory to at least 1.5GB</p> </li> <li> <p>Click <code>deploy</code></p> <p>You will be redirected to the home page and the code will be built, deployed and started.</p> </li> </ol>"},{"location":"platform/tutorials/data-science/data-science/#7-see-the-models-output","title":"7. See the models output","text":"<p>Once the prediction service has started you can once more restart the 'Open Weather API' service and view the data.</p> <p>You should be familiar with some of the following steps, they are very similar to 'Get the data' above.</p> <ol> <li> <p>Restart the 'Open Weather API' service</p> </li> <li> <p>Click <code>Persisted streams</code> in the left hand menu</p> </li> <li> <p>Click the toggle switch next to the <code>ny-bikes-prediction</code> topic to persist the data (wait for this to complete)</p> </li> <li> <p>Mouse over the <code>stream name</code> of one of the rows in the table</p> </li> <li> <p>Click the <code>Visualize stream</code> button</p> </li> <li> <p>Select both of the parameters (<code>timestamp_ny_prediction</code> and <code>forecast_1d</code>)</p> </li> <li> <p>You can select the <code>Waveform</code> tab to see a graphical representation of the forecast or select the <code>Table</code> tab to see the raw data.</p> </li> </ol> <p>Success</p> <p>You made it to the end! Give yourself a high five.</p>"},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/","title":"Build an Interactive, Realtime Experience","text":""},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/#aim","title":"Aim","text":"<p>This Data Streaming and Processing tutorial will help you build your own realtime interactive creation.</p> <p>The multiple elements, tools and techniques used in this tutorial will allow you to experience much of what Quix has to offer.</p> <p>By the end you will have:</p> <ul> <li> <p>Created services to process data in realtime</p> </li> <li> <p>Deployed a public facing website with desktop and mobile elements</p> </li> <li> <p>Created an experience you can share with friends and colleagues</p> </li> </ul> <p></p> <p>Tip</p> <p>If you need any help, get into difficulties or just want to say hi then please join our Slack community. We're eager to hear from anyone working with real-time data and that includes people following our tutorials.</p>"},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/#project-architecture","title":"Project Architecture","text":"<p>The solution has 3 main elements:</p> <ul> <li> <p>Two services to process data</p> </li> <li> <p>A NodeJS UI to display a race track and send data from your mobile device</p> </li> </ul> <p>However, this is all running with the Quix Serverless environment.</p> <p>You have to create and deploy 3 projects, we have: . Created an always on high performance back-end . Created APIs and Services focused on performance . Opened firewall ports and optimized DNS propagation</p>"},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>A Quix account</p> </li> <li> <p>A mobile device</p> </li> </ul> <p>Tip</p> <p>If you don\u2019t have a Quix account yet, go here to create one.</p>"},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/#overview","title":"Overview","text":"<p>This walk through covers the following:</p> <ol> <li> <p>Creating the python services and NodeJS UI\u2019s</p> </li> <li> <p>Deployment of the above</p> </li> <li> <p>Some fun!</p> </li> </ol>"},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/#getting-started","title":"Getting Started","text":"<p>Login to Quix and open your Workspace, you get one workspace on the free tier, more on higher tiers. A Quix Workspace is a container to help you manage all the data, topics, models and services related to a single solution so we advise using a new, clean one for this tutorial.</p>"},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/#quix-library","title":"Quix Library","text":"<p>The code you will need for this tutorial is located in the Quix Library. </p> <p>Open the library and search for <code>Streaming Demo</code>. You will see 3 results.</p> <p></p> <p>You will save the code for each of these to your workspace and deploy the two services and the UI.</p>"},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/#input","title":"Input","text":"<p>First you will select, build and deploy the input project, this project handles and transforms data from your phone.</p> <p>Don't worry, all the code you'll need is in the <code>Streaming Demo - Input</code> project.</p>"},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/#save-the-code","title":"Save the code","text":"<p>Follow these steps to get the code and deploy the project as a microservice.</p> <ol> <li>Click the tile</li> <li> <p>Click <code>Edit code</code></p> <p>Note</p> <p>Leave the name, input and output as they are.</p> <p>The input and output values are Topics. These have been pre-configured in this and the other projects in this tutorial to allow the services to communicate with each other.</p> </li> <li> <p>Click <code>Save as project</code></p> <p>This will save a copy of this code to your workspace.</p> About the code <p>The code's main purpose is to listen for and respond in real-time to data and events being streamed to it via the <code>gamedata</code> topic.</p> <p>The data is streamed into the <code>gamedata</code> topic from your phone.</p> <p>When data is received, the <code>on_parameter_data_handler</code> callback is immediately triggered and the code within the callback processes the data.</p> <p>It extracts numeric values for <code>throttle</code>, <code>brake</code> and the phones <code>Y</code> axis aka steering.</p> <p>These are combined with some additional synchronization data, into a new ParameterData object and published to the output topic (<code>car-game-input</code>)</p> </li> <li> <p>Tag this version of the code by clicking the small tag icon at the top of the code window.</p> <ul> <li>Type <code>v1.0</code> into the input box</li> <li>Hit enter</li> </ul> </li> </ol> <p>Next you will deploy the code as a microservice.</p>"},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/#deploy","title":"Deploy","text":"<p>To deploy a microservice in Quix is a very simple three step process and step one isn't read the Kubernetes manual!</p> <ol> <li> <p>Click <code>Deploy</code> near the top right hand corner of the screen.</p> </li> <li> <p>Select the <code>v1.0</code> tag you created earlier</p> </li> <li> <p>Click <code>Deploy</code></p> <p>You will be redirected to the homepage and the code will be built, deployed and your microservice will be started.</p> </li> </ol> <p>Success</p> <p>\ud83d\ude80 You've deployed the first microservice! Congrats!</p> What's actually happening <p>We built the code, created any missing topics and then deployed the compiled code into a Kubernetes pod running in our shared cluster.</p> <p>Once started, the microservice is ready to handle any data streamed to the input topic.</p>"},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/#control","title":"Control","text":"<p>Now that you have the first service up and running it's time for the next one.</p> <p>Follow the same process as above and deploy the <code>Streaming Demo - Control</code> project.</p> <p>Remember the steps are:</p> <ol> <li> <p>Search the library for <code>Streaming Demo</code></p> </li> <li> <p>Select the <code>Streaming Demo - Control</code> project</p> </li> <li> <p>Save it to your workspace</p> </li> <li> <p>Tag it</p> </li> <li> <p>Deploy it</p> </li> </ol> <p>Success</p> <p>\ud83d\ude80 You deployed another service and should now have two services in your data processing pipeline.</p> About the code <p>The code's main purpose is to listen for and respond in real-time to data and events being streamed to it via the <code>car-game-input</code> topic.</p> <p>When data is received, the <code>on_parameter_data_handler</code> callback is immediately triggered and the code within the callback processes the data.</p> <p>It extracts numeric values for <code>throttle</code>, <code>brake</code> and <code>steering</code> and uses these to determine the cars position on the track, it also slows the car if it goes off the track.</p> <p>The cars updated x and y coordinates, speed and angle are then published to the output topic called <code>car-game-control</code>. The UI subscribes to this topic and keeps the car in the correct position.</p>"},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/#ui","title":"UI","text":"<p>Now we can get to the user interface (UI). This will allow you to see the game board on your computer and see the controls on your phone. It's all linked together with Quix and a QR code.</p> <p>You should be familiar with the process by now.</p> <ol> <li> <p>Search the library for <code>Streaming Demo</code></p> </li> <li> <p>Select the <code>Streaming Demo - UI</code> tile.</p> </li> <li> <p>Save it to your workspace</p> </li> <li> <p>Tag it</p> <p>Make it public</p> <p>This time before you click <code>Deploy</code>, you'll need to make the deployment publicly accessible.</p> <p>This means making it available to the internet so your computer and phone can access it.</p> <p>It's simple, just follow these steps:</p> <ol> <li>Click <code>PUBLIC ACCESS</code></li> <li>Click the toggle switch to turn on public access</li> <li>Change the <code>URL prefix</code> to <code>game-demo</code></li> </ol> </li> <li> <p>Deploy it by clicking <code>Deploy</code></p> </li> </ol> <p>Build Time</p> <p>It'll take a few minutes to build the UI for the demo. The build will download all the necessary packages, build and deploy the UI as well as opening up the firewall to allow your devices access.</p> <p>Success</p> <p>\ud83d\ude80 You deployed the UI to your workspace and can now proceed to the fun part of the tutorial</p> About the code <p>This UI code is Javascript and HTML, it displays the track and car and subscribes to data coming from the topics to keep the car where it's supposed to be or at least where you drive it!</p> <p>The most relevant part of the code is where websockets are used via Microsoft's SignalR.</p> <p>For example these lines subscribe to various parameter values on the <code>car-game-control</code> topic.</p> <pre><code>    connection.invoke(\"SubscribeToParameter\", \"car-game-control\", stream_id + \"-car-game-input-control\", \"x\");\nconnection.invoke(\"SubscribeToParameter\", \"car-game-control\", stream_id + \"-car-game-input-control\", \"y\");\nconnection.invoke(\"SubscribeToParameter\", \"car-game-control\", stream_id + \"-car-game-input-control\", \"angle\");\nconnection.invoke(\"SubscribeToParameter\", \"car-game-control\", stream_id + \"-car-game-input-control\", \"speed\");\n</code></pre> <p>And then when data is published for those parameters this code will handle it</p> <pre><code>    connection.on(\"ParameterDataReceived\", data =&gt; {\nif (data.streamId.endsWith(\"-car-game-input-control\")) { ... }\n}\n</code></pre> <p>Look for those lines in the <code>create-page.js</code> file and explore what they do.</p> <p>For more about WebSockets see this page.</p>"},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/#fun","title":"Fun","text":"<p>With the microservices for control and input processing deployed along with the UI we can move on to trying the game.</p> <ol> <li> <p>Click the <code>Streaming Demo - UI</code> tile on the home page</p> </li> <li> <p>Click the <code>Public URL</code> in the <code>Details</code> panel on the left</p> <p>A new tab will open and load the games first page.</p> </li> <li> <p>Scan the QR code with your phone and follow the link</p> </li> <li> <p>Ensure your phone is rotated either left or right depending on your device</p> </li> <li> <p>Click the <code>START</code> button on your phones screen</p> <p>Note</p> <p>You might need to allow notifications (this is just for the buzzer/vibrate to work)</p> <p>Note</p> <p>Notice that the page on your computer has also changed to the racing track. </p> <p>You can now use your phone as a steering wheel to drive the car around the track.</p> </li> <li> <p>DRIVE! \ud83c\udfce\ufe0f\ud83c\udfc1</p> </li> </ol>"},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/#phone-screen","title":"Phone screen","text":""},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/#game-screen","title":"Game screen","text":""},{"location":"platform/tutorials/data-stream-processing/data-stream-processing/#thanks","title":"Thanks","text":"<p>Thanks for following the tutorial, hopefully you learnt something about Quix and had some fun doing it!</p> <p>If you need any help, got into difficulties or just want to say hi then please join our Slack community.</p>"},{"location":"platform/tutorials/eventDetection/","title":"Event Detection","text":"<p>In this tutorial you will learn how to build a real-time event detection pipeline. Events will be simulated crashes or accidents that could occur while driving or cycling. </p> <p>You will deploy a crash detection service to detect crashes in real time and a UI to view the location of the incident.</p> <p>In this example the actual crash detection is performed using an ML model, pretrained with data from Quix. We have a tutorial that shows you how to train a basic model with a small subset of data, you can collect more data to train your model using more scenarios to ensure it\u2019s accurate for your needs.</p> <p>Once you have completed the tutorial you will see the following UI:</p> <p></p> <p>You will see a map with the route being taken, various data values sent from the data source and the times and locations of events.</p>"},{"location":"platform/tutorials/eventDetection/#the-parts-of-the-tutorial","title":"The parts of the tutorial","text":"<p>This tutorial is divided into several parts to make it more a manageable learning experience. Feel free to complete the various parts at your own pace, your Quix account is free and credit renews each month.</p> <ol> <li> <p>Data Acquisition. Streaming data into the crash detection pipeline will bring the solution to life. You\u2019ll have 2 options here, stream data live from your phone or use some CSV data we prerecorded. We'll guide you through whichever option is best for you.</p> </li> <li> <p>Build and deploy the crash detection service. This service uses an ML model to detect crashes in real-time. We\u2019ll show you how to train the model using data obtained from Quix, but we also provide you with a pretrained model for convenience.</p> </li> <li> <p>Deploy the UI. You will deploy the demo UI. This is a prebuilt UI from the Quix Samples Library. It allows the user to see where a device is located and indicates where a crash has occurred.</p> </li> </ol> <p>Info</p> <p>If at any point you run into an issue or want to know more then please reach out to us in our Slack community The Stream.</p>"},{"location":"platform/tutorials/eventDetection/conclusion/","title":"Conclusion","text":"<p>You\u2019ve just made use of the Quix library, our collection of open source connectors, and examples, to deploy a UI and event detection microservice.</p> <p>Congratulations, you have achieved a lot!</p>"},{"location":"platform/tutorials/eventDetection/conclusion/#next-steps","title":"Next Steps","text":"<p>Here are some suggested next steps to continue on your Quix learning journey:</p> <ul> <li>Build a pipeline to perform real-time sentiment analysis on text, including high volume messages from Twitter.</li> </ul> <p>What will you build? Let us know! We\u2019d love to feature your project or use case in our newsletter.</p>"},{"location":"platform/tutorials/eventDetection/conclusion/#getting-help","title":"Getting help","text":"<p>If you need any assistance, we're here to help in The Stream, our free Slack community. Introduce yourself and then ask any questions in <code>quix-help</code>.</p>"},{"location":"platform/tutorials/eventDetection/crash-detection-ui/","title":"3. Demo UI","text":"<p>This is the UI for this tutorial, it enables the user to see where a device is located, the path it has taken and indicates where a crash has occurred.</p> <p>The UI you will deploy is shown in the following screenshot:</p> <p></p>"},{"location":"platform/tutorials/eventDetection/crash-detection-ui/#deploying-the-ui","title":"Deploying the UI","text":"<p>The following steps demonstrate how to select the UI from the Samples Library and deploy it to your Quix workspace.</p> <p>Follow these steps to deploy the prebuilt UI:</p> <ol> <li> <p>Navigate to the Library and search for <code>Event Detection Demo UI</code>.</p> </li> <li> <p>Click the <code>Setup &amp; deploy</code> button.</p> </li> <li> <p>Ensure that the <code>topic</code> input box contains <code>phone-data</code>.</p> <p>This topic will be subscribed to and will contain the sensor data from the data source you deployed earlier.</p> </li> <li> <p>Ensure that the <code>eventTopic</code> input contains <code>phone-out</code>.</p> <p>This topic will be subscribed to and will contain any events generated by the crash event detection service you deployed earlier.</p> </li> <li> <p>Click <code>Deploy</code> and wait while the UI is deployed and started.     You will be redirected to your workspace homepage once it's completed.</p> </li> </ol> <p>Success</p> <p>You have deployed the UI for this demo!</p> <ol> <li> <p>Click the \"open in new window\" button on the <code>Crash Detection Demo UI</code> tile.</p> <p></p> </li> <li> <p>Observe the various elements of the UI:</p> <p>Note</p> <p>You will need some data streaming into the <code>phone-data</code> topic for the UI to activate. </p> <p>Ensure the services you deployed in the previous stages of this tutorial are up and running. Have your phone to hand or make sure the CSV data source is running</p> <p>a. The map showing the location reported by your phone or from the prerecorded data.</p> <p>b. Telemetry data from your phone.</p> <p>c. The timestamp of any crash events.</p> <pre><code>You can click a crash event to see where it ocurred on the map.\n</code></pre> </li> </ol> <p>Conclusion, next steps and getting help </p>"},{"location":"platform/tutorials/eventDetection/crash-detection/","title":"2. Event Detection","text":"<p>Our event detection pipeline is centered around this service, which executes an ML model to detect whether a vehicle has been involved in an accident.</p> <p>In reality our ML model was trained to detect the difference between a phone being shaken versus just being used normally. You actually don\u2019t have to use an ML model at all! There are various ways this service could have been written, for example, you could detect a change in the speed or use the speed and another parameter to determine if an event has occurred.</p>"},{"location":"platform/tutorials/eventDetection/crash-detection/#service-template","title":"Service template","text":"<p>Follow these steps to start creating the <code>crash detection service</code>:</p> <ol> <li> <p>Navigate to the library and search for <code>Empty template</code>.</p> </li> <li> <p>Click <code>Preview code</code> on the transformation template (shown with a violet highlight).</p> </li> <li> <p>Click <code>Edit code</code>.</p> </li> <li> <p>Change the name to <code>Crash event detection</code>.</p> </li> <li> <p>Enter <code>phone-data</code> into the input field.</p> </li> <li> <p>Enter <code>phone-out</code> into the output field.</p> </li> <li> <p>Click <code>Save as project</code>.</p> </li> </ol> <p>You now have the basic template for the service saved to your workspace.</p>"},{"location":"platform/tutorials/eventDetection/crash-detection/#test","title":"Test","text":"<p>At this stage you should test the code to make sure it passes some basic functional tests:</p> <ol> <li> <p>Ensure that the data source you deployed earlier is running.</p> </li> <li> <p>Click the <code>Run</code> button in the top right of your browser.</p> </li> <li> <p>Explore the <code>Console</code> and <code>Messages</code> tabs and verify that there is data arriving into the <code>phone-data</code> topic.</p> </li> <li> <p>Stop the code from running once you are finished investigating the tabs.</p> </li> </ol>"},{"location":"platform/tutorials/eventDetection/crash-detection/#adding-functionality","title":"Adding functionality","text":"<p>Next you will add code to detect the crash events, making use of some code snippets and Python libraries.</p>"},{"location":"platform/tutorials/eventDetection/crash-detection/#requirementstxt","title":"requirements.txt","text":"<p>Follow these steps:</p> <ol> <li> <p>Open the <code>requirements.txt</code> file and add the following lines to ensure all the packages are installed.</p> <pre><code>urllib3\nxgboost==0.90\nscikit-learn \n</code></pre> </li> <li> <p>Save the file.</p> </li> </ol>"},{"location":"platform/tutorials/eventDetection/crash-detection/#mainpy","title":"main.py","text":"<ol> <li> <p>Open <code>main.py</code> and add these lines to the existing imports.</p> <pre><code>import pickle\nfrom urllib import request\n</code></pre> <p>Next add these lines which will download the <code>.pkl</code> file (the pretrained model) from our storage account and load the model into memory.</p> <pre><code># download the model with urllib\nf = request.urlopen(\"https://quixtutorials.blob.core.windows.net/tutorials/event-detection/XGB_model.pkl\")\nwith open(\"XGB_model.pkl\", \"wb\") as model_file:\nmodel_file.write(f.read())\n# load it with pickle\nloaded_model = pickle.load(open(\"XGB_model.pkl\", 'rb'))\n</code></pre> </li> <li> <p>Next, to ensure that the latest messages are processed instead of any historical data, set the \"ConsumerGroup\" parameter to <code>None</code> when opening the input topic. Look for the line with <code>open_input_topic</code>.</p> <p>This line: <pre><code>input_topic = client.open_input_topic(os.environ[\"input\"], auto_offset_reset=AutoOffsetReset.Latest)\n</code></pre> Should be changed to this: <pre><code>input_topic = client.open_input_topic(os.environ[\"input\"], None, auto_offset_reset=AutoOffsetReset.Latest)\n</code></pre></p> </li> <li> <p>Locate the line that instantiates the <code>QuixFunction</code> and pass the <code>loaded_model</code> as the last parameter. The QuixFunction class will use this to predict crash events.</p> <p>The resulting code should look like this:</p> <pre><code>quix_function = QuixFunction(input_stream, output_stream, loaded_model)\n</code></pre> The completed <code>main.py</code> should look like this <pre><code>from quixstreaming import QuixStreamingClient, StreamEndType, StreamReader, AutoOffsetReset\nfrom quixstreaming.app import App\nfrom quix_function import QuixFunction\nimport os\nimport pickle\nfrom urllib import request\n# download the model\nf = request.urlopen(\"https://quixtutorials.blob.core.windows.net/tutorials/event-detection/XGB_model.pkl\")\nwith open(\"XGB_model.pkl\", \"wb\") as model_file:\nmodel_file.write(f.read())\n# load it with pickle\nloaded_model = pickle.load(open(\"XGB_model.pkl\", 'rb'))\n# Quix injects credentials automatically to the client. Alternatively, you can always pass an SDK token manually as an argument.\nclient = QuixStreamingClient()\n# Change consumer group to a different constant if you want to run model locally.\nprint(\"Opening input and output topics\")\ninput_topic = client.open_input_topic(os.environ[\"input\"], None, auto_offset_reset=AutoOffsetReset.Latest)\noutput_topic = client.open_output_topic(os.environ[\"output\"])\n# Callback called for each incoming stream\ndef read_stream(input_stream: StreamReader):\n# Create a new stream to output data\noutput_stream = output_topic.create_stream(input_stream.stream_id)\noutput_stream.properties.parents.append(input_stream.stream_id)\n# handle the data in a function to simplify the example\nquix_function = QuixFunction(input_stream, output_stream, loaded_model)\n# React to new data received from input topic.\ninput_stream.events.on_read += quix_function.on_event_data_handler\ninput_stream.parameters.on_read_pandas += quix_function.on_pandas_frame_handler\n# When input stream closes, we close output stream as well. \ndef on_stream_close(endType: StreamEndType):\noutput_stream.close()\nprint(\"Stream closed:\" + output_stream.stream_id)\ninput_stream.on_stream_closed += on_stream_close\n# Hook up events before initiating read to avoid losing out on any data\ninput_topic.on_stream_received += read_stream\n# Hook up to termination signal (for docker image) and CTRL-C\nprint(\"Listening to streams. Press CTRL-C to exit.\")\n# Handle graceful exit of the model.\nApp.run()\n</code></pre> </li> <li> <p>Save <code>main.py</code>.</p> </li> </ol>"},{"location":"platform/tutorials/eventDetection/crash-detection/#quix_functionpy","title":"quix_function.py","text":"<ol> <li> <p>Open <code>quix_function.py</code>     This file contains handlers for tabular data and event data. You will add code to the <code>on_pandas_frame_handler</code> function to handle the input streams tabular data.</p> </li> <li> <p>Start by locating the <code>__init__</code> function definition.</p> </li> <li> <p>Add <code>loaded_model</code> as the last parameter to the <code>__init__</code> function, this will receive the loaded model from <code>main.py</code>.</p> </li> <li> <p>Store the <code>loaded_model</code> in a class property by adding the following line to the the <code>__init__</code> function.</p> <pre><code>self.loaded_model = loaded_model\n</code></pre> </li> <li> <p>Now replace the current <code>on_pandas_frame_handler</code> with this code:</p> <pre><code>    def on_pandas_frame_handler(self, df: pd.DataFrame):\nif \"gForceX\" in df:\ndf[\"gForceTotal\"] = df[\"gForceX\"].abs() +  df[\"gForceY\"].abs() + df[\"gForceZ\"].abs()\ndf[\"shaking\"] = self.loaded_model.predict(df[[\"gForceZ\", \"gForceY\", \"gForceX\", \"gForceTotal\"]])\nif df[\"shaking\"].max() == 1: \nprint(\"Crash detected.\")\nself.output_stream.events.add_timestamp_nanoseconds(df.iloc[0][\"time\"]) \\\n                    .add_value(\"crash\", \"Crash detected.\") \\\n                    .write()\n</code></pre> <p>The first thing this code does is check that the required data is in the DataFrame then uses the ML model with the g-force data to determine when shaking is occurring.</p> <p>If shaking is occurring an event is generated and streamed to the output topic.</p> The completed <code>quix_function.py</code> should look like this <pre><code>from quixstreaming import StreamReader, StreamWriter, EventData, ParameterData\nimport pandas as pd\nclass QuixFunction:\ndef __init__(self, input_stream: StreamReader, output_stream: StreamWriter, loaded_model):\nself.input_stream = input_stream\nself.output_stream = output_stream\nself.loaded_model = loaded_model\n# Callback triggered for each new event.\ndef on_event_data_handler(self, data: EventData):\nprint(data.value)\n# Here transform your data.\nself.output_stream.events.write(data)\n# Callback triggered for each new parameter data.\ndef on_pandas_frame_handler(self, df: pd.DataFrame):\n# if the expected column is in the dataframe\nif \"gForceX\" in df:\n# calc total g-force\ndf[\"gForceTotal\"] = df[\"gForceX\"].abs() +  df[\"gForceY\"].abs() + df[\"gForceZ\"].abs()\n# predict 'shaking'\ndf[\"shaking\"] = self.loaded_model.predict(df[[\"gForceZ\", \"gForceY\", \"gForceX\", \"gForceTotal\"]])\n# if 'shaking'\nif df[\"shaking\"].max() == 1: \nprint(\"Crash detected.\")\n# write the event to the output stream\nself.output_stream.events.add_timestamp_nanoseconds(df.iloc[0][\"time\"]) \\\n                    .add_value(\"crash\", \"Crash detected.\") \\\n                    .write()\n</code></pre> </li> <li> <p>Save <code>quix_function.py</code>.</p> </li> </ol>"},{"location":"platform/tutorials/eventDetection/crash-detection/#dockerfile","title":"dockerfile","text":"<ol> <li> <p>Now update the <code>dockerfile</code> in the <code>build</code> folder.</p> <p>Under the line with <code>COPY --from=git /project .</code> Add</p> <pre><code>RUN apt-get install libgomp1\n</code></pre> <p>This will install <code>libgomp1</code> which a requirement of <code>XGBoost</code></p> The completed <code>dockerfile</code> should look like this <pre><code>FROM quixpythonbaseimage\n\nWORKDIR /app\nCOPY --from=git /project .\nRUN apt-get install libgomp1\nRUN find | grep requirements.txt | xargs -I '{}' python3 -m pip install -i http://pip-cache.pip-cache.svc.cluster.local/simple --trusted-host pip-cache.pip-cache.svc.cluster.local -r '{}' --extra-index-url https://pypi.org/simple --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/\nENTRYPOINT python3 main.py\n</code></pre> </li> <li> <p>Save <code>dockerfile</code>. </p> </li> </ol>"},{"location":"platform/tutorials/eventDetection/crash-detection/#test-again","title":"Test again","text":"<p>You can once again run the code in the development environment to test the functionality:</p> <ol> <li> <p>Ensure that the data source you deployed earlier is running.</p> </li> <li> <p>Click <code>Run</code> in the top right of the browser to run the event detection code.</p> </li> <li> <p>If you chose to stream live data then gently shake your phone. </p> </li> <li> <p>If you chose to use CSV data, wait for a crash event to be streamed from the data set, or stop and start the service in another browser tab.</p> </li> <li> <p>Observe the <code>Console</code> tab. You should see a message saying \"Crash detected\".</p> </li> <li> <p>On the <code>Messages</code> tab select <code>output : phone-out</code> from the first drop-down.</p> </li> <li> <p>Gently shake your phone, or wait for another crash event from the CSV data, and observe that crash events are streamed to the output topic. You can click these rows to investigate the event data e.g. </p> <pre><code>[{\n    \"Timestamp\": 1674137774598972000,\n    \"Tags\": {},\n    \"Id\": \"crash\",\n    \"Value\": \"Crash detected.\"\n}]\n</code></pre> </li> <li> <p>Stop the code.</p> </li> </ol> <p>Success</p> <p>The crash detection service is working as expected and can now be deployed</p>"},{"location":"platform/tutorials/eventDetection/crash-detection/#deploy-crash-detection","title":"Deploy crash detection","text":"<p>Now that you have verified the service is working you can go ahead and deploy the service:</p> <ol> <li> <p>Tag the code by clicking the <code>add tag</code> icon at the top of the code panel.</p> </li> <li> <p>Enter a tag such as <code>v1</code>.</p> </li> <li> <p>Now click the <code>Deploy</code> button near the top right of the code panel.</p> </li> <li> <p>From the <code>Version tag</code> drop-down, select the tag you created.</p> </li> <li> <p>Click <code>Deploy</code>.</p> </li> </ol> <p>Success</p> <p>You now have a data source and the crash detection service running in your workspace. </p> <p>Next you\u2019ll deploy a real-time UI to visualize the route being taken, the location of any crash events and also to see some of the sensor data.</p> <p>Deploy the UI for this tutorial by following step 3 </p>"},{"location":"platform/tutorials/eventDetection/data-acquisition/","title":"1. Data acquisition","text":"<p>You\u2019ll start this tutorial by streaming data into a topic. Starting with the data feed allows you to verify that the code and services you\u2019ll build and deploy are working properly.</p> <p>You have two options for this stage:</p> <ol> <li> <p>Stream data from your Android phone with the Quix Tracker app.</p> </li> <li> <p>Stream prerecorded CSV data.</p> </li> </ol> <p>You will only need to set up one of these data sources but if you want to do both you can do that too!</p>"},{"location":"platform/tutorials/eventDetection/data-acquisition/#quix-tracker-app","title":"Quix tracker App","text":""},{"location":"platform/tutorials/eventDetection/data-acquisition/#external-source","title":"External source","text":"<p>To get started with these steps you first need to add a Topic and pipeline placeholder called an external source.</p> <p>An external source is a representation of a data source that is external to Quix, for example an IoT device or a service that is pushing data to a Quix topic using one of our API's.</p> <p>To add an external source:</p> <ol> <li> <p>Navigate to the Library</p> </li> <li> <p>Under <code>Pipeline Stage</code> click <code>Source</code>.</p> </li> <li> <p>Locate the <code>External Source</code> library item and click <code>Add external source</code>.</p> </li> <li> <p>Enter <code>phone-data</code> in the <code>Output</code> field and click <code>Add new topic</code> in the drop-down.</p> </li> <li> <p>Enter <code>Quix Tracker web gateway</code> in the <code>Name</code> field.</p> </li> <li> <p>Click <code>Add external Source</code>.</p> </li> </ol>"},{"location":"platform/tutorials/eventDetection/data-acquisition/#install-and-configure-the-apps","title":"Install and configure the apps","text":"<p>To stream data from your phone you\u2019ll need to install the Quix Tracker app on your Android phone and deploy the QR Settings Share app to your Quix workspace.</p> <p>Follow these steps:</p> <ol> <li> <p>Install the <code>Quix Tracker</code> from the Google Play Store. </p> <p></p> </li> <li> <p>Open the app and navigate to the <code>Settings</code> page via the menu.</p> </li> <li> <p>Click the <code>SCAN QR CODE</code> button at the top of the settings page. Now continue to follow these steps. When directed, in step #17, you will scan the QR code using the Quix Tracker app.</p> </li> <li> <p>In the Quix Portal, click the user icon in the top right of the browser.</p> <p></p> </li> <li> <p>Click <code>Tokens</code>.</p> </li> <li> <p>Generate a token with any name and allow at least a few days before it expires.</p> </li> <li> <p>Copy the token to your clipboard or somewhere safe.</p> </li> <li> <p>In the Quix Library, search for <code>QR Settings Share</code>.</p> </li> <li> <p>Click <code>Setup &amp; deploy</code>.</p> </li> <li> <p>Paste the token into the <code>token</code> field.</p> </li> <li> <p>Click <code>Deploy</code>.</p> </li> <li> <p>Open the QR Settings Share by clicking the \"open in new window\" icon.</p> <p></p> </li> <li> <p>Enter a name and a device identifier into the input fields.</p> </li> <li> <p>These can be any value such as your name and CAR-001.</p> </li> <li> <p>Click <code>Generate Token</code>.</p> </li> <li> <p>A QR code will be generated and presented to you in the UI.</p> <p></p> </li> <li> <p>Scan the QR code using the Quix Tracker app.</p> <p>The app has now been configured with the access token, allowing it to communicate with Quix.</p> </li> <li> <p>Ensure the rest of the fields are configured as follows:</p> <p>a. Topic field is set to <code>phone-data</code>.</p> <p>b. <code>Notifications Topic</code> field is set to <code>phone-out</code>.</p> <p>c. <code>DeviceId</code> field is set to your chosen device identifier.</p> <p>d. <code>Rider</code> is set to your name.</p> <p>e. <code>Team</code> has a value. For example <code>Quix</code>.</p> </li> <li> <p>Select <code>Dashboard</code> from the menu.</p> </li> <li> <p>Click the <code>START</code> button.</p> <p>This will open a connection to your Quix workspace and start streaming data from your phone.</p> </li> </ol>"},{"location":"platform/tutorials/eventDetection/data-acquisition/#verify-the-live-data","title":"Verify the live data","text":"<p>Follow these steps to ensure that everything is working as expected:</p> <ol> <li> <p>In the Quix Portal, navigate to <code>Data Explorer</code>.</p> </li> <li> <p>Ensure you are on the <code>Live data</code> tab.</p> </li> <li> <p>Under <code>Select a topic</code> select <code>phone-data</code>.</p> </li> <li> <p>Under <code>select streams</code> select the active stream. This should be the only stream that exists.</p> </li> <li> <p>Under <code>select parameters or events</code> select <code>gForceX</code>, <code>gForceY</code> and <code>gForceZ</code>.</p> </li> <li> <p>The waveform view should display the live g-force data from your phone.</p> </li> <li> <p>Move or gently shake your phone and notice that the waveform reflects whatever movement your phone is experiencing.</p> </li> </ol> <p>Success</p> <p>You have connected the Quix Tracker app to your workspace and verified the connection using the Live Data Explorer.</p>"},{"location":"platform/tutorials/eventDetection/data-acquisition/#csv-data","title":"CSV data","text":"<p>If you don\u2019t have an Android device, or you\u2019d rather stream some data provided for you, then use this data source.</p> <p>Follow these instructions to deploy the data source:</p> <ol> <li> <p>In the Quix Library, select <code>Python</code> under languages and <code>Source</code> under pipeline stage.</p> </li> <li> <p>In the search box enter <code>Empty template</code>.</p> </li> <li> <p>On <code>Empty template</code>, click <code>Preview code</code> then <code>Edit code</code>.</p> </li> <li> <p>Change the <code>Name</code> field to <code>CSV data source</code>.</p> </li> <li> <p>Change the <code>output</code> field to <code>phone-data</code>.</p> </li> <li> <p>Click <code>Save as project</code>.</p> </li> <li> <p>Open the <code>requirements.txt</code> file and add <code>urllib3</code> to a new line.</p> </li> <li> <p>Open the <code>main.py</code> file and replace the code with the following code:</p> <pre><code>from quixstreaming import QuixStreamingClient\nfrom quixstreaming.app import App\nimport time\nimport pandas as pd\nimport os\nfrom urllib import request\n# download data.csv\n# if you want to use your own data file just comment these lines\n# and place your data.csv file in the same folder as main.py\nf = request.urlopen(\"https://quixtutorials.blob.core.windows.net/tutorials/event-detection/data.csv\")\nwith open(\"data.csv\", \"wb\") as data_file:\ndata_file.write(f.read())\n# Quix injects credentials automatically to the client. Alternatively, you can always pass an SDK token manually as an argument.\nclient = QuixStreamingClient()\n# Open the output topic\nprint(\"Opening output topic\")\noutput_topic = client.open_output_topic(os.environ[\"output\"])\noutput_stream = output_topic.create_stream()\ndf = pd.read_csv(\"data.csv\")\nfor col_i in ['device_id','rider','streamId','team','version']:\ndf = df.rename(columns={col_i: \"TAG__\" + col_i})\nprint(\"Writing data\")\nseconds_to_wait = 0.5\nwhile True:\nfor i in range(len(df)):\nstart_loop = time.time()\ndf_i = df.iloc[[i]]\noutput_stream.parameters.write(df_i)\nprint(\"Sending \" + str(i) + \"/\" + str(len(df)))\nend_loop = time.time()\ntime.sleep(max(0.0, seconds_to_wait - (end_loop - start_loop)))\nApp.run()\n</code></pre> <p>Info</p> <p>The code:</p> <ul> <li> <p>Connects to Quix.</p> </li> <li> <p>Opens the CSV file using Pandas.</p> </li> <li> <p>Renames some columns so Quix Streams teams them as tags.</p> </li> <li> <p>Then streams each row to the <code>phone-data</code> topic.</p> </li> <li> <p>It does this continuously so stop the service when you have completed the tutorial.</p> </li> </ul> </li> <li> <p>Test the code by clicking <code>run</code> near the top right corner of the code window.</p> <p>With the code running you will see messages in the <code>Console</code> tab</p> <pre><code>[xx-xx-xx xx:xx:xx.xxx (8) INF] Topic phone-data is still creating\nOpening output topic\nWriting data\nSending 0/18188\nSending 1/18188\nSending 2/18188\n</code></pre> </li> <li> <p>Click the <code>Messages</code> tab and you\u2019ll see the raw messages being streamed to the <code>phone-data</code> topic.</p> </li> <li> <p>Click one of the messages and you\u2019ll see the raw data in JSON format.</p> Your data should look like JSON <p>Note! Depending on which row you click you may see slightly different results. Only some rows contain location data.</p> <pre><code>\"Epoch\": 0,\n\"Timestamps\": [\n    1673966764000000000\n],\n\"NumericValues\": {\n    \"Longitude\": [\n    0.52202169\n    ],\n    \"Latitude\": [\n    51.73491702\n    ],\n    \"Speed\": [\n    98.0639991760254\n    ],\n    \"Heading\": [\n    347.5\n    ],\n    \"BatteryLevel\": [\n    0.27\n    ],\n    \"Altitude\": [\n    59.63983154296875\n    ],\n    \"Accuracy\": [\n    3.790092468261719\n    ]\n},\n\"StringValues\": {\n    \"EnergySaverStatus\": [\n    \"Off\"\n    ],\n    \"BatteryState\": [\n    \"Discharging\"\n    ],\n    \"BatteryPowerSource\": [\n    \"Battery\"\n    ]\n},\n\"BinaryValues\": {},\n\"TagValues\": {}\n}\n</code></pre> </li> <li> <p>Stop the code by clicking the same button you clicked to run it.</p> </li> <li> <p>You'll now deploy the code as a service, so it stays running when you navigate around the platform.</p> </li> <li> <p>Click <code>Deploy</code>.</p> </li> <li> <p>On the dialog, click <code>Deploy</code>.</p> <p>Once deployed the service will be started and data will be streamed to the <code>phone-data</code> topic.</p> </li> </ol> <p>Detect crash events by following step 2 </p>"},{"location":"platform/tutorials/image-processing/","title":"Real-time image processing","text":"<p>In this tutorial you learn how to build a real-time image processing pipeline in Quix, using the Transport for London (TfL) traffic cameras, known as Jam Cams, the webcam on your laptop or phone,  and a YOLO v3 machine learning model. </p> <p>You'll use prebuilt Quix library items to build the pipeline. A prebuilt UI is also provided that shows you where the recognized objects are located around London.</p> <p>The following screenshot shows the pipeline you build in this tutorial:</p> <p></p>"},{"location":"platform/tutorials/image-processing/#getting-help","title":"Getting help","text":"<p>If you need any assistance while following the tutorial, we're here to help in The Stream community, our public Slack channel.</p>"},{"location":"platform/tutorials/image-processing/#tutorial-live-stream","title":"Tutorial live stream","text":"<p>If you'd rather watch a live stream, where one of our developers steps through this tutorial, you can view it here:</p>"},{"location":"platform/tutorials/image-processing/#prerequisites","title":"Prerequisites","text":"<p>To get started make sure you have a free Quix account.</p> <p>You'll also need a free TfL account. </p> <p>Follow these steps to locate your TfL API key:</p> <ol> <li> <p>Register for an account.</p> </li> <li> <p>Login and click the <code>Products</code> menu item.</p> </li> <li> <p>You should have one product to choose from: <code>500 Requests per min.</code></p> </li> <li> <p>Click <code>500 Requests per min.</code></p> </li> <li> <p>Enter a name for your subscription into the box, for example \"QuixFeed\", and click <code>Register</code>.</p> </li> <li> <p>You can now find your API Keys in the profile page.</p> </li> </ol>"},{"location":"platform/tutorials/image-processing/#quix-library","title":"Quix Library","text":"<p>The Quix Library is a collection of ready-to-use components you can leverage to build your own real-time streaming solutions. Typically these components require minimal configuration.</p> <p>Most of the code you need for this tutorial has already been written, and is located in the Quix Library. </p> <p>When you are logged into the Quix Portal, click on the Library icon in the left-hand navigation, to access the Quix Library.</p>"},{"location":"platform/tutorials/image-processing/#the-pipeline-you-will-create","title":"The pipeline you will create","text":"<p>There are four stages to the processing pipeline you build in this tutorial:</p> <ol> <li> <p>Video feeds</p> <ul> <li>Webcam image capture </li> <li>TfL Camera feed or \"Jam Cams\"</li> </ul> </li> <li> <p>Frame grabber</p> <ul> <li>Grab frames from TfL video feed</li> </ul> </li> <li> <p>Object detection</p> <ul> <li>Detect objects within images</li> </ul> </li> <li> <p>Web UI configuration</p> <ul> <li> <p>A simple UI showing:</p> <ul> <li>Images with identified objects    </li> <li>Map with count of objects at each camera's location</li> </ul> </li> </ul> </li> </ol> <p>Now that you know which components will be needed in the image processing pipeline, the following sections will step through the creation of the required microservices.</p>"},{"location":"platform/tutorials/image-processing/#the-parts-of-the-tutorial","title":"The parts of the tutorial","text":"<p>This tutorial is divided up into several parts, to make it a more manageable learning experience. The parts are summarized here:</p> <ol> <li> <p>Connect the webcam video feed. You learn how to quickly connect a video feed from your webcam, using a prebuilt library item.</p> </li> <li> <p>Object detection. You use a computer vision library item to detect a chosen type of object. You'll preview these events in the live preview. The object type to detect can be selected through a web UI, which is described later.</p> </li> <li> <p>Connect the TfL video feed. You learn how to quickly connect the TfL traffic cam feeds, using a prebuilt library item. You can perform object detection across these feeds, as they are all sent into the objection detection service in this tutorial.</p> </li> <li> <p>Frame grabber. You use a standard library item to grab frames from the TfL video feed.</p> </li> <li> <p>Deploy the web UI. You the deploy a prebuilt web UI. This UI enables you to select an object type to detect across all of your input video feeds. It displays the location pof object detection and object detection count on a map.</p> </li> <li> <p>Summary. In this concluding part you are presented with a summary of the work you have completed, and also some next steps for more advanced learning about the Quix Platform.</p> </li> </ol> <p>Part 1 - Connect the webcam feed </p>"},{"location":"platform/tutorials/image-processing/connect-video-tfl/","title":"3. Connect the TfL video feeds","text":"<p>In this part of the tutorial you connect your pipeline to the TfL traffic cam video feeds.</p> <p>Follow these steps to deploy the traffic camera feed service:</p> <ol> <li> <p>Navigate to the Library and locate <code>TfL Camera Feed</code>.</p> </li> <li> <p>Click <code>Setup &amp; deploy</code>.</p> </li> <li> <p>Paste your TfL API Key into the appropriate input.</p> </li> <li> <p>Click <code>Deploy</code>.</p> <p>Deploying will start the service in the Quix pre-provisioned infrastructure. This service will stream data from the TfL cameras to the <code>tfl-cameras</code> topic.</p> <p>At this point your pipeline view has one service deployed. When it has started the arrow pointing out of the service will be green. This indicates that data is flowing out of the service into a topic. Now, you need to deploy something to consume the data that is streaming into that topic.</p> </li> <li> <p>Once deployed successfully, stop the service. You will restart it later, but for now it can be stopped.</p> </li> </ol> <p>Part 5 - Frame grabber </p>"},{"location":"platform/tutorials/image-processing/connect-video-webcam/","title":"1. Connect the webcam video feed","text":"<p>In this part of the tutorial you connect your webcam video feed.</p> <p>Follow these steps to deploy the webcam service:</p> <ol> <li> <p>Navigate to the Library and locate <code>Image processing - Webcam input</code>.</p> </li> <li> <p>Click <code>Setup &amp; deploy</code>.</p> </li> <li> <p>Click <code>Deploy</code>.</p> <p>This service will stream data from your webcam to the <code>image-base64</code> topic.</p> </li> <li> <p>Click the <code>Public URL</code> icon, in the webcam service tile.</p> <p></p> <p>This opens the deployed website which uses your webcam to stream images to Quix.</p> <p>Note</p> <p>Your browser may prompt you to allow access to your webcam. You can allow access.</p> </li> </ol> <p>Part 2 - Decode images </p>"},{"location":"platform/tutorials/image-processing/decode/","title":"2. Decode images","text":"<p>In this part of the tutorial you decode the base64 encoded images coming from the webcam.</p>"},{"location":"platform/tutorials/image-processing/decode/#create-the-base64-decoder-service","title":"Create the base64 decoder service","text":"<p>Follow these steps to deploy the base64 decoder service:</p> <ol> <li> <p>Navigate to the Library and locate the Python <code>Empty template</code> transformation.</p> <p>Tip</p> <p>You can use the filters on the left hand side to select <code>Python</code> and <code>Transformation</code> then select <code>Empty template</code> in the resulting filtered items.</p> </li> <li> <p>Click <code>Edit code</code>.</p> </li> <li> <p>Enter <code>base64 decoder service</code> as the name for the project.</p> </li> <li> <p>Select or enter <code>image-base64</code> for the input.</p> </li> <li> <p>Select or enter <code>image-raw</code> for the output.</p> </li> <li> <p>Click <code>Save as project</code>.</p> </li> </ol>"},{"location":"platform/tutorials/image-processing/decode/#update-the-code","title":"Update the code","text":"<p>The code is now saved to your workspace and you can edit it to perform any actions you need it to.</p> <p>Using the following steps, update the default code so it decodes the web cam images being received on the <code>image-base64</code> topic. Then publish the decoded images to the <code>image-raw</code> topic.</p> <ol> <li> <p>Add <code>import base64</code> to the imports at the top of <code>main.py</code></p> </li> <li> <p>Update the <code>on_dataframe_received_handler</code> method by adding the following line to base64 decode the images.</p> <pre><code>df['image'] = df[\"image\"].apply(lambda x: base64.b64decode(x))\n</code></pre> <p>This should go immediately before this line:</p> <pre><code>stream_producer.timeseries.buffer.publish(df)\n</code></pre> </li> </ol> The completed <code>main.py</code> should look like this <pre><code>import quixstreams as qx\nimport os\nimport pandas as pd\nimport base64 # Added import (1)\nclient = qx.QuixStreamingClient()\ntopic_consumer = client.get_topic_consumer(os.environ[\"input\"], consumer_group = \"empty-transformation\")\ntopic_producer = client.get_topic_producer(os.environ[\"output\"])\ndef on_dataframe_received_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\n# Transform data frame here in this method. You can filter data or add new features.\n# Pass modified data frame to output stream using stream producer.\n# Set the output stream id to the same as the input stream or change it,\n# if you grouped or merged data with different key.\nstream_producer = topic_producer.get_or_create_stream(stream_id = stream_consumer.stream_id)\ndf['image'] = df[\"image\"].apply(lambda x: base64.b64decode(x)) # Added code (2)\nstream_producer.timeseries.buffer.publish(df)\n# Handle event data from library items that emit event data\ndef on_event_data_received_handler(stream_consumer: qx.StreamConsumer, data: qx.EventData):\nprint(data)\n# handle your event data here\ndef on_stream_received_handler(stream_consumer: qx.StreamConsumer):\n# subscribe to new DataFrames being received\n# if you aren't familiar with DataFrames there are other callbacks available\n# refer to the docs here: https://docs.quix.io/client-library/subscribe.html\nstream_consumer.events.on_data_received = on_event_data_received_handler # register the event data callback\nstream_consumer.timeseries.on_dataframe_received = on_dataframe_received_handler\n# subscribe to new streams being received\ntopic_consumer.on_stream_received = on_stream_received_handler\nprint(\"Listening to streams. Press CTRL-C to exit.\")\n# Handle termination signals and provide a graceful exit\nqx.App.run()\n</code></pre> <ol> <li>Import base64 which will be used to decode the images</li> <li>Call <code>base64.b64decode</code> and store the resulting data in the dataframe</li> </ol>"},{"location":"platform/tutorials/image-processing/decode/#deploy","title":"Deploy","text":"<p>Now it's time to deploy this microservice.</p> <p>Follow these steps:</p> <ol> <li> <p>Tag the code by clicking <code>add tag</code> at the top of the code panel. Enter <code>v1.0</code> for your tag.</p> </li> <li> <p>Click <code>Deploy</code> near the top right hand corner of the screen.</p> </li> <li> <p>Select the <code>v1.0</code> from the verison tag drop down.</p> </li> <li> <p>Click <code>Deploy</code>.</p> <p>You will be redirected to the homepage and the code will be built and deployed and your microservice will be started.</p> </li> </ol> <p>Part 3 - Object detection </p>"},{"location":"platform/tutorials/image-processing/object-detection/","title":"2. Object detection","text":"<p>In this part of the tutorial you add an object detection service into the pipeline. This service detects objects in any video feeds connected to its input. This service uses a YOLO v3 machine learning model for object detection.</p> <p>In a later stage of the pipeline you add a simple UI which enables you to select the type of object to detect.</p> <p>Follow these steps to deploy the object detection service:</p> <ol> <li> <p>Navigate to the Library and locate <code>Computer Vision object detection</code>.</p> </li> <li> <p>Click <code>Setup &amp; deploy</code>.</p> </li> <li> <p>Click <code>Deploy</code>.</p> <p>This service receives data from the <code>image-raw</code> topic and streams data to the <code>image-processed</code> topic.</p> </li> </ol> Understand the code <p>Here's the code in the file <code>quix_function.py</code>:</p> <pre><code># Callback triggered for each new parameter data. (1)\ndef on_parameter_data_handler(self, data: ParameterData):\n# Loop every row in incoming data. (2)\nfor timestamp in data.timestamps:\nbinary_value = timestamp.parameters['image'].binary_value\nsource_img = self.image_processor.img_from_base64(binary_value)\nstart = time.time()\n# We call YOLO3 model with binary values of the image\n# and receive objects with confidence values. (3)\nimg, class_ids, confidences = self.image_processor.process_image(source_img)\ndelta = start - time.time() # (4)\n# We count how many times each class ID is present in the picture. (5)\ncounter = Counter(class_ids)\nprint(\"New image in {0} at {1}\".format(self.input_stream.stream_id, timestamp.timestamp))\n# Starts by creating new row with timestamp that we carry from input. (6)\nrow = self.output_stream.parameters.buffer.add_timestamp_nanoseconds(timestamp.timestamp_nanoseconds) \n#\u00a0For each class ID we sent column with number of occurrences in the picture. (7)\nfor key, value in counter.items():\nprint(\"Key:{}\".format(key))\nrow = row.add_value(key, value)\n# Attach image column with binary data, GPS coordinates and model performance metrics. (8)\nrow.add_value(\"image\", self.image_processor.img_to_binary(img)) \\\n            .add_value(\"lat\", timestamp.parameters[\"lat\"].numeric_value) \\\n            .add_value(\"lon\", timestamp.parameters[\"lon\"].numeric_value) \\\n            .add_value(\"delta\", delta) \\\n            .write()\n</code></pre> <ol> <li>Each time a new parameter data arrives, this callback is invoked.</li> <li>Parameter data can be thought of as data in a tabular form. This code loops over all rows in the table.</li> <li>The object detection model is called with the source image. It returns an annoted image, an array containing the ids of the types of objects detected (for example: ['bus', 'car', 'truck', 'car', 'car', 'person', 'car', 'car', 'person', 'car', 'car', 'car', 'person', 'car', 'person', 'person']), and the confidence of the detection.</li> <li>The <code>delta</code> is a variable used to record how long it takes for the object detection. This is used as a measure of performance. </li> <li><code>Counter</code> is a dictionary that object type counts, for example, <code>{'truck': 3, 'car': 3}</code>.</li> <li>Timestamp TDB</li> <li>Add the class ID (object type detected) and the count for that object type. </li> <li>The row is written out. The row includes the image binary, geolocation, and delta as a measure of performance for object detection.</li> </ol> <p>Part 4 - TfL video </p>"},{"location":"platform/tutorials/image-processing/summary/","title":"6. Summary","text":"<p>In this tutorial you have learned that it is possible to quickly build a real-time image processing pipeline, using prebuilt library items. You have seen how to can connect to multiple types of video feed, perform object detection, and display the locations of the detected objects on a map, using the prebuilt UI.</p>"},{"location":"platform/tutorials/image-processing/summary/#library-items-used","title":"Library items used","text":"<p>Here is a list of the Quix open source library items used in this tutorial, with links to their code in GitHub:</p> <ul> <li>TfL traffic cam video feed</li> <li>TfL traffic cam frame grabber</li> <li>Webcam interface</li> <li>Computer vision object detection</li> <li>Web UI</li> </ul>"},{"location":"platform/tutorials/image-processing/summary/#next-steps","title":"Next Steps","text":"<p>Here are some suggested next steps to continue on your Quix learning journey:</p> <ul> <li> <p>Try the sentiment analysis tutorial.</p> </li> <li> <p>If you decide to build your own connectors and apps, you can contribute something to the Quix Library. Visit the Quix GitHub. Fork our library repo and submit your code, updates, and ideas.</p> </li> </ul> <p>What will you build? Let us know! We\u2019d love to feature your project or use case in our newsletter.</p>"},{"location":"platform/tutorials/image-processing/summary/#getting-help","title":"Getting help","text":"<p>If you need any assistance, we're here to help in The Stream, our free Slack community. Introduce yourself and then ask any questions in <code>quix-help</code>.</p>"},{"location":"platform/tutorials/image-processing/tfl-frame-grabber/","title":"4. Frame extraction","text":"<p>In this part of the tutorial you add a frame extraction service. </p> <p>The frame extraction service grabs single frames from the video feeds, so that object detection can be performed in the next stage of the pipeline. </p> <p>Follow these steps to deploy the frame extraction service:</p> <ol> <li> <p>Navigate to the Library and locate <code>TfL traffic camera frame grabber</code>.</p> </li> <li> <p>Click <code>Setup &amp; deploy</code>.</p> </li> <li> <p>Click <code>Deploy</code>.</p> <p>This service receives data from the <code>tfl-cameras</code> topic and streams data to the <code>image-raw</code> topic.</p> </li> </ol> <p>Part 6 - Web UI </p>"},{"location":"platform/tutorials/image-processing/web-ui/","title":"5. Deploy the web UI","text":"<p>In this part of the tutorial you add a service to provide a simple UI with which to monitor and control the pipeline.</p> <p>The following screenshot shows the last image processed from one of the video streams, as well as the map with a count of all the objects detected so far, and their location:</p> <p></p> <p>Tip</p> <p>At this point, make sure that all the services in your pipeline are running.</p> <p>Follow these steps to deploy the web UI service:</p> <ol> <li> <p>Navigate to the Library and locate <code>TFL image processing UI</code>.</p> </li> <li> <p>Click <code>Setup &amp; deploy</code>.</p> </li> <li> <p>Click <code>Deploy</code>.</p> </li> <li> <p>Once deployed, click the service tile.</p> </li> <li> <p>Click the <code>Public URL</code> to launch the UI in a new browser tab.</p> <p></p> </li> </ol> <p>You have now deployed the web UI.</p> <p>You can select the type of object you want to detect, and the locations at which that object are detected are displayed on the map. The number of occurrences of detection at that location are also displayed in the map pin.</p> <p>Part 7 - Summary </p>"},{"location":"platform/tutorials/nocode-sentiment/nocode-sentiment-analysis/","title":"No code sentiment analysis","text":"<p>This tutorial shows how to build a data processing pipeline without code. You\u2019ll analyze tweets that contain information about Bitcoin and stream both raw and transformed data into Snowflake, a storage platform, using the Twitter, HuggingFace and Snowflake connectors.</p> <p>I\u2019ve made a video of this tutorial if you prefer watching to reading.</p>"},{"location":"platform/tutorials/nocode-sentiment/nocode-sentiment-analysis/#what-you-need-for-this-tutorial","title":"What you need for this tutorial","text":"<ol> <li> <p>Free Quix account</p> </li> <li> <p>Snowflake account</p> </li> <li> <p>Twitter developer account     (You can follow this tutorial to set up a developer account)</p> </li> </ol>"},{"location":"platform/tutorials/nocode-sentiment/nocode-sentiment-analysis/#step-one-create-a-database","title":"Step one: create a database","text":"<p>Sign in to your Snowflake account to create the Snowflake database which will receive your data. Call this \"demodata\" and click \"Create.\"</p>"},{"location":"platform/tutorials/nocode-sentiment/nocode-sentiment-analysis/#step-two-get-your-data","title":"Step two: get your data","text":"<p>In Quix, click into the library and search for the Twitter source connector.</p> <p>Click \"Add new.\" This adds the source to your pipeline and brings you back to the library.</p> <p>Fill in the necessary fields:</p> <ul> <li> <p>Name: Twitter Data - Source</p> </li> <li> <p>Output: twitter-data</p> </li> <li> <p>Twitter bearer token: paste your Twitter Dev token here</p> </li> <li> <p>Twitter_search_paramaters: (#BTC OR btc #btc OR BTC)</p> </li> </ul> <p>Tip</p> <p>Use search parameters to obtain Tweets on a subject that interests you! e.g. (#dolphins OR dolphins)</p> <p>Click \"Deploy\"</p> <p></p>"},{"location":"platform/tutorials/nocode-sentiment/nocode-sentiment-analysis/#step-three-transformation-for-sentiment-analysis","title":"Step three: transformation for sentiment analysis","text":"<ul> <li> <p>Click the \"Add transformation\" button</p> </li> <li> <p>In the library, search for \"HuggingFace\"</p> </li> <li> <p>Click \"Set up and deploy\" on the HuggingFace connector</p> </li> <li> <p>Choose \"Twitter data\" as the input topic</p> </li> <li> <p>The output field should be set to \"hugging-face-output\" by default,     leave this as it is or enter this if it\u2019s not pre-populated.</p> </li> <li> <p>Leave all other values with their defaults</p> </li> <li> <p>Click \"Deploy\"</p> </li> </ul> <p>Note</p> <p>Find out more about the hugging face model and the other models you could use at huggingface.co</p> <p></p>"},{"location":"platform/tutorials/nocode-sentiment/nocode-sentiment-analysis/#step-five-delivery-to-your-snowflake-database","title":"Step five: delivery to your Snowflake database","text":"<ul> <li> <p>Click the \"Add destination\" button on the home screen</p> </li> <li> <p>Search the library for the Snowflake connector</p> </li> <li> <p>Click \"Set up and deploy\" on the connector</p> </li> <li> <p>Fill in the necessary fields:</p> </li> <li> <p>Choose the hugging-face-output output topic</p> </li> <li> <p>The \"Broker__TopicName\" field should be set to     \"hugging-face-output\". This means it will receive the data being     output by the sentiment analysis model.</p> </li> </ul> <p>To fill in the Snowflake locator and region (these are similar to a unique ID for your Snowflake instance), navigate to your Snowflake account. Copy the locator and region from the URL and paste them into the corresponding fields in the connector setup in Quix. Lastly, input your username and password.</p> <p></p> <p>Click \"Deploy\" on the Snowflake connector. If the credentials and connection details are correct, you\u2019ll see the \"Connected\" status in the log and will be redirected to your workspace.</p> <p></p> <p>Congratulations! You built a no-code pipeline that filters and collects data from Twitter, transforms it with a HuggingFace model and delivers it to a Snowflake database.</p> <p>You can now go back over to Snowflake and find the \"Databases\" menu. Expand the \"demodata\" database and then find the tables under \"public\".</p> <p></p> <p>Tip</p> <p>If you need help with this tutorial or want to chat about anything related to Quix or stream processing in general please come and say hi on The Stream, our slack community.</p>"},{"location":"platform/tutorials/quick-start/quick-start/","title":"Quickstart","text":"<p>This quickstart guide aims to get you using Quix effectively in the shortest possible time.</p> <p>It shows you how to deploy a real-time app in the Quix platform and highlights important aspects of the code. It also shows you how to create your own source and transformation, using Quix templates.</p> <p>Sign up for a free Quix account.</p> <p>For convenience this guide is divided into two parts:</p> <ol> <li> <p>Deploy a chat UI with sentiment analysis - in this part you deploy a real-time chat application and connect to it with your computer and phone. You also perform sentiment analysis on the messages in a chat room. </p> <p>This part does not require any coding, as it uses prebuilt library items for services, so is suitable for those who'd like to get an understanding of how Quix works, without needing to write code. </p> <p>The pipeline you create in this part is shown in the following screenshot:</p> <p></p> </li> <li> <p>Connect an external service - in this part you add an external data feed to the pipeline using library items that you will create from templates, and see data delivered in real time to your chat app.</p> <p>This part requires some basic Python coding skills. However, as all code you need is provided for you, this is suitable for beginners.</p> <p>The completed pipeline you create is shown in the following screenshot:</p> <p></p> </li> </ol>"},{"location":"platform/tutorials/quick-start/quick-start/#getting-help","title":"Getting help","text":"<p>If you need help with this guide, then please join our public Slack community <code>The Stream</code> and ask any questions you have there.</p>"},{"location":"platform/tutorials/quick-start/quick-start/#part-1-deploy-a-chat-ui-with-sentiment-analysis","title":"Part 1. Deploy a chat UI with sentiment analysis","text":"<p>To use Quix effectively in the shortest possible time, you will initially use prebuilt library items from the Quix Library. These open source library items have already been coded and tested by Quix engineers, and other contributors. All you have to do is configure them (if required) and deploy them to your workspace.</p>"},{"location":"platform/tutorials/quick-start/quick-start/#deploy-sentiment-analysis","title":"Deploy sentiment analysis","text":"<p>To compliment the chat UI, you will first deploy a prebuilt microservice designed to analyze the sentiment of messages in the chat.</p> <ol> <li> <p>Click <code>+ Add transformation</code> on the home screen.</p> Not your first time? <p>If this is not your first time deploying a service to this workspace then navigate to the Library using the left-hand navigation instead.</p> </li> <li> <p>Use the search box to find the <code>Sentiment analysis</code> library item. </p> </li> <li> <p>Click <code>Setup &amp; deploy</code>.</p> </li> <li> <p>Set <code>input</code> to the <code>messages</code> topic and <code>output</code> to the <code>sentiment</code> topic. You may need to create the required topics using the <code>new topic</code> dropdown.</p> </li> <li> <p>Click the <code>Deploy</code> button in the top right.</p> </li> <li> <p>In the <code>Deploy</code> dialog, in deployment settings panel, increase the CPU and memory to your maximum.</p> </li> <li> <p>Click <code>Deploy</code>.</p> </li> </ol> <p>Success</p> <p>You located and deployed the sentiment analysis microservice to the Quix platform.</p> <p>This microservice will subscribe to the <code>messages</code> topic and process data to determine the sentiment of any messages in real time. This sentiment value is published to the <code>sentiment</code> topic.</p>"},{"location":"platform/tutorials/quick-start/quick-start/#deploy-the-chat-ui","title":"Deploy the chat UI","text":"<p>You are going to locate and deploy a chat UI. The chat UI is written in Angular and connects to Quix using the Streaming Reader API. The chat UI enables you to see messages on both your phone and computer in real-time. The sentiment of each message is also displayed.</p> <ol> <li> <p>Navigate to the Quix Library using the left-hand navigation.</p> </li> <li> <p>Use the search box to find the <code>Sentiment Demo UI</code> library item.</p> </li> <li> <p>Click <code>Setup &amp; deploy</code>. Notice that this service will read from both the <code>messages</code> topic and the <code>sentiment</code> topic.</p> </li> <li> <p>Click <code>Deploy</code>. The <code>Deploy</code> dialog is displayed.</p> </li> <li> <p>Click the <code>Public Access</code> section of the <code>Deploy</code> dialog to expand it.</p> </li> <li> <p>Click the toggle switch to enable public access.</p> </li> <li> <p>Click <code>Deploy</code> on the dialog.</p> </li> </ol> <p>Success</p> <p>You located, and deployed the chat UI code to the Quix platform.</p> <p>The UI is comprised of a relatively simple Angular app that subscribes to Quix topics and streams chat messages and sentiment data in real time.</p> Understand the code <ol> <li> <p>Locate the <code>Sentiment Demo UI</code> item in the library again, and then click <code>Preview code</code>. This is just one way to access the code.</p> </li> <li> <p>Expand the tree view and select the <code>webchat.component.ts</code> file.</p> <p></p> </li> <li> <p>Locate the <code>connect()</code> method.</p> <p>Notice the <code>SubscribeToEvent</code> and <code>SubscribeToParameter</code> lines. These are used to tell Quix that the code should be notified as soon as data arrives. Specifically any data arriving for the specified topic, stream and event or parameter.</p> </li> <li> <p>Above the parameter and event subscriptions in the same file you will see the handlers. These will handle the data, doing whatever is needed for the app. In this case we add the messages to a list, which is then displayed in the UI.</p> <pre><code>this.quixService.readerConnection.on('EventDataReceived', (payload) =&gt; {...}\n\nthis.quixService.readerConnection.on('ParameterDataReceived', (payload) =&gt; {...}\n</code></pre> </li> </ol> <p>For more on connecting to Quix with a web-based UI, take a look at how to read and write with Node.js.</p>"},{"location":"platform/tutorials/quick-start/quick-start/#try-it-out","title":"Try it out","text":""},{"location":"platform/tutorials/quick-start/quick-start/#in-the-browser","title":"In the browser","text":"<p>Once the UI is built and deployed you can go ahead and click the  icon on the <code>Sentiment Demo UI</code> service tile.</p> <p>You will see a form asking you to enter the name for a chat room and your own name. </p> <ol> <li>Enter <code>MyRoom</code> for the room name (although it can be anything) and your name.</li> </ol> <p></p> <ol> <li>Click <code>Connect</code>.</li> </ol> <p>You will be redirected to the chat page.</p> <p>The most notable features of this page are the chat area, the sentiment graph, and the QR code.</p> <ol> <li> <p>Enter some positive and negative messages in the chat window.</p> </li> <li> <p>You will see your messages and a short time later the sentiment of the message will be indicated by the name tag next to each message changing color.</p> </li> </ol> <p></p>"},{"location":"platform/tutorials/quick-start/quick-start/#on-your-mobile","title":"On your mobile","text":"<p>Now join the chat with your mobile phone, chat messages will be displayed both on the phone and in the browser.</p> <ol> <li> <p>With your mobile phone, scan the QR code.</p> </li> <li> <p>Use the same room name as before <code>Room1</code>.</p> </li> <li> <p>Use a different name, such as <code>Mobile</code>.</p> </li> <li> <p>Type some messages.</p> </li> </ol> <p>You will see the message and its sentiment on your phone:</p> <p></p> <p>And the same messages and sentiment will appear in real time in your computer's web browser.</p>"},{"location":"platform/tutorials/quick-start/quick-start/#part-2-connect-an-external-service","title":"Part 2. Connect an external service","text":"<p>Now that you have the basics of searching the library for a library item, selecting and deploying it to the Quix serverless infrastructure, you can learn how to add additional services to the pipeline. In this guide you'll connect to a web service to receive data, and then transform it so it's compatible with the chat UI.</p>"},{"location":"platform/tutorials/quick-start/quick-start/#create-the-data-source","title":"Create the data source","text":"<p>In this section you will learn how to use a template to help quickly build a Quix source.</p> <ol> <li> <p>Go to the Quix Library.</p> </li> <li> <p>Search for the <code>Empty template - Source</code>. If should have a blue highlight (blue is used to indicate a source).</p> </li> <li> <p>Click <code>Preview code</code>.</p> </li> <li> <p>Click <code>Edit code</code>.</p> </li> <li> <p>Change the name to <code>API Data</code>.</p> </li> <li> <p>In the <code>output</code> field, click <code>new topic</code> to create a new topic called <code>api-data</code>.</p> </li> <li> <p>Click <code>Save as project</code>.</p> </li> </ol> <p>You now have a project for a Quix source you can modify to suit your own requirements. In this case the code will pull data periodically from an external REST API, using the <code>requests</code> library. To modify the template code:</p> <ol> <li> <p>Add <code>requests</code> on a new line to the <code>requirements.txt</code> file and save it. </p> <p>You use the <code>requests</code> library to fetch data from the web service. Any libraries included in the requirements file will automatically be included in the project build.</p> </li> <li> <p>Open the <code>main.py</code> file.</p> </li> <li> <p>Add the following import statement at the top of the file:</p> <pre><code>import requests\n</code></pre> </li> <li> <p>Delete the for-loop code between the following print statements:</p> <pre><code>print(\"Sending values for 30 seconds.\")\n# delete code here\nprint(\"Closing stream\")\n</code></pre> </li> <li> <p>Add the following code between those print statements:</p> <pre><code>while True:\n# get a random beer from this free API\nresponse = requests.get(\"https://random-data-api.com/api/v2/beers\")\n# print the response data\nprint(response.json())\n# sink the beer's `style` to Quix as an event\nstream.events.add_timestamp(datetime.datetime.utcnow()) \\\n    .add_value(\"beer\", response.json()[\"style\"]) \\\n    .write()\n# sleep for a bit\ntime.sleep(4)\n</code></pre> <p>This code performs a <code>GET</code> request to retrieve beer information from the REST API.</p> </li> <li> <p>Lastly, delete the following lines:</p> <pre><code>stream.parameters.add_definition(\"ParameterA\").set_range(-1.2, 1.2)\nstream.parameters.buffer.time_span_in_milliseconds = 100\n</code></pre> </li> <li> <p>Save and then run the code by clicking the <code>Run</code> button near the top right of the code editor window.</p> </li> </ol> <p>Every 4 seconds the random beer API is called, and a new style of beer is published to the Quix topic <code>api-data</code>.</p> <ol> <li>Click <code>Stop</code> to stop the code running (mouse over the <code>Running</code> button).</li> </ol> Understand the code <p>The complete code for <code>main.py</code> is shown here:</p> <pre><code>from quixstreaming import QuixStreamingClient\nimport time\nimport datetime\nimport os\nimport requests  # (1)\n# Quix injects credentials automatically to the client. Alternatively, you can always pass an SDK token manually as an argument.\nclient = QuixStreamingClient()\n# Open the output topic where to write data out\noutput_topic = client.open_output_topic(os.environ[\"output\"])\nstream = output_topic.create_stream()\nstream.properties.name = \"Hello World python stream\"\nprint(\"Sending values for 30 seconds.\")\nwhile True:\n# get a random beer from this free API\nresponse = requests.get(\"https://random-data-api.com/api/v2/beers\") # (2)\n# print the response data\nprint(response.json()) # (3)\n# sink the beer's `style` to Quix as an event (4)\nstream.events.add_timestamp(datetime.datetime.utcnow()) \\  \n    .add_value(\"beer\", response.json()[\"style\"]) \\\n    .write()\n# sleep for a bit\ntime.sleep(4) # (5)\nprint(\"Closing stream\")\nstream.close()\n</code></pre> <ol> <li>Import the <code>requests</code> library. You use this to make a <code>GET</code> request on the beer REST API.</li> <li>Make the request to the beer API endpoint. This is a blocking call.</li> <li>Print the response received from the API endpoint.</li> <li>Adds a timestamp and a value to the event data. This is then written to the output stream.</li> <li>Sleep for four seconds before looping back to make another request. </li> </ol>"},{"location":"platform/tutorials/quick-start/quick-start/#tag-and-deploy-the-api-data-service","title":"Tag and deploy the API Data service","text":"<p>To create a tagged version of your code:</p> <ol> <li> <p>Click the add tag button:</p> <p></p> </li> <li> <p>Enter <code>v1</code> and press Enter.</p> </li> </ol> <p>You have now create the <code>v1</code> tag. You'll select to deploy this version in the next section.</p> <p>Deploy the <code>v1</code> code so it will run continuously:</p> <ol> <li> <p>Click <code>Deploy</code> in the top right.</p> </li> <li> <p>In the <code>Version tag</code> drop-down, select <code>v1</code>.</p> </li> <li> <p>In the <code>Deployment Settings</code> panel, make sure the <code>Service</code> radio button is selected.</p> </li> <li> <p>Click <code>Deploy</code> on the dialog.</p> </li> </ol> <p>Success</p> <p>You have now created your own Quix source by modifying a standard source template, and deployed it.</p>"},{"location":"platform/tutorials/quick-start/quick-start/#transformation-of-api-data","title":"Transformation of API Data","text":"<p>Now that you have some data, you need to transform it to make it compatible with the rest of your data processing pipeline, in this case the chat UI and the sentiment analysis service.</p> <p>You will now locate a suitable transformation template and modify it to handle the incoming beer styles and output them as chat messages.</p> <ol> <li> <p>Search the library for <code>Empty template - Transformation</code>.</p> </li> <li> <p>Click <code>Preview code</code>.</p> </li> <li> <p>Click <code>Edit code</code>.</p> </li> <li> <p>Change the value of the <code>Name</code> field to <code>Beer to chat</code>.</p> </li> <li> <p>Change the <code>input</code> field to <code>api-data</code>. This is the topic you set as the output for the API data.</p> </li> <li> <p>In the <code>output</code> field, click <code>new topic</code> and create the <code>messages</code> topic.</p> </li> <li> <p>Click <code>Save as project</code>.</p> </li> </ol> <p>You have now saved the template to your workspace. In the next section you'll modify your code to suit your requirements.</p> <ol> <li> <p>Add the following import to the <code>quix_function.py</code> file:</p> <pre><code>import datetime\n</code></pre> <p>You'll need to include this module to use the <code>datetime</code> function to add a timestamp to your event data.</p> </li> <li> <p>Locate the <code>on_event_data_handler</code> method.</p> </li> <li> <p>Replace the comment <code># Here transform your data.</code> with the following code</p> <pre><code># stream chat-messages to the output topic\nself.output_stream.parameters.buffer.add_timestamp(datetime.datetime.utcnow()) \\\n    .add_value(\"chat-message\", data.value) \\\n    .add_tag(\"room\", \"Beer\") \\\n    .add_tag(\"name\", \"BeerAPI\") \\\n    .write()\n</code></pre> <p>This writes data to the output stream in the parameter data format.</p> </li> <li> <p>Delete the last line of the method:</p> <pre><code>self.output_stream.events.write(data)\n</code></pre> <p>This is not longer required as the previous method writes out the data in the parameter format.</p> </li> <li> <p>Save the file.</p> </li> <li> <p>Open the <code>main.py</code> file.</p> </li> <li> <p>Locate the following line:</p> <pre><code>output_stream = output_topic.create_stream(input_stream.stream_id)\n</code></pre> </li> <li> <p>Replace it with:</p> <pre><code>output_stream = output_topic.get_or_create_stream(\"beer\")\n</code></pre> <p>This code creates the output stream if it does not exist. The chat UI will write messages from this stream into the chat room with the same name.</p> </li> <li> <p>Save, tag, and deploy this project!</p> </li> </ol> <p>Success</p> <p>You have built a transformation to take output from an API and turn it into messages that the existing parts of the pipeline can use.</p>"},{"location":"platform/tutorials/quick-start/quick-start/#try-it-out_1","title":"Try it out","text":"<ol> <li> <p>Navigate to the UI you deployed earlier. Ensure you are in the <code>lobby</code>.</p> </li> <li> <p>Enter <code>beer</code> for the room name and provide any name for yourself.</p> </li> <li> <p>You can now see the messages arriving from the API as well as the calculated sentiment for them:</p> <p></p> </li> </ol>"},{"location":"platform/tutorials/quick-start/quick-start/#summary","title":"Summary","text":"<p>This quickstart guide aimed to give you a tour of some important Quix features. You have learned:</p> <ol> <li> <p>Quix enables you to build complex data processing pipelines, using prebuilt items from the Quix library.</p> </li> <li> <p>You can get data into and out of Quix using a variety of methods, including polling data, and using a websockets-based API such as the Streaming Reader API. Webhooks are also supported.</p> </li> <li> <p>You can use templates to help rapidly develop new library items. You built a new source and a new transformation from templates.</p> </li> <li> <p>Quix uses topics and streams to route data between services in a pipeline.</p> </li> <li> <p>You can build part of a Quix data processing pipeline, test it, and then extend the pipeline as required.</p> </li> </ol>"},{"location":"platform/tutorials/quick-start/quick-start/#next-steps","title":"Next steps","text":"<p>Try one of the following resources to continue your Quix learning journey:</p> <ul> <li> <p>Quix definitions</p> </li> <li> <p>The Stream community on Slack</p> </li> <li> <p>Stream processing glossary</p> </li> <li> <p>Sentiment analysis tutorial</p> </li> </ul>"},{"location":"platform/tutorials/rss-tutorial/rss-processing-pipeline/","title":"RSS Processing","text":""},{"location":"platform/tutorials/rss-tutorial/rss-processing-pipeline/#rss-processing-pipeline","title":"RSS processing pipeline","text":"<p>This tutorial explains how to build a pipeline that gathers and processes data from an RSS feed and alerts users when specific criteria are met. It\u2019s a companion to a live coding session on YouTube, where we sourced data from a StackOverflow RSS feed for use in a Python service.</p> <p>This tutorial has three parts</p> <ul> <li> <p>Sourcing data</p> </li> <li> <p>Processing data</p> </li> <li> <p>Sending alerts</p> </li> </ul> <p>What you need</p> <ul> <li> <p>A free Quix account. It     comes with enough credits to create this project.</p> </li> <li> <p>A Slack account with access to create a webhook. (This guide can help you with this step.)</p> </li> </ul>"},{"location":"platform/tutorials/rss-tutorial/rss-processing-pipeline/#sourcing-data","title":"Sourcing data","text":""},{"location":"platform/tutorials/rss-tutorial/rss-processing-pipeline/#1-get-the-rss-data-source-connector","title":"1. Get the \u201cRSS Data Source\u201d connector","text":"<p>In your Quix account, go to the library and search for \u201cRSS Data Source.\u201d (Hint: you can watch Steve prepare this code in the video tutorial if you\u2019re like to learn more about it.)</p> <p>Click \u201cSetup &amp; deploy\u201d on the \u201cRSS Data Source\u201d library item. (The card has a blue line across its top that indicates it\u2019s a source connector.)</p> <p></p>"},{"location":"platform/tutorials/rss-tutorial/rss-processing-pipeline/#2-configure-the-connector","title":"2. Configure the connector","text":"<p>In the configuration panel, keep the default name and output topic. Enter the following URL into the rss_url field: https://stackoverflow.com/feeds/tag/python</p> <p>Click \u201cDeploy\u201d and wait a few seconds for the pre-built connector to be deployed to your workspace.</p> <p>You will then begin to receive data from the RSS feed. The data then goes into the configured output topic. Don\u2019t worry, you won\u2019t lose data. It\u2019s cached in the topic until another deployment starts reading it.</p>"},{"location":"platform/tutorials/rss-tutorial/rss-processing-pipeline/#processing-data","title":"Processing data","text":"<p>You can do anything you want in the processing phase of a pipeline. You might want to merge several input streams or make decisions on your data. In this tutorial, you\u2019ll filter and augment data so that only questions with certain tags get delivered to you.</p>"},{"location":"platform/tutorials/rss-tutorial/rss-processing-pipeline/#1-get-the-rss-data-filtering-connector","title":"1. Get the \u201cRSS Data Filtering\u201d connector","text":"<p>Return to the library tab in Quix and search for \u201cRSS Data Filtering.\u201d Click \u201cSetup &amp; deploy\u201d on the card.</p> <p>If you created a new workspace for this project, the fields automatically populate. If you\u2019re using the workspace for other projects, you may need to specify the input topic as \u201crss-data.\u201d</p> <p>You might also want to customize the tag_filter. It is automatically populated with a wide range of tags related to Python. This works well for this demo, because you\u2019ll see a large return of interesting posts. But you can decrease or add tags.</p>"},{"location":"platform/tutorials/rss-tutorial/rss-processing-pipeline/#2-deploy-rss-data-filtering-connector","title":"2. Deploy \u201cRSS Data Filtering\u201d connector","text":"<p>Click \u201cDeploy\u201d on the \u201cRSS Data Filtering\u201d connector. Once deployed, the connector will begin processing the data that\u2019s been building up in the rss-data topic.</p> <p>Have a look in the logs by clicking the Data Filtering Model tile (pink outlined) on the workspace home page.</p> <p></p> <p>The transformation stage is now complete. Your project is now sending the filtered and enhanced data to the output topic.</p>"},{"location":"platform/tutorials/rss-tutorial/rss-processing-pipeline/#sending-alerts","title":"Sending alerts","text":"<p>Last in our pipeline is the destination for our RSS data. This demo uses a Slack channel as its destination.</p>"},{"location":"platform/tutorials/rss-tutorial/rss-processing-pipeline/#1-get-the-slack-notification-connector","title":"1. Get the \u201cSlack Notification\u201d connector","text":"<p>Return to the Quix library and search for the \u201cSlack Notification.\u201d Click \u201cPreview code.\u201d You\u2019re going to modify the standard code before deploying this connector.</p> <p></p> <p>Click \u201cNext\u201d on the dialog box. Ensure \u201cfiltered-rss-data\u201d is selected as the input topic and provide a Slack \u201cwebhook_url.\u201d</p> <p>Note</p> <p>If you have your own slack, head over to the Slack API pages and create a webhook following their guide \u201cGetting started with Incoming Webhooks.\u201d If you don\u2019t have your own Slack or don\u2019t have the account privileges to create the webhook, you can choose another destination from the library, such as Twilio.</p> <p>Warning: Use a dev or demo or unimportant Slack channel while you\u2019re developing this. Trust me.</p>"},{"location":"platform/tutorials/rss-tutorial/rss-processing-pipeline/#2-modify-and-deploy-the-slack-notification-connector","title":"2. Modify and deploy the \u201cSlack Notification\u201d connector","text":"<p>Enter your webhook into the webhook_url field. Click \u201cSave as project.\u201d This will save the code to your workspace, which is a GitLab repository.</p> <p>Once saved, you\u2019ll see the code again. The quix_function.py file should be open. This is what you\u2019ll alter. The default code dumps everything in the parameter data and event data to the Slack channel. It\u2019ll do to get you up and going, but we want something more refined. \ud83d\ude09</p> <p>Go to our GitHub library of tutorial code here. The code picks out several field values from the parameter data and combines them to form the desired Slack alert.</p> <p>Copy the code and paste it over the quix_function.py file in your project in the Quix portal.</p> <p>Save it by clicking \u201cCTRL+S\u201d or \u201cCommand + S\u201d or click the tick in the top right.</p> <p>Then deploy by clicking the \u201cDeploy\u201d button in the top right. On the dialogue, change the deployment type to \u201cService\u201d and click \u201cDeploy\u201d.</p>"},{"location":"platform/tutorials/rss-tutorial/rss-processing-pipeline/#congratulations","title":"Congratulations","text":"<p>You have deployed all three stages of the pipeline and should be receiving thousands of Slack messages. You might be thinking that Quix has led you on the path to destroying your Slack server. But don\u2019t worry \u2014 the pipeline is processing all the cached messages and will stop soon, honest. If this were a production pipeline, you\u2019d be very happy you haven\u2019t lost all those precious live messages.</p> <p></p>"},{"location":"platform/tutorials/rss-tutorial/rss-processing-pipeline/#help","title":"Help","text":"<p>If you run into trouble with the tutorial, want to chat with us about your project or anything else associated with streaming processing you can join our public Slack community called The Stream.</p>"},{"location":"platform/tutorials/sentiment-analysis/","title":"Sentiment analysis","text":"<p>In this tutorial you will learn how to build a real-time sentiment analysis pipeline. You'll deploy a UI, a sentiment analysis service, and then connect to Twitter to analyze a high-volume of Twitter data.</p> <p>This is the message processing pipeline you will build in this tutorial:</p> <p></p> <p>The completed project is capable of the sentiment analysis of a high volume of tweets, or your own chat messages, as illustrated in the following screenshot:</p> <p></p> <p>There are also optional parts of the tutorial where you can learn how to build your own sentiment analysis service rather than use a prebuilt service, and customize your UI, which all help to enhance your learning of key Quix concepts.</p> <p>Tip</p> <p>If you need any assistance, we\u2019re here to help in The Stream, our free Slack community. Introduce yourself and then ask any questions in <code>quix-help</code>.</p>"},{"location":"platform/tutorials/sentiment-analysis/#the-parts-of-the-tutorial","title":"The parts of the tutorial","text":"<p>This tutorial is divided up into several parts, to make it a more manageable learning experience. The parts are summarized here:</p> <ol> <li> <p>Build your UI. You deploy the Sentiment Demo UI. This is the UI for the tutorial, it allows the user to see messages from all of the users of the app and, in later parts of the tutorial, allow the users to see the sentiment of the chat messages.</p> </li> <li> <p>Deploy a sentiment analysis microservice. You configure and deploy a microservice in your pipeline capable of Analyzing the sentiment of the messages sent through the Sentiment Demo UI.</p> </li> <li> <p>Extend your pipeline to handle Twitter data. In this part, you can increase the volume of messages by using the Twitter integration. You deploy a data source that subscribes to Twitter messages and then publishes them to the Sentiment Demo UI. Sentiment is then determined in real-time.</p> </li> <li> <p>Summary. In this concluding part you are presented with a summary of the work you have completed, and also some next steps for more advanced learning about the Quix Platform. These additional items are listed next.</p> </li> <li> <p>Build a sentiment analysis microservice. In this optional part, you'll build your own sentiment analysis microservice, rather than use a prebuilt service.</p> </li> <li> <p>Customize the UI. In this optional part you learn how to customize the Sentiment Demo UI.</p> </li> </ol> <p>Deploy the first part of the solution by following step 1 </p>"},{"location":"platform/tutorials/sentiment-analysis/analyze/","title":"2. Analyzing sentiment","text":"<p>In Part 1 you deployed the Sentiment Demo UI, interacted with the UI to send messages and view messages of other users, and saw those messages displayed in the UI in real time.</p> <p>In this part of the tutorial you analyze the sentiment of the conversation by adding a new node to the processing pipeline. </p> <p>This sentiment analysis microservice utilizes a prebuilt model from huggingface.co to analyze the sentiment of each message flowing through the microservice.</p> <p>The microservice subscribes to data from the <code>messages</code> topic and publishes sentiment results to the <code>sentiment</code> topic.</p> <p>Tip</p> <p>While this tutorial uses a prebuilt sentiment analysis library item, it is also possible to build one from a basic template available in the Quix library. If you are interested in building your own service, you can refer to an optional part of this tutorial, where you learn how to code a sentiment analysis service from the basic template.</p>"},{"location":"platform/tutorials/sentiment-analysis/analyze/#deploying-the-sentiment-analysis-service","title":"Deploying the sentiment analysis service","text":"<p>The sentiment of each message will be evaluated by this new microservice in your message processing pipeline.</p> <p>Follow these steps to deploy the prebuilt sentiment analysis microservice:</p> <ol> <li> <p>Navigate to the Library and search for <code>Sentiment analysis</code>.</p> </li> <li> <p>Click the <code>Setup &amp; deploy</code> button.</p> </li> <li> <p>Ensure the \"input\" is set to <code>messages</code>. </p> <p>This is the topic that is subscribed to for messages to analyze.</p> </li> <li> <p>Ensure the \"output\" is set to <code>sentiment</code>. </p> <p>This is the topic that sentiment results are published to.</p> </li> <li> <p>Click the <code>Deploy</code> button. </p> </li> </ol> <p>This deploys the service using the default settings. If you later find that this microservice is not performing as expected, then you can subsequently edit the deployment, and increase the resources allocated.  </p> <ol> <li> <p>Navigate to the web page for the UI project you deployed in Part 1. </p> </li> <li> <p>Enter values for <code>Room</code> and <code>Name</code> and click <code>CONNECT</code>, or re-enter the room.</p> </li> <li> <p>Now enter chat messages and see the sentiment being updated in real time each time a message is posted. An example of this is shown in the following screenshot:</p> <p></p> </li> </ol> <p>The sentiment analysis service you just deployed subscribes to the <code>messages</code> topic. The sentiment is returned to the UI through the <code>sentiment</code> topic, and displayed both in the chart and next to the comment in the chat window by colorizing the chat user's name.</p> <p>Success</p> <p>You have added to the pipeline by building and deploying a microservice to analyze the chat messages in real time.</p> <p>Subscribe to Tweets from Twitter by following Part 3 of this tutorial </p>"},{"location":"platform/tutorials/sentiment-analysis/code-and-deploy-sentiment-service/","title":"Sentiment analysis microservice","text":"<p>In this optional tutorial part, you learn how to code a sentiment analysis microservice, starting with a template from the Quix Library. Templates are useful building blocks the Quix platform provides, and which give you a great starting point from which to build your own microservices.</p> <p>Note</p> <p>The code shown here is kept as simple as possible for learning purposes. Production code would require more robust error handling.</p>"},{"location":"platform/tutorials/sentiment-analysis/code-and-deploy-sentiment-service/#prerequisites","title":"Prerequisites","text":"<p>It is assumed that you have a data source such as the Sentiment Demo UI used in the Sentiment Analysis tutorial. It supplies data to a <code>messages</code> topic and has a <code>chat-message</code> column in the dataset.</p> <p>Follow the steps below to code, test, and deploy a new microservice to your workspace.</p>"},{"location":"platform/tutorials/sentiment-analysis/code-and-deploy-sentiment-service/#select-the-template","title":"Select the template","text":"<p>Follow these steps to locate and save the code to your workspace:</p> <ol> <li> <p>Navigate to the Library and apply the following filters:</p> <ol> <li> <p>Languages = <code>Python</code></p> </li> <li> <p>Pipeline Stage = <code>Transformation</code></p> </li> <li> <p>Type = <code>Basic templates</code></p> </li> </ol> </li> <li> <p>Select <code>Empty template - Transformation</code>.</p> <ul> <li>This is a simple example of how to subscribe and publish messages to Quix.</li> <li>You can't edit anything here, this is a read-only view so you can explore the files in the template and see what each one does.</li> </ul> </li> <li> <p>Click <code>Preview code</code> then <code>Edit code</code>.</p> </li> <li> <p>Change the name to <code>Sentiment analysis</code>.</p> </li> <li> <p>Ensure the \"input\" is set to <code>messages</code>.</p> <p>This is the topic that is subscribed to for messages to analyze.</p> </li> <li> <p>Ensure the \"output\" is set to <code>sentiment</code>.</p> <p>This is the topic that sentiment results are published to.</p> </li> <li> <p>Click <code>Save as project</code>.</p> <p>The code is now saved to your workspace, you can edit and run it as needed before deploying it into the Quix production-ready, serverless, and scalable environment.</p> </li> </ol>"},{"location":"platform/tutorials/sentiment-analysis/code-and-deploy-sentiment-service/#development-lifecycle","title":"Development lifecycle","text":"<p>You're now located in the Quix online development environment, where you will develop the code to analyze the sentiment of each message passing through the pipeline. The following sections step through the development process for this tutorial:</p> <ol> <li>Running the unedited code</li> <li>Creating a simple transformation to test your code</li> <li>Implementing the sentiment analysis code</li> <li>Running the sentoment analysis code</li> </ol>"},{"location":"platform/tutorials/sentiment-analysis/code-and-deploy-sentiment-service/#running-the-code","title":"Running the code","text":"<p>Begin by running the code as it is, using the following steps:</p> <ol> <li> <p>To get started with this code, click the <code>run</code> button near the top right of the code window.</p> <p>You'll see the message below in the console output:</p> <pre><code>Opening input and output topics\nListening to streams. Press CTRL-C to exit.\n</code></pre> </li> <li> <p>Open the Chat App UI you deployed in part 1 of this tutorial and send some messages.</p> <p>You will see output similar to this:</p> <pre><code>Opening input and output topics\nListening to streams. Press CTRL-C to exit.\n                time  ... TAG__email\n0  1670349744309000000  ...           [1 rows x 7 columns]\n</code></pre> <p>This is the Panda DataFrame printed to the console.</p> </li> <li> <p>To enable you to view the messages more easily you can click the \"Messages\" tab and send another message from the UI.</p> <p>You will see messages arriving in the messages tab:</p> <p></p> <p>Now click one of the messages. You will see the JSON formatted message showing the various parts of the message payload, for example, the \"chat-message\" and \"room\":</p> <p></p> </li> </ol>"},{"location":"platform/tutorials/sentiment-analysis/code-and-deploy-sentiment-service/#creating-a-simple-transformation","title":"Creating a simple transformation","text":"<p>Now that you know the code can subscribe to messages, you need to transform the messages and publish them to an output topic.</p> <ol> <li> <p>If your code is still running, stop by clicking the same button you used to run it.</p> </li> <li> <p>Locate the <code>on_pandas_frame_handler</code> in <code>quix_function.py</code>.</p> </li> <li> <p>Replace the comment <code># Here transform your data.</code> with the code below:</p> <pre><code># transform \"chat-message\" column to uppercase\ndf[\"chat-message\"] = df[\"chat-message\"].str.upper()\n</code></pre> </li> <li> <p>Run the code again, and send some more chat messages from the UI.</p> </li> <li> <p>The messages in the UI are now all in uppercase as a result of your transformation.</p> </li> </ol> <p>Don't forget to stop the code again.</p>"},{"location":"platform/tutorials/sentiment-analysis/code-and-deploy-sentiment-service/#sentiment-analysis","title":"Sentiment analysis","text":"<p>Now it's time to update the code to perform the sentiment analysis.</p>"},{"location":"platform/tutorials/sentiment-analysis/code-and-deploy-sentiment-service/#requirementstxt","title":"requirements.txt","text":"<ol> <li> <p>Select the <code>requirements.txt</code> file.</p> </li> <li> <p>Add a new line, insert the following text and save the file:</p> <pre><code>transformers[torch]\n</code></pre> </li> </ol>"},{"location":"platform/tutorials/sentiment-analysis/code-and-deploy-sentiment-service/#mainpy","title":"main.py","text":"<p>Follow these steps to make the necessary changes:</p> <ol> <li> <p>Locate the file <code>main.py</code>.</p> </li> <li> <p>Import <code>pipeline</code> from <code>transformers</code>:</p> <pre><code>from transformers import pipeline\n</code></pre> </li> <li> <p>Create the <code>classifier</code> property and set it to a new pipeline:</p> <pre><code>classifier = pipeline('sentiment-analysis')\n</code></pre> What's this <code>pipeline</code> thing? <p>The pipeline object comes from the transformers library. It's a library used to integrate huggingface.co models.</p> <p>The pipeline object contains several transformations in series, including cleaning and transforming to using the prediction model, hence the term <code>pipeline</code>.</p> <p>When you initialize the pipeline object you specify the model you want to use for predictions.</p> <p>You specified <code>sentiment-analysis</code> which directs huggingface to provide their standard one for sentiment analysis.</p> </li> <li> <p>Locate the <code>read_stream</code> method and pass the <code>classifier</code> property into the <code>QuixFunction</code> initializer as the last parameter:</p> <p>The <code>QuixFunction</code> initialization should look like this: <pre><code># handle the data in a function to simplify the example\nquix_function = QuixFunction(input_stream, output_stream, classifier)\n</code></pre></p> </li> </ol> The completed <code>main.py</code> should look like this <pre><code>from quixstreaming import QuixStreamingClient, StreamEndType, StreamReader, AutoOffsetReset\nfrom quixstreaming.app import App\nfrom quix_function import QuixFunction\nimport os\nfrom transformers import pipeline\nclassifier = pipeline('sentiment-analysis')\n# Quix injects credentials automatically to the client. Alternatively, you can always pass an SDK token manually as an argument.\nclient = QuixStreamingClient()\n# Change consumer group to a different constant if you want to run model locally.\nprint(\"Opening input and output topics\")\ninput_topic = client.open_input_topic(os.environ[\"input\"], auto_offset_reset=AutoOffsetReset.Latest)\noutput_topic = client.open_output_topic(os.environ[\"output\"])\n# Callback called for each incoming stream\ndef read_stream(input_stream: StreamReader):\n# Create a new stream to output data\noutput_stream = output_topic.create_stream(input_stream.stream_id)\noutput_stream.properties.parents.append(input_stream.stream_id)\n# handle the data in a function to simplify the example\nquix_function = QuixFunction(input_stream, output_stream, classifier)\n# React to new data received from input topic.\ninput_stream.events.on_read += quix_function.on_event_data_handler\ninput_stream.parameters.on_read_pandas += quix_function.on_pandas_frame_handler\n# When input stream closes, we close output stream as well. \ndef on_stream_close(endType: StreamEndType):\noutput_stream.close()\nprint(\"Stream closed:\" + output_stream.stream_id)\ninput_stream.on_stream_closed += on_stream_close\n# Hook up events before initiating read to avoid losing out on any data\ninput_topic.on_stream_received += read_stream\n# Hook up to termination signal (for docker image) and CTRL-C\nprint(\"Listening to streams. Press CTRL-C to exit.\")\n# Handle graceful exit of the model.\nApp.run()\n</code></pre>"},{"location":"platform/tutorials/sentiment-analysis/code-and-deploy-sentiment-service/#quix_functionpy","title":"quix_function.py","text":"<p>You have completed the changes needed in <code>main.py</code>, now you need to update <code>quix_function.py</code>.</p>"},{"location":"platform/tutorials/sentiment-analysis/code-and-deploy-sentiment-service/#imports","title":"imports","text":"<ol> <li> <p>Select the <code>quix_function.py</code> file.</p> </li> <li> <p>Add the following to the top of the file under the existing imports:</p> <pre><code>from transformers import Pipeline\n</code></pre> </li> </ol>"},{"location":"platform/tutorials/sentiment-analysis/code-and-deploy-sentiment-service/#init-function","title":"init function","text":"<ol> <li> <p>Add the following parameter to the <code>__init__</code> function:</p> <pre><code>classifier: Pipeline\n</code></pre> <p>You will pass this in from the <code>main.py</code> file in a moment.</p> </li> <li> <p>Initialize the <code>classifier</code> property with the passed in parameter:</p> <pre><code>self.classifier = classifier\n</code></pre> </li> <li> <p>Initialize <code>sum</code> and <code>count</code> properties:</p> <pre><code>self.sum = 0\nself.count = 0\n</code></pre> <p>init</p> <p>The completed <code>__init__</code> function should look like this:</p> <pre><code>def __init__(self, input_stream: StreamReader, output_stream: StreamWriter, classifier: Pipeline):\nself.input_stream = input_stream\nself.output_stream = output_stream\nself.classifier = classifier\nself.sum = 0\nself.count = 0\n</code></pre> </li> </ol>"},{"location":"platform/tutorials/sentiment-analysis/code-and-deploy-sentiment-service/#on_pandas_frame_handler-function","title":"on_pandas_frame_handler function","text":"<p>Now, following these steps, edit the code to calculate the sentiment of each chat message using the classifier property you set in the init function.</p> <ol> <li> <p>Locate the <code>on_pandas_frame_handler</code> function you added code to earlier.</p> </li> <li> <p>Change the <code>on_pandas_frame_handler</code> function to the following code:</p> <pre><code># Callback triggered for each new parameter data.\ndef on_pandas_frame_handler(self, df_all_messages: pd.DataFrame):\n# Use the model to predict sentiment label and confidence score on received messages\nmodel_response = self.classifier(list(df_all_messages[\"chat-message\"]))\n# Add the model response (\"label\" and \"score\") to the pandas dataframe\ndf = pd.concat([df_all_messages, pd.DataFrame(model_response)], axis=1)\n# Iterate over the df to work on each message\nfor i, row in df.iterrows():\n# Calculate \"sentiment\" feature using label for sign and score for magnitude\ndf.loc[i, \"sentiment\"] = row[\"score\"] if row[\"label\"] == \"POSITIVE\" else - row[\"score\"]\n# Add average sentiment (and update memory)\nself.count = self.count + 1\nself.sum = self.sum + df.loc[i, \"sentiment\"]\ndf.loc[i, \"average_sentiment\"] = self.sum/self.count\n# Output data with new features\nself.output_stream.parameters.write(df)\n</code></pre> <p>This is the heart of the sentiment analysis processing code. It analyzes the sentiment of each message and tracks the average sentiment of the whole conversation. The code works as follows:</p> <ol> <li> <p>Pass a list of all of the \"chat messages\" in the data frame to the classifier (the sentiment analysis model) and store the result in memory.</p> </li> <li> <p>Concatenate (or add) the model response data to the original data frame.</p> </li> <li> <p>For each row in the data frame:</p> <ol> <li> <p>Use the <code>label</code>, obtained from running the model, which is either <code>POSITIVE</code> or <code>NEGATIVE</code> together with the <code>score</code> to assign either <code>score</code> or <code>- score</code> to the <code>sentiment</code> column.</p> </li> <li> <p>Maintain the count of all messages and total of the sentiment for all messages so that the average sentiment can be calculated.</p> </li> <li> <p>Calculate and assign the average sentiment to the <code>average_sentiment</code> column in the data frame.</p> </li> </ol> </li> </ol> </li> </ol> The completed <code>quix_function.py</code> should look like this <pre><code>from quixstreaming import StreamReader, StreamWriter, EventData, ParameterData\nimport pandas as pd\nfrom transformers import Pipeline\nclass QuixFunction:\ndef __init__(self, input_stream: StreamReader, output_stream: StreamWriter, classifier: Pipeline):\nself.input_stream = input_stream\nself.output_stream = output_stream\nself.classifier = classifier\nself.sum = 0\nself.count = 0\n# Callback triggered for each new event.\ndef on_event_data_handler(self, data: EventData):\nprint(data.value)\nprint(\"events\")\n# Callback triggered for each new parameter data.\ndef on_pandas_frame_handler(self, df_all_messages: pd.DataFrame):\n# Use the model to predict sentiment label and confidence score on received messages\nmodel_response = self.classifier(list(df_all_messages[\"chat-message\"]))\n# Add the model response (\"label\" and \"score\") to the pandas dataframe\ndf = pd.concat([df_all_messages, pd.DataFrame(model_response)], axis=1)\n# Iterate over the df to work on each message\nfor i, row in df.iterrows():\n# Calculate \"sentiment\" feature using label for sign and score for magnitude\ndf.loc[i, \"sentiment\"] = row[\"score\"] if row[\"label\"] == \"POSITIVE\" else - row[\"score\"]\n# Add average sentiment (and update memory)\nself.count = self.count + 1\nself.sum = self.sum + df.loc[i, \"sentiment\"]\ndf.loc[i, \"average_sentiment\"] = self.sum/self.count\n# Output data with new features\nself.output_stream.parameters.write(df)\n</code></pre>"},{"location":"platform/tutorials/sentiment-analysis/code-and-deploy-sentiment-service/#running-the-completed-code","title":"Running the completed code","text":"<p>Now that the code is complete you can <code>Run</code> it one more time, just to be certain it's doing what you expect.</p> <p>Note</p> <p>This time, when you run the code, it will start-up and then immediately download the <code>sentiment-analysis</code> model from huggingface.co </p> <ol> <li> <p>Click <code>Run</code>.</p> </li> <li> <p>Click the <code>Messages</code> tab and select the <code>output</code> topic called <code>sentiment</code>.</p> </li> <li> <p>Send some \"Chat\" messages from the Chat App UI.</p> </li> <li> <p>Now select a row in the <code>Messages</code> tab and inspect the JSON message.</p> <p>You will see the <code>sentiment</code> and <code>average_sentiment</code> in the <code>NumericValues</code> section and the <code>chat-message</code> and <code>label</code> in the <code>StringValues</code> section:</p> <p></p> </li> <li> <p>You can also verify that the Web Chat UI shows an indication of the sentiment for each message as well as showing the average sentiment in the graph:</p> <p></p> </li> </ol>"},{"location":"platform/tutorials/sentiment-analysis/code-and-deploy-sentiment-service/#deploying-your-sentiment-analysis-code","title":"Deploying your sentiment analysis code","text":"<p>Now that the sentiment analysis stage is working as expected you can deploy it to the Quix serverless environment.</p> <p>Info</p> <p>If you're thinking that it's already running, so why do you need to bother with this extra step, you should know that the code is currently running in a development sandbox environment. This is separate from the production environment, and is not scalable or resilient. Its main purpose is to allow you to iterate on the development cycle of your Python code, and make sure it runs without error, before deployment.</p> <p>Tag the code and deploy the service:</p> <ol> <li> <p>Click the <code>+tag</code> button at the top of the code file.</p> </li> <li> <p>Enter <code>v1</code> and press Enter.</p> <p>This tags the code with a specific identifier and allows you to know exactly which version of the code you are deploying.</p> </li> <li> <p>Click <code>Deploy</code> near the top right corner.</p> </li> <li> <p>Select <code>v1</code> under the <code>Version Tag</code>.</p> <p>This is the same tag you created in step 2.</p> </li> <li> <p>In <code>Deployment settings</code> change the CPU to 1 and the Memory to 1. </p> <p>This ensures the service has enough resources to download and store the hugging face model and to efficiently process the messages. If you are on the free tier, you can try things out with your settings on the maximum for CPU and Memory.</p> </li> <li> <p>Click <code>Deploy</code>.</p> <ul> <li>Once the service has been built and deployed it will be started. </li> <li>The first thing it will do is download the hugging face model for <code>sentiment-analysis</code>.</li> <li>Then the input and output topics will be opened and the service will begin listening for messages to process.</li> </ul> </li> <li> <p>Go back to the UI, and make sure everything is working as expected. Your messages will have a color-coded sentiment, and the sentiment will displayed on the graph.</p> </li> </ol> <p>You have now completed this optional tutorial part. You have learned how to create your own sentiment analysis microservice from the library template.</p>"},{"location":"platform/tutorials/sentiment-analysis/conclusion/","title":"Conclusion","text":"<p>You\u2019ve just made extensive use of the Quix library, our collection of open source connectors, and examples, to deploy a UI and sentiment analysis microservice, and subscribe to Tweets.</p> <p>Congratulations, that's quite an achievement!</p>"},{"location":"platform/tutorials/sentiment-analysis/conclusion/#next-steps","title":"Next Steps","text":"<p>Here are some suggested next steps to continue on your Quix learning journey:</p> <ul> <li> <p>If you want to build your own sentiment analysis service, rather than use a prebuilt service, you can learn how in the optional tutorial part how to code a sentiment analysis service.</p> </li> <li> <p>If you want to customize the Sentiment Demo UI, you can learn how in the optional tutorial part how to customize the UI.</p> </li> <li> <p>If you decide to build your own connectors and apps, you can contribute something to the Quix Library. Visit the Quix GitHub. Fork our library repo and submit your code, updates, and ideas.</p> </li> </ul> <p>What will you build? Let us know! We\u2019d love to feature your project or use case in our newsletter.</p>"},{"location":"platform/tutorials/sentiment-analysis/conclusion/#getting-help","title":"Getting help","text":"<p>If you need any assistance, we're here to help in The Stream, our free Slack community. Introduce yourself and then ask any questions in <code>quix-help</code>.</p>"},{"location":"platform/tutorials/sentiment-analysis/customize-the-ui/","title":"Customizing the Sentiment Demo UI","text":"<p>In this optional tutorial part, you learn how to customize the Sentiment Demo UI.</p> <p>If you want to customize the Sentiment Demo UI, you would follow three main steps:</p> <ol> <li>Create the new project from the existing UI code.</li> <li>Modify the code in your next project as required to customize the UI.</li> <li>Deploy the modified UI.</li> </ol>"},{"location":"platform/tutorials/sentiment-analysis/customize-the-ui/#create-the-project","title":"Create the project","text":"<ol> <li> <p>Navigate to the Library and locate <code>Sentiment Demo UI</code>.</p> </li> <li> <p>Click <code>Preview code</code> and then <code>Edit code</code>.</p> </li> <li> <p>Ensure that the <code>sentiment</code> input box contains <code>sentiment</code>.</p> <p>This topic will be subscribed to and will contain the sentiment scores from the sentiment analysis service, you'll deploy this in a later part of this tutorial.</p> </li> <li> <p>Ensure that the <code>messages</code> input contains <code>messages</code>.</p> <ul> <li>This topic will contain all the chat messages.</li> <li>The UI will subscribe to this topic, to display new messages, as well as publishing to the topic when a user sends a message using the 'send' button in the UI.</li> <li>Later, the sentiment analysis service will also subscribe to messages on this topic to produce sentiment scores.</li> </ul> </li> <li> <p>Click <code>Save as project</code>. </p> <p>The code for this Angular UI is now saved to your workspace.</p> </li> </ol> <p>You have created the project and you can now modifiy the code as required.</p>"},{"location":"platform/tutorials/sentiment-analysis/customize-the-ui/#modify-the-code","title":"Modify the code","text":"<p>At this stage if you want to customize the code you can do so. You can also deploy what you have and customize it later by repeating the steps in the following section.</p>"},{"location":"platform/tutorials/sentiment-analysis/customize-the-ui/#deploy-your-modified-code","title":"Deploy your modified code","text":"<ol> <li> <p>Click the <code>+tag</code> button at the top of any code file.</p> </li> <li> <p>Enter <code>v1</code> and press Enter.</p> </li> <li> <p>Click <code>Deploy</code> near the top right corner.</p> </li> <li> <p>In the deployment dialog, select your tag, for example, <code>v1</code> under the <code>Version Tag</code>.</p> <p>This is the tag you just created.</p> </li> <li> <p>Click <code>Service</code> in <code>Deployment Settings</code>.</p> <p>This ensures the service runs continuously.</p> </li> <li> <p>Click the toggle in <code>Public Access</code>.</p> <p>This enables access from anywhere on the internet.</p> </li> <li> <p>Click <code>Deploy</code>.</p> <ul> <li>The UI will stream data from the <code>sentiment</code> and <code>messages</code> topics as well as send messages to the <code>messages</code> topic.</li> <li>The <code>sentiment</code> topic will be used later for sentiment analysis.</li> </ul> </li> </ol> <p>In this tutorial you've learned how you can modify the Sentiment Demo UI.</p>"},{"location":"platform/tutorials/sentiment-analysis/sentiment-demo-ui/","title":"1. Sentiment Demo UI","text":"<p>The Sentiment Demo UI is the UI for the tutorial and enables the user to see messages from all of the users of the app and, in later parts of this tutorial, allows the users to see the sentiment of the chat messages.</p> <p>The UI you will build in this part of the tutorial is shown in the following screenshot:</p> <p></p>"},{"location":"platform/tutorials/sentiment-analysis/sentiment-demo-ui/#creating-the-gateways","title":"Creating the gateways","text":"<p>Gateways provide a way for external apps to subscribe and publish to topics, and help visualize those connections in the pipeline view of the Quix platform. An example of their use is shown in the following screenshot:</p> <p></p> <p>In this scenario, the Sentiment Demo UI is the external app since it is using the Quix websockets API.</p> <p>Follow these steps to create the messages and sentiment web gateways:</p> <ol> <li> <p>Make sure the <code>sentiment</code> topic is available. Click <code>Topics</code> on the main left-hand navigation, and click <code>+ Create topic</code>, enter <code>sentiment</code>, and then click <code>Create</code>. </p> <p>This topic needs to be available so you can select it in a later step.</p> </li> <li> <p>Navigate to the Library and locate <code>External Source</code>.</p> </li> <li> <p>Click <code>Add external source</code>.</p> </li> <li> <p>In the <code>name</code> field enter <code>Chat messages WebGateway</code>. </p> </li> <li> <p>Select or enter <code>messages</code> in the <code>output</code> field.</p> </li> <li> <p>Click <code>Add external Source</code> to create the external source.</p> </li> <li> <p>Navigate to the Library and locate <code>External Destination</code>.</p> </li> <li> <p>Click <code>Add external destination</code>.</p> </li> <li> <p>In the <code>name</code> field enter <code>Chat sentiment WebGateway</code>. </p> </li> <li> <p>Select <code>sentiment</code> in the <code>input</code> field.</p> </li> <li> <p>Click <code>Add external Destination</code> to create the external destination.</p> </li> </ol> <p>You've now created the gateways needed for this tutorial.</p>"},{"location":"platform/tutorials/sentiment-analysis/sentiment-demo-ui/#locating-and-deploying-the-sentiment-demo-ui","title":"Locating and deploying the Sentiment Demo UI","text":"<p>The following steps demonstrate how to select the demo UI Library item and deploy it to your Quix workspace. </p> <p>Follow these steps to deploy the prebuilt UI:</p> <ol> <li> <p>Navigate to the Library and locate <code>Sentiment Demo UI</code>.</p> </li> <li> <p>Click the <code>Setup &amp; deploy</code> button.</p> </li> <li> <p>Ensure that the <code>sentiment</code> input box contains <code>sentiment</code>.</p> <p>This topic will be subscribed to and will contain the sentiment scores from the sentiment analysis service, you'll deploy this in a later part of this tutorial.</p> </li> <li> <p>Ensure that the <code>messages</code> input contains <code>messages</code>.</p> <ul> <li>This topic will contain all the chat messages.</li> <li>The UI will subscribe to this topic, to display new messages, as well as publishing to the topic when a user sends a message using the <code>send</code> button in the UI.</li> <li>Later, the sentiment analysis service will also subscribe to messages on this topic to produce sentiment scores.</li> </ul> </li> <li> <p>Click <code>Deploy</code>.</p> </li> </ol> <p>You've now Deployed the Sentiment Demo UI.</p>"},{"location":"platform/tutorials/sentiment-analysis/sentiment-demo-ui/#trying-out-the-ui","title":"Trying out the UI","text":"<p>Now try out the UI you just deployed. </p> <ol> <li> <p>Find the URL for the deployed UI by navigating to the homepage and locating the tile representing the deployed UI, as shown in the following screenshot:</p> <p></p> </li> <li> <p>Click the <code>open in new window</code> icon .</p> <p>This is the user interface for the demo. The view you\u2019ll see after creating a <code>room</code> to chat in is shown in the following screenshot:</p> <p></p> </li> <li> <p>Now enter some messages. They will be displayed in the chat list.</p> </li> <li> <p>To make the demo more entertaining, use your phone to scan the QR code, or send a link to this page to a friend or colleague. When they interact you'll see their chat messages appear in your UI in real time!</p> </li> </ol> <p>Success</p> <p>You have successfully deployed and tested the UI.</p> <p>Analyze the sentiment of your messages by following Part 2 of this tutorial </p>"},{"location":"platform/tutorials/sentiment-analysis/twitter-data/","title":"3. Adding Twitter data","text":"<p>In the previous part of this tutorial you deployed a microservice to analyze the sentiment of messages in the chat. </p> <p>In this part of the tutorial you will learn how to:</p> <ol> <li>Deploy a data source that subscribes to Twitter messages.</li> <li>Deploy a new microservice to normalize the Twitter messages, making them compatible with the sentiment analysis microservice and UI you have already deployed. The sentiment of all the messages will be determined in real time.</li> </ol> <p>The objective is to show you how to integrate with an external system, and demonstrate the sentiment analysis service processing a higher volume of messages.</p> <p></p> <p>If you're asking \"Why Twitter?\" it's a good question. Quix has a great Twitter connector and want to show it off! Plus it allows you to source real-world data at volume (if you choose the right search parameters).</p> <p>There are two steps in this part of the tutorial:</p> <ol> <li>Fetching the tweets.</li> <li>Transforming the tweets to ensure they're compatible with the Sentiment Demo UI.</li> </ol>"},{"location":"platform/tutorials/sentiment-analysis/twitter-data/#prerequisites","title":"Prerequisites","text":"<p>To complete this part of the tutorial you'll need a Twitter developer account.</p> <p>You can follow this tutorial to set up a developer account.</p>"},{"location":"platform/tutorials/sentiment-analysis/twitter-data/#fetching-the-tweets","title":"Fetching the tweets","text":"<p>You are going to be using a prebuilt library item for fetching the tweets. The default search parameters for the library item search for anything to do with Bitcoin, using the search term <code>(#BTC OR btc OR #btc OR BTC)</code>. It's a high-traffic subject and great for this demo. However, if you are on the Quix free tier, you might find it better to use a lower-traffic subject, as less CPU and Memory resource can be allocated to a deployment on this tier. To do this, you can edit the <code>twitter_search_params</code> field in the library item to contain a different search term, such as <code>(#rail OR railway)</code>. This will create less load on the sentiment analysis microservice.</p> <p>Follow these steps to deploy the Twitter data source:</p> <ol> <li> <p>Navigate to the Library and locate the <code>Twitter</code> data source.</p> </li> <li> <p>Click the <code>Setup &amp; deploy</code> button.</p> </li> <li> <p>Enter your Twitter bearer token into the <code>twitter_bearer_token</code> field.</p> </li> <li> <p>Click <code>Deploy</code>.</p> <p>This service receives data from Twitter and streams it to the <code>twitter-data</code> topic. You can verify this by clicking the <code>Twitter</code> data source service in the pipeline and then viewing the <code>Logs</code> or <code>Messages</code> tab.    </p> </li> </ol> <p>Note</p> <p>The default Twitter search criteria is looking for Bitcoin tweets, it's a high traffic subject and great for the demo. However, because of the volume of Bitcoin tweets it will use up your Twitter Developer Account credits in a matter of days. So stop the Twitter feed when you're finished with it.</p> <p>Feel free to change the search criteria once you\u2019ve got the demo working. </p>"},{"location":"platform/tutorials/sentiment-analysis/twitter-data/#building-the-transformation","title":"Building the transformation","text":"<p>In the first part of this part of the tutorial, Fetching the tweets you deployed a microservice which subscribes to tweets on a predefined subject and publishes them to a topic. </p> <p>In order to get the tweets to appear in the Sentiment Demo UI, and have their sentiment analyzed, you now need to transform the Twitter data into a format that the Sentiment Demo UI can understand.</p> <p>This service will subscribe to the <code>twitter-data</code> topic and publish data to the <code>messages</code> topic. It will transform the incoming data to make it compatible with the UI and sentiment analysis service.</p>"},{"location":"platform/tutorials/sentiment-analysis/twitter-data/#creating-the-project","title":"Creating the project","text":"<p>Follow these steps to code and deploy the tweet-to-chat conversion stage:</p> <ol> <li> <p>Navigate to the Library and apply the following filters:</p> <ol> <li> <p>Languages = <code>Python</code></p> </li> <li> <p>Pipeline Stage = <code>Transformation</code></p> </li> <li> <p>Type = <code>Basic templates</code></p> </li> </ol> </li> <li> <p>Select <code>Empty template - Transformation</code>.</p> </li> <li> <p>Click <code>Preview code</code> then <code>Edit code</code>.</p> </li> <li> <p>Change the name to <code>tweet-to-chat</code>.</p> </li> <li> <p>Change the input to <code>twitter-data</code> by either selecting it or typing it.</p> </li> <li> <p>Ensure the output is set to <code>messages</code>.</p> </li> <li> <p>Click <code>Save as project</code>.</p> <p>The code for this transformation is now saved to your workspace.</p> </li> </ol>"},{"location":"platform/tutorials/sentiment-analysis/twitter-data/#editing-the-code","title":"Editing the code","text":"<p>Once saved, you'll be redirected to the online development environment. This is where you can edit, run and test the code before deploying it to production.</p> <p>Follow these steps to create the tweet-to-chat service.</p> <ol> <li> <p>Locate <code>main.py</code>.</p> </li> <li> <p>Add <code>import pandas as pd</code> to the imports at the top of the file.</p> </li> <li> <p>Locate the line of code that creates the output stream:</p> <pre><code>output_stream = output_topic.create_stream(input_stream.stream_id)\n</code></pre> </li> <li> <p>Change this line to get or create a stream called <code>tweets</code>:</p> <pre><code>output_stream = output_topic.get_or_create_stream(\"tweets\")\n</code></pre> <p>This will ensure that any messages published by this service go into a stream called <code>tweets</code>. You'll use the <code>tweets</code> room later on to see all of the tweets and their sentiment.</p> </li> <li> <p>Now locate <code>quix_function.py</code>.</p> <p>Alter <code>on_pandas_frame_handler</code> to match the code below:</p> <pre><code>def on_pandas_frame_handler(self, df: pd.DataFrame):\ndf = df.rename(columns={'text': 'chat-message'})\ndf[\"TAG__name\"] = \"Twitter\"\ndf[\"TAG__role\"] = \"Customer\"\nself.output_stream.parameters.write(df)\n</code></pre> <p>This will take <code>text</code> from incoming <code>twitter-data</code> and stream it to the output topics <code>tweets</code> stream, as parameter or table values, with a column name of <code>chat-message</code>, which the other stages of the pipeline will recognize.</p> </li> <li> <p>Click <code>Run</code> near the top right of the code window.</p> </li> <li> <p>Click the <code>Messages</code> tab.</p> <p>In the default view showing <code>input</code> messages you will see the incoming <code>twitter-data</code> messages.</p> <p>Select a message and you will see that these have <code>\"text\"</code> in the string values. This is the tweet text.</p> <pre><code>\"StringValues\": {\n\"tweet_id\": [\"1600540408815448064\"],\n    \"text\": [\"Some message about @BTC\"]\n}\n</code></pre> </li> <li> <p>Select the \"output\" messages from the messages dropdown list. These are messages being published from the code.</p> <p>Select a message, and you will see that the output messages have a different structure.</p> <p>The string values section of the JSON message now contains \"chat-message\" instead of \"text\":</p> <pre><code>\"StringValues\": {\n\"tweet_id\": [\"1600541061583192066\"],\n    \"chat-message\": [\"Some message about @BTC\"]\n}\n</code></pre> </li> <li> <p>Stop the running code and proceed to the next section.</p> </li> </ol>"},{"location":"platform/tutorials/sentiment-analysis/twitter-data/#deploying-the-twitter-service","title":"Deploying the Twitter service","text":"<p>You'll now tag the code and deploy the service with these steps:</p> <ol> <li> <p>Click the <code>+tag</code> button at the top of any code file.</p> </li> <li> <p>Enter <code>v1</code> and press Enter.</p> </li> <li> <p>Click <code>Deploy</code> near the top right corner.</p> </li> <li> <p>In the deployment dialog, select <code>v1</code> under the <code>Version Tag</code>.</p> <p>There is no need to allocate much resource to this service, it is very light weight.</p> </li> <li> <p>Click <code>Deploy</code>.</p> </li> <li> <p>Navigate to, or reload, the Sentiment Demo UI you deployed in the first part of this tutorial.</p> </li> <li> <p>Enter the chat room called <code>tweets</code>. Using <code>tweets</code> for the chat room name will ensure you are seeing the messages coming from Twitter. For <code>Name</code>, you can use any name you want. The dialog is show here:</p> <p></p> </li> <li> <p>You can now see messages arriving from Twitter and their sentiment being analyzed in real-time.</p> <p></p> </li> </ol> <p>Success</p> <p>You will see 'Bitcoin' tweets arriving in the chat along with the calculated average sentiment in a chart.</p> <p>Your pipeline is now complete, you can send and view chat messages, receive tweets, and analyze the sentiment of all of the messages.</p> <p>Share the QR code with colleagues and friends, to talk about anything you like while Quix analyzes the sentiment in the room in real time.</p> <p>Conclusion and next steps </p>"},{"location":"platform/tutorials/slack-alerting/slack-alerting/","title":"Sending TFL BikePoint availability alerts to Slack","text":""},{"location":"platform/tutorials/slack-alerting/slack-alerting/#aim","title":"Aim","text":"<p>In this tutorial you will learn how to use Quix to create event driven notifications in real-time. In this example we\u2019ll connect to the Transport for London BikePoint API and send availability alerts to Slack using Slacks Webhooks.</p> <p>We will start by sending the raw BikePoint data to Slack and then we'll show you how to alter the pre-built solution to refine the Slack message.</p> <p>By the end you will have:</p> <ul> <li> <p>Configured Slack to allow external services to send messages via WebHooks</p> </li> <li> <p>Built and deployed a Quix Service that streams data received from the TFL BikePoint API</p> </li> <li> <p>Received messages to your slack channel on the availability of bikes around London</p> </li> </ul> <p>Note</p> <p>If at any point you run into trouble please reach out to us. We\u2019ll be waiting for your message in The Stream our Slack community.</p>"},{"location":"platform/tutorials/slack-alerting/slack-alerting/#project-architecture","title":"Project Architecture","text":"<p>The solution has 2 main elements:</p> <ul> <li> <p>A Quix Service to pull TFL BikePoint data</p> </li> <li> <p>A Slack Alerting Quix service</p> </li> </ul> <p>A Quix Service receives data from the Transport For London BikePoint API and streams the data onto the Quix message broker. Another service listens to the bike data stream. When the number of available bikes at a station falls below the desired level an alert is sent to the Slack channel via the Slack Webhook.</p>"},{"location":"platform/tutorials/slack-alerting/slack-alerting/#prerequisites","title":"Prerequisites","text":"<p>To proceed with this tutorial you need:</p> <ul> <li> <p>Quix portal login. (You'll need an account for this, go to here and create one)</p> </li> <li> <p>Access to Slack, you\u2019ll need to be an admin.</p> </li> <li> <p>A TFL account and API keys</p> Getting your TFL API key <p>You\u2019ll need a free TfL account which you can register for here: https://api-portal.tfl.gov.uk/</p> <p>A rough guide to finding your TfL API key is as follows:</p> <ol> <li> <p>Register for an account.</p> </li> <li> <p>Login and click the <code>Products</code> menu item.</p> </li> <li> <p>You should have 1 product to choose from <code>500 Requests per min.</code></p> </li> <li> <p>Click <code>500 Requests per min.</code></p> </li> <li> <p>Enter a name for your subscription into the box, e.g. QuixFeed, and click <code>Register.</code></p> </li> <li> <p>You can now find your API Keys in the profile page.</p> </li> </ol> </li> </ul> <p>Tip</p> <p>Check out the projects README.md later on in the tutorial if you need help creating a Slack WebHook</p>"},{"location":"platform/tutorials/slack-alerting/slack-alerting/#overview","title":"Overview","text":"<p>This walk through covers the following steps:</p> <ol> <li> <p>Creating a Slack App</p> </li> <li> <p>Setup Webhooks</p> </li> <li> <p>Deploy a service to pull TFL BikePoint data</p> </li> <li> <p>Publish messages to Slack from Quix</p> </li> <li> <p>Edit the code and send a custom Slack message</p> </li> </ol>"},{"location":"platform/tutorials/slack-alerting/slack-alerting/#part-one","title":"Part one","text":"<p>In part one we will guide you through the process of selecting and deploying the TFL BikePoint connector and the Slack connector.  The end result will be Slack alerts containing data from the BikePoint API.</p>"},{"location":"platform/tutorials/slack-alerting/slack-alerting/#tfl-bikepoint-data","title":"TFL BikePoint data","text":"<p>Obtaining data from TFL's BikePoint API is fairly straight forward. You need to make a request to the relevant API endpoint, passing the correct parameters and API keys. When data is returned, you'll need to process it and ensure it's in the correct format to publish to a Quix topic.</p> <p>However, there is a much easier way to achieve the same outcome.</p> <ol> <li> <p>Navigate to the Quix library</p> </li> <li> <p>Search for <code>TFL Bikepoint</code> and click the tile</p> <p></p> </li> <li> <p>Click <code>Setup &amp; deploy</code></p> </li> <li> <p>Paste your TFL API keys into the <code>tfl_primary_key</code> and <code>tfl_secondary_key</code> input fields</p> </li> <li> <p>Click <code>Deploy</code></p> </li> </ol> <p>Success</p> <p>You've deployed the TFL API microservice and it is publishing data to Quix</p>"},{"location":"platform/tutorials/slack-alerting/slack-alerting/#configure-slack","title":"Configure Slack","text":"<p>You'll need to be an admin on Slack for this.</p>"},{"location":"platform/tutorials/slack-alerting/slack-alerting/#slack-app","title":"Slack App","text":"<p>Ensure you\u2019re logged into the Slack web portal here then create a new app.</p> <ol> <li> <p>Click <code>Create your Slack app</code></p> </li> <li> <p>On the popup, select <code>From Scratch</code></p> </li> <li> <p>Enter a name and choose your workspace</p> </li> <li> <p>Click <code>Create App</code></p> </li> </ol>"},{"location":"platform/tutorials/slack-alerting/slack-alerting/#webhooks","title":"Webhooks","text":"<p>With the app created you'll now need to setup a webhook. This will give you a URL that you can use to publish messages to a Slack channel.</p> <ol> <li> <p>On the left hand menu under <code>Features</code> select the <code>Incoming Webhooks</code> menu item</p> </li> <li> <p>Switch the slider to <code>On</code> to activate incoming Webhooks</p> </li> <li> <p>Click the <code>Add New Webhook to Workspace</code> button</p> </li> <li> <p>Select the channel you want to give access to and thus publish messages to</p> </li> <li> <p>Click allow</p> </li> <li> <p>Copy the <code>Webhook URL</code> near the bottom of the page. Keep this safe you will need it soon</p> </li> </ol>"},{"location":"platform/tutorials/slack-alerting/slack-alerting/#integration","title":"Integration","text":"<p>The time has come to actually connect Quix and Slack. Once again, with the help of the Quix Library, this is a simple task.</p> <ol> <li> <p>Navigate to the Library</p> </li> <li> <p>Search for <code>Slack</code></p> </li> <li> <p>Click <code>Setup &amp; deploy</code></p> </li> <li> <p>Ensure that the <code>input</code> is set to <code>tfl-bikepoint-data</code></p> </li> <li> <p>Past your Webhook URL into the <code>webhook_url</code> input box</p> </li> <li> <p>Click <code>Deploy</code></p> </li> </ol> <p>Success</p> <p>You will see messages arriving in your chosen slack channel</p> <p></p>"},{"location":"platform/tutorials/slack-alerting/slack-alerting/#part-two","title":"Part two","text":"<p>In this part of the tutorial you will replace the current Slack connector with a new connector to send customized alerts. It's based on the existing connector, so most of the work has already been done.</p> <p>Note</p> <p>Begin by stopping the existing Slack connector from the home page.</p>"},{"location":"platform/tutorials/slack-alerting/slack-alerting/#slack-connector-project","title":"Slack connector project","text":"<p>Follow these steps to save the connector code to your workspace.</p> <ol> <li> <p>Navigate to the library and search for <code>Slack</code></p> </li> <li> <p>Click <code>Preview code</code> on the tile     You can preview the code here and read the readme. You can't edit the code right now</p> </li> <li> <p>Click <code>Edit code</code></p> </li> <li> <p>Ensure that the <code>input</code> field is set to <code>tfl-bikepoint-data</code> and past your Slack WebHook URL into the appropriate field</p> </li> <li> <p>Click <code>Save as project</code>     The code is now saved to your workspace and you can now edit the code and make any modifications you need</p> </li> </ol>"},{"location":"platform/tutorials/slack-alerting/slack-alerting/#customize-the-message","title":"Customize the message","text":"<p>Now that you have the code saved and can edit it, you can customize the message that's sent to Slack.</p> <ol> <li> <p>Ensure you are viewing the file called <code>quix_function.py</code></p> </li> <li> <p>Locate the function named <code>on_pandas_frame_handler</code></p> </li> <li> <p>Replace the code inside the function with the following code:</p> <pre><code># iterate the data frame\nfor i, row in df.iterrows():\n# get the number of bikes\nnum_bikes = row[\"NbBikes\"]\n# get the location\nbike_loc = row[\"Name\"]\n# print a message\nprint(\"{} has {} bikes available\".format(bike_loc, num_bikes))\n</code></pre> </li> <li> <p>You can now run the code here in the development environment by clicking the <code>Run</code> button near the top right of the code editor.</p> <p>Success</p> <p>You will see messages in the output window showing how many bikes there are at each location</p> <p></p> </li> <li> <p>Click the run button again to stop the code</p> </li> <li> <p>Now add the following code to the function named <code>on_pandas_frame_handler</code> (Add this to the code you already added in the steps above)</p> <pre><code>message = \"\"\n# check the number of remaining bikes\nif num_bikes &lt; 3: \nmessage = \"Hurry! {} only has {} bike left\".format(bike_loc, num_bikes)\nelse:\nmessage = \"{} has {} bikes available\".format(bike_loc, num_bikes)\n# compose and send your slack message\nslack_message = {\"text\": message}\nrequests.post(self.webhook_url, json=slack_message)\n</code></pre> </li> <li> <p>Repeat the process of running the code in the editor (see step 4 above)</p> <p>Success</p> <p>You now have customized messages in your Slack channel.</p> <p></p> <p>Don't forget to stop the service before proceeding</p> </li> </ol>"},{"location":"platform/tutorials/slack-alerting/slack-alerting/#deploy-the-service","title":"Deploy the service","text":"<p>Now that you have verified that the code works it's time to deploy it as a microservice to the Quix serverless environment.</p> <p>Follow these steps:</p> <ol> <li> <p>Tag the code</p> This is how you tag the code <p></p> </li> <li> <p>Click <code>Deploy</code> near the top right corner of the code editor window</p> </li> <li> <p>On the Deploy dialog select the version tag you just created</p> </li> <li> <p>Click <code>Deploy</code></p> <p>The service will be built, deployed and started</p> </li> </ol> <p>Success</p> <p>You modified an existing library item and deployed a microservice.</p> <p>You should start seeing Slack messages as soon as the service starts.</p> <p>Clean up</p> <p>You can delete the <code>Slack Notifications - Destination</code> you deployed in part one of this tutorial</p>"},{"location":"platform/tutorials/slack-alerting/slack-alerting/#whats-next","title":"What\u2019s Next","text":"<p>There are many ways you can use this code, try enhancing it so it only alerts you about each location once, or once every 5 minutes.</p> <p>Using Quix, coupled with something like Slack, allows for automatic real-time alerting. Quix allows you to react in real-time to events or anomalies found either in raw data or generated by ML models.</p> <p>If you ran into trouble please reach out to us. We\u2019ll be more than happy to help. We hang out at The Stream. Come and say hi!</p>"},{"location":"platform/tutorials/telemetry-data/telemetry-data/","title":"Telemetry data","text":""},{"location":"platform/tutorials/telemetry-data/telemetry-data/#stream-and-visualize-real-time-telemetry-data-with-an-android-app-and-streamlit","title":"Stream and visualize real-time telemetry data with an Android app and Streamlit","text":"<p>Learn how to build an end-to-end telemetry data pipeline for IoT applications. Use our Android companion app to stream sensor data from your phone and visualize it in a Streamlit app.</p>"},{"location":"platform/tutorials/telemetry-data/telemetry-data/#get-started","title":"Get Started","text":"<p>To get started make sure you have a Quix account, signup for a completely free account at https://quix.io</p> <p>You will need an Android mobile phone for this tutorial (we're working on the Apple app).</p> <p>If you need any assistance, we're here to help in The Stream, our free Slack community.</p>"},{"location":"platform/tutorials/telemetry-data/telemetry-data/#library","title":"Library","text":"<p>Most of the code you'll need has already been written. It lives in our library, which is accessible from inside the Quix portal or directly via our open source GitHub repo. We'll be referring to the library often so make sure you know where it is.</p>"},{"location":"platform/tutorials/telemetry-data/telemetry-data/#components","title":"Components","text":"<p>Android App - Our companion app for collecting real-time sensor data from your phone. It's pre-built and published to the Play store to save you time. You can also access the source code in our GitHub repo.</p> <p>Streamlit App - See your location on a map and other activity metrics.</p> <p>QR Settings Share - A lightweight UI and API for sharing settings with the Android app.</p>"},{"location":"platform/tutorials/telemetry-data/telemetry-data/#build-it","title":"Build It","text":"<p>This guide will show you how to deploy each of the components, starting with QR Settings Share.</p>"},{"location":"platform/tutorials/telemetry-data/telemetry-data/#1-qr-settings-share","title":"1. QR Settings Share","text":"<p>Follow these steps to deploy the QR Settings Share.</p> <ol> <li> <p>Navigate to the Library and locate \"QR Settings Share\"</p> </li> <li> <p>Click \"Setup &amp; deploy\"</p> </li> <li> <p>Click \"Deploy\"</p> </li> </ol> <p>Info</p> <p>The service will be deployed to your workspace</p> <p>Open the UI with these steps</p> <ol> <li> <p>Once deployed, click the service tile</p> </li> <li> <p>Click the \"Public URL\"</p> </li> <li> <p>Append the following querystring to the url in the address bar</p> </li> </ol> <p>?topic=phone-data&amp;notificationsTopic=notifications-topic</p> <p>These are needed by the Android app and will be passed to it via the QR code you'll generate in a moment</p> <ol> <li>You can now enter a username and device name into the relevant inputs</li> </ol> <p></p> <p>These can be anything! But sensible values will help understand which token belongs to which user</p> <ol> <li> <p>Click OK</p> </li> <li> <p>You can now click Generate Token</p> </li> </ol> <p>Several things happened when you clicked the button.</p> <p>A token was generated in Quix with an expiry time in around 1 month.</p> <p>The token was published to an API and a link to the token was generated.</p> <p>The QR shown on screen is a short-lived link to the longer lasting token.</p> <p>Info</p> <p>Scanning the token with the Android App will allow the app to receive its configuration information without you having to enter any long token strings by hand.</p>"},{"location":"platform/tutorials/telemetry-data/telemetry-data/#2-android-app","title":"2. Android App","text":"<p>It's time to install the Android app!</p> <ol> <li>Go to the Google Play store and search for \"Quix Tracker App\" ensuring it's the one published by us.</li> </ol> <p>[Screenshot of Play store coming soon]</p> <ol> <li> <p>Install the app.</p> </li> <li> <p>Tap the menu hamburger</p> </li> <li> <p>Tap settings</p> </li> <li> <p>Tap SCAN QR CODE</p> </li> <li> <p>Generate a new QR code in the QR Settings Share website you deployed in step 1</p> </li> <li> <p>Scan the QR code with your device</p> <p>The Token, Workspace and topics should now be configured</p> </li> <li> <p>Tap the menu hamburger once again</p> </li> <li> <p>Tap Dashboard</p> </li> <li> <p>Tap START at the bottom of the screen</p> Your device is now streaming to the topic called 'phone-data' in your workspace <p>These parameters are being streamed from your phone:</p> <ul> <li>Accuracy</li> <li>Altitude</li> <li>BatteryLevel</li> <li>BatteryPowerSource</li> <li>BatteryState</li> <li>EnergySaverStatus</li> <li>gForceX</li> <li>gForceY</li> <li>gForceZ</li> <li>Heading</li> <li>Latitude</li> <li>LogInfo</li> <li>Longitude</li> <li>Speed</li> </ul> <p>Tip</p> <p>Leave the app running until you've completed the next step.</p> </li> </ol>"},{"location":"platform/tutorials/telemetry-data/telemetry-data/#3-streamlit-app","title":"3. Streamlit app","text":"<p>Explore the data inside Quix and then deploy a frontend app.</p> <p>Within Quix:</p> <ol> <li> <p>Click Data Explorer on the left hand menu</p> </li> <li> <p>Select the \"phone-data\" topic</p> </li> <li> <p>You should have at least one stream, select it</p> </li> <li> <p>Select as many parameters as you want. The gForce ones are great for this tutorial</p> </li> <li> <p>You can see the gForce from your device displayed on the waveform in real time</p> </li> </ol> <p>Deploy an app:</p> <ol> <li> <p>Click Library on the left hand menu</p> </li> <li> <p>Search for Streamlit Dashboard</p> </li> <li> <p>Click Preview code</p> </li> <li> <p>Click Edit code</p> </li> <li> <p>Ensure phone-data is selected for the input field</p> </li> <li> <p>Click Save as project</p> <p>The code for the dashboard is now saved to your workspace. You can now edit it to ensure it works with the data coming from the device</p> </li> <li> <p>Locate the following line in streamlit_file.py</p> </li> </ol> <pre><code>st.line_chart(local_df[[\"datetime\",'Speed','EngineRPM']].set_index(\"datetime\"))\n</code></pre> <ol> <li> <p>Replace 'Speed' with 'gForceX'</p> </li> <li> <p>Replace 'EngineRPM' with 'gForceY'</p> </li> <li> <p>Remove the following code block</p> </li> </ol> <pre><code>with fig_col2:\nst.markdown(\"### Chart 2 Title\")\nst.line_chart(local_df[[\"datetime\", 'Gear', 'Brake']].set_index(\"datetime\"))\n</code></pre> <ol> <li> <p>Click Run near the top right corner of the code editor</p> <p>The dashboard will run right here in the development environment</p> <p>Click the icon that appears next to the project name (circled here in green)</p> <p></p> </li> <li> <p>Once the dashboard has loaded you will see sensor data from your device in real time</p> <p></p> <p>Below this is also the raw data showing all the values from the device</p> </li> <li> <p>Close the dashboard</p> <p>The dashboard you were just using was deployed to a temporary URL in the development area</p> <p>You need to deploy it to its permanent home!</p> </li> <li> <p>Click the Deploy button in the top right corner.</p> </li> <li> <p>Click Service in the Deployment Settings</p> </li> <li> <p>Click the toggle button under Public Access</p> </li> <li> <p>Click Deploy</p> <p>You will be redirected to the home page</p> </li> <li> <p>Once the dashboard has been built and deployed click the deployment tile</p> </li> <li> <p>Click the Public URL</p> </li> </ol> <p>You are now looking at your dashboard in its permanent home</p> <p>Be aware that there is no security on the dashboard</p>"},{"location":"platform/tutorials/telemetry-data/telemetry-data/#next-steps","title":"Next Steps","text":"<p>You've just connected your mobile device to Quix and deployed a Streamlit app to view the data in real time.</p> <p>Now try showing some different parameters in the app, or build a transformation in Quix to make better use of the raw data.</p> <p>What will you build? Let us know. We'd love to feature your project or use case in our newsletter.</p> <p>If you need any assistance, we're here to help in The Stream, our free Slack community.</p>"},{"location":"platform/tutorials/telemetry-data/telemetry-data/#ciao-for-now","title":"Ciao for now!","text":"<p>We hope you enjoyed this tutorial. If you have any questions or feedback please contact us on The Stream.</p> <p>Thank you.!</p>"},{"location":"platform/tutorials/train-and-deploy-ml/deploy-ml/","title":"Run ML model in realtime environment","text":"<p>In this article, you will learn how to use pickle file trained on historic data in a realtime environment.</p> <p>Ensure you have completed the previous stage first, if not find it here.</p>"},{"location":"platform/tutorials/train-and-deploy-ml/deploy-ml/#watch","title":"Watch","text":"<p>If you prefer watching instead of reading, we've recorded a short video:</p>"},{"location":"platform/tutorials/train-and-deploy-ml/deploy-ml/#why-this-is-important","title":"Why this is important","text":"<p>With the Quix platform, you can run and deploy ML models to the leading edge reacting to data coming from the source with milliseconds latency.</p>"},{"location":"platform/tutorials/train-and-deploy-ml/deploy-ml/#end-result","title":"End result","text":"<p>At the end of this article, we will end up with a live model using the pickle file from How to train ML model to process live data on the edge.</p> <p></p>"},{"location":"platform/tutorials/train-and-deploy-ml/deploy-ml/#preparation","title":"Preparation","text":"<p>You\u2019ll need to complete the How to train ML model article to get pickle file with trained model logic.</p>"},{"location":"platform/tutorials/train-and-deploy-ml/deploy-ml/#run-the-model","title":"Run the model","text":"<p>Now let's run the model you created in the previous article. If you have your own model and already know how to run the Python to execute it then these steps might also be useful for you.</p> <p>Ensure you are logged into the Quix Portal</p> <ol> <li> <p>Navigate to the Library</p> </li> <li> <p>Filter the library by selecting <code>Python</code> under languages and <code>Transformation</code> under pipeline stage</p> </li> <li> <p>Select the <code>Event Detection</code> item</p> <p>Tip</p> <p>If you can't see <code>Event Detection</code> you can also use search to find it</p> <p>Info</p> <p>Usually, after clicking on the <code>Event Detection</code> you can look at the code and the readme to ensure it's the correct sample for your needs.</p> </li> <li> <p>Now click Edit code</p> </li> <li> <p>Change the name to \"Prediction Model\"</p> </li> <li> <p>Ensure the input is \"f1-data\"</p> </li> <li> <p>Ensure the output is \"brake-prediction\"</p> <p>Info</p> <p>The platform will automatically create any topics that don't already exist</p> </li> </ol> <p>Success</p> <p>The code from the Library sample is now saved to your workspace.</p> <p>You can edit and run the code from here or clone it to your computer and work locally.</p> <p>See more about setting up your local environment here.</p>"},{"location":"platform/tutorials/train-and-deploy-ml/deploy-ml/#upload-the-model","title":"Upload the model","text":"<p>Now you need to upload the ML model created in the previous article and edit this code to run the model.</p> <ol> <li> <p>Click the upload file icon at the top of the file list</p> </li> <li> <p>Find the file saved in the previous article.</p> <p>Hint</p> <p>It's called 'decision_tree_5_depth.sav' and should be in \"C:\\Users[USER]\\\" on Windows</p> <p>Warning</p> <p>When you click off the file e.g. onto quix_function.py, the editor might prompt you to save the .sav file.</p> <p>Click \"Do not commit\"</p> </li> <li> <p>Click quix_function.py in the file list (remember do not commit changes to the model file)</p> </li> </ol>"},{"location":"platform/tutorials/train-and-deploy-ml/deploy-ml/#modify-the-code","title":"Modify the code","text":"<ol> <li> <p>Add the following statements to import the required libraries</p> <pre><code>import pickle\nimport math\n</code></pre> </li> <li> <p>In the <code>__init__</code> function add the following lines to load the model</p> <pre><code>## Import ML model from file\nself.model = pickle.load(open('decision_tree_5_depth.sav', 'rb'))\n</code></pre> </li> <li> <p>Under the <code>__init__</code> function add the following new function</p> <p>This will pre-process the data, a necessary step before passing it to the model.</p> <pre><code>## To get the correct output, we preprocess data before we feed them to the trained model\ndef preprocess(self, df):\nsignal_limits = {\n\"Speed\": (0, 400),\n\"Steer\": (-1, 1),\n\"Gear\": (0, 8),\n\"Motion_WorldPositionX\": (-math.pi, math.pi),\n\"Brake\": (0, 1),\n}\ndef clamp(n, minn, maxn):\nreturn max(min(maxn, n), minn)\nfor signal, limits in signal_limits.items():\ndf[signal] = df[signal].map(lambda x: clamp(x, limits[0], limits[1]))\ndf[\"Motion_WorldPositionX_sin\"] = df[\"Motion_WorldPositionX\"].map(lambda x: math.sin(x))\ndf[\"Motion_WorldPositionX_cos\"] = df[\"Motion_WorldPositionX\"].map(lambda x: math.cos(x))\nreturn df\n</code></pre> </li> <li> <p>Delete the <code>on_pandas_frame_handler</code> function and paste this code in it's place.</p> <pre><code># Callback triggered for each new parameter data.\ndef on_pandas_frame_handler(self, df: pd.DataFrame):\n# if no speed column, skip this record        \nif not \"Speed\" in df.columns:\nreturn df\noutput_df = pd.DataFrame()\n# Preprocessing\ndf = self.preprocess(df)\nfeatures = [\"Motion_WorldPositionX_cos\", \"Motion_WorldPositionX_sin\", \"Steer\", \"Speed\", \"Gear\"]\nX = df[features]\n# Lets shift data into the future by 5 seconds. (Note that time column is in nanoseconds).\noutput_df[\"time\"] = df[\"time\"].apply(lambda x: int(x) + int((5 * 1000 * 1000 * 1000)))\noutput_df[\"brake-prediction\"] = self.model.predict(X)\nprint(\"Prediction\")\nprint(output_df[\"brake-prediction\"])        \n# Merge the original brake value into the output data frame\noutput_df = pd.concat([df[[\"time\", \"Brake\"]], output_df]).sort_values(\"time\", ascending=True)\nself.output_stream.parameters.buffer.write(output_df)  # Send filtered data to output topic\n</code></pre> </li> </ol>"},{"location":"platform/tutorials/train-and-deploy-ml/deploy-ml/#update-requirements","title":"Update requirements","text":"<p>Click on the requirements.txt file and add <code>sklearn</code> on a new line</p> <p>Success</p> <p>You have edited the code to load and run the model.</p>"},{"location":"platform/tutorials/train-and-deploy-ml/deploy-ml/#run-the-code","title":"Run the code","text":"<p>The fastest way to run the code is to click Run in the top right hand corner.</p> <p>This will install any dependencies into a sandboxed environment and then run the code.</p> <p>In the output console you will see the result of the prediction.</p> <p>In the next few steps you will deploy the code and then see a visualization of the output.</p>"},{"location":"platform/tutorials/train-and-deploy-ml/deploy-ml/#deploy","title":"Deploy","text":"<ol> <li> <p>Click Stop if you haven't already done so.</p> </li> <li> <p>To deploy the code, click Deploy.</p> </li> <li> <p>On the dialog that appears click Deploy.</p> </li> </ol> <p>Once the code has been built, deployed it will be started automatically.</p> <p>Success</p> <p>Your code is now running in a fully production ready ecosystem.</p>"},{"location":"platform/tutorials/train-and-deploy-ml/deploy-ml/#visualize-whats-happening","title":"Visualize whats happening","text":"<p>To see the output of your model in real time you will use the Data explorer.</p> <ol> <li> <p>Click the Data explorer button on the left hand menu.</p> </li> <li> <p>If it's not already selected click the Live data tab at the top</p> </li> <li> <p>Ensure the <code>brake-prediciton</code> topic is selected</p> </li> <li> <p>Select a stream (you should only have one)</p> </li> <li> <p>Select <code>brake-prediction</code> and <code>brake</code> from the parameters list</p> </li> </ol> <p>Success</p> <p>You should now see a graphical output for the prediction being output by the model as well as the actual brake value</p> <p></p> <p>Note</p> <p>Don't forget this exercise was to deploy an ML model in the Quix platform. </p> <p>We didn't promise to train a good model. So the prediciton may not always match the actual brake value.</p>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/","title":"How to train an ML model","text":"<p>In this article, you will learn how to manage ML model training with Quix. In this example, we will train a model to predict car braking on a racing circuit 5 seconds ahead of time.</p>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#watch","title":"Watch","text":"<p>If you prefer watching instead of reading, we've recorded a short video:</p>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#why-this-is-important","title":"Why this is important","text":"<p>With the Quix platform, you can leverage historic data to train your model to react to data coming from source with milliseconds latency.</p>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#end-result","title":"End result","text":"<p>At the end of this article, we will end up with a pickle file trained on historic data.</p> <p></p>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#preparation","title":"Preparation","text":"<p>You will need Python3 installed.</p> <p>You\u2019ll need some data stored in the Quix platform. You can use any of our Data Sources available in the samples Library, or just follow the onboarding process when you sign-up to Quix</p> <p>Tip</p> <p>If in doubt, login to the Quix Portal, navigate to the Library and deploy \"Demo Data - Source\".</p> <p>This will provide you with some real-time data for your experiments.</p> <p>You\u2019ll also need a Jupyter notebook environment to run your experiments and load data for training. Please use \"How to work with Jupyter notebook\".</p>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#install-required-libraries","title":"Install required libraries","text":"<pre><code>python3 -m pip install seaborn\npython3 -m pip install sklearn\npython3 -m pip install mlflow\npython3 -m pip install matplotlib\n</code></pre> <p>Note</p> <p>If you get an 'Access Denied' error installing mlflow try adding '--user' to the install command or run the installer from an Anaconda Powershell Prompt (with --user)</p> <p>Tip</p> <p>If you don\u2019t see Python3 kernel in your Jupyter notebook, execute the following commands in your python environment:  <pre><code>python3 -m pip install ipykernel\npython3 -m ipykernel install --user\n</code></pre></p>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#necessary-imports","title":"Necessary imports","text":"<p>To execute all code blocks below, you need to start with importing these libraries. Add this code to the top of you Jupyter notebook.</p> <pre><code>import math\nimport matplotlib.pyplot as plt\nimport mlflow\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport seaborn as sns\nfrom sklearn import tree\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\n</code></pre>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#training-ml-model","title":"Training ML model","text":""},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#getting-training-data","title":"Getting training data","text":"<p>The Quix web application has a python code generator to help you connect your Jupyter notebook with Quix.</p> <p>You need to be logged into the platform for this:</p> <ol> <li> <p>Select workspace (you likley only have one)</p> </li> <li> <p>Go to the Data Explorer</p> </li> <li> <p>Add a query to visualize some data. Select parameters, events, aggregation and time range</p> <p>Note</p> <p>Select <code>Brake</code>, <code>Motion_WorldPositionX</code>, <code>Steer</code>, <code>Speed</code>, <code>Gear</code> parameters and turn off aggregation!</p> </li> <li> <p>Select the Code tab</p> </li> <li> <p>Ensure Python is the selected language</p> </li> </ol> <p></p> <p>Copy the Python code to your Jupyter notebook and execute.</p> <p></p> <p>Tip</p> <p>If you want to use this generated code for a long time, replace the temporary token with a PAT token. See authenticate your requests for how to do that.</p>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#preprocessing-of-features","title":"Preprocessing of features","text":"<p>We will prepare data for training by applying some transformation on the downloaded data.</p> <p>Execute this in your notebook:</p> <pre><code>## Convert motion to continuous values\ndf[\"Motion_WorldPositionX_sin\"] = df[\"Motion_WorldPositionX\"].map(lambda x: math.sin(x))\ndf[\"Motion_WorldPositionX_cos\"] = df[\"Motion_WorldPositionX\"].map(lambda x: math.cos(x))\n</code></pre>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#preprocessing-of-label","title":"Preprocessing of label","text":"<p>Here we simplify braking to a boolean value.</p> <pre><code>## Conversion of label\ndf[\"Brake_bool\"] = df[\"Brake\"].map(lambda x: round(x))\n</code></pre>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#generate-advanced-brake-signal-for-training","title":"Generate advanced brake signal for training","text":"<p>Now we need to shift breaking 5 seconds ahead to train the model to predict breaking 5 seconds ahead.</p> <pre><code>## Offset dataset and trim it\nNUM_PERIODS = -round(5e9/53852065.77281786)\ndf[\"Brake_shifted_5s\"] = df[\"Brake_bool\"].shift(periods=NUM_PERIODS)\ndf = df.dropna(axis='rows')\n</code></pre> <p>Lets review it in plot:</p> <pre><code>plt.figure(figsize=(15, 8))\nplt.plot(df[\"Brake_shifted_5s\"])\nplt.plot(df[\"Brake_bool\"])\nplt.legend(['Shifted', 'Unshifted'])\n</code></pre> <p></p>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#fit-predict-and-score-a-model","title":"Fit, predict and score a model","text":"<p>Calculate class weighting in case we gain any accuracy by performing class balancing.</p> <pre><code>Y = df[\"Brake_shifted_5s\"]\ncw = {}\nfor val in set(Y):\ncw[val] = np.sum(Y != val)\nprint(cw)\n</code></pre>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#experiment","title":"Experiment","text":"<p>In the following code snippet we are executing an experiment using MLflow. Notice in last 3 lines that each experiment is logging MLflow metrics for experiments comparison later.</p> <pre><code>model_accuracy = pd.DataFrame(columns=[\n'Baseline Training Accuracy',\n'Model Training Accuracy',\n'Baseline Testing Accuracy',\n'Model Testing Accuracy',\n])\nkfold = KFold(5, shuffle=True, random_state=1)\nwith mlflow.start_run():\nclass_weight = None\nmax_depth = 5\nfeatures = [\"Motion_WorldPositionX_cos\", \"Motion_WorldPositionX_sin\", \"Steer\", \"Speed\", \"Gear\"]\nmlflow.log_param(\"class_weight\", class_weight)\nmlflow.log_param(\"max_depth\", max_depth)\nmlflow.log_param(\"features\", features)\nmlflow.log_param(\"model_type\", \"DecisionTreeClassifier\")\nX = df[features]\ndecision_tree = DecisionTreeClassifier(class_weight=class_weight, max_depth=max_depth)\nfor train, test in kfold.split(X):\nX_train = X.iloc[train]\nY_train = Y.iloc[train]\nX_test = X.iloc[test]\nY_test = Y.iloc[test]\n# Train model\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\n# Assess accuracy\ntrain_accuracy = round(decision_tree.score(X_train, Y_train) * 100, 2)\ntest_accuracy = round(decision_tree.score(X_test, Y_test) * 100, 2)\nY_baseline_zeros = np.zeros(Y_train.shape)\nbaseline_train_accuracy = round(accuracy_score(Y_train, Y_baseline_zeros) * 100, 2)\nY_baseline_zeros = np.zeros(Y_test.shape)\nbaseline_test_accuracy = round(accuracy_score(Y_test, Y_baseline_zeros) * 100, 2)\nmodel_accuracy = model_accuracy.append({\n\"Baseline Training Accuracy\": baseline_train_accuracy,\n\"Model Training Accuracy\": train_accuracy,\n\"Baseline Testing Accuracy\": baseline_test_accuracy,\n\"Model Testing Accuracy\": test_accuracy\n}, ignore_index=True)\nmlflow.log_metric(\"train_accuracy\", model_accuracy[\"Model Training Accuracy\"].mean())\nmlflow.log_metric(\"test_accuracy\", model_accuracy[\"Model Testing Accuracy\"].mean())\nmlflow.log_metric(\"fit_quality\", 1/abs(model_accuracy[\"Model Training Accuracy\"].mean() - model_accuracy[\"Model Testing Accuracy\"].mean()))\n</code></pre> <p>We review experiment model accuracy:</p> <pre><code>model_accuracy\n</code></pre> Depth Baseline Training Accuracy Model Training Accuracy Baseline Testing Accuracy Model Testing Accuracy 0 88.97 97.93 86.49 86.49 1 87.59 97.24 91.89 83.78 2 89.04 96.58 86.11 88.89 3 88.36 97.95 88.89 83.33 4 88.36 97.95 88.89 80.56 <p>Table with model accuracy preview</p>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#prediction-preview","title":"Prediction preview","text":"<p>Let\u2019s plot actual versus predicted braking using a trained model:</p> <pre><code>f, (ax1, ax2) = plt.subplots(2, 1, sharey=True, figsize=(50,8))\nax1.plot(Y)\nax1.plot(X[\"Speed\"]/X[\"Speed\"].max())\nax2.plot(decision_tree.predict(X))\nax2.plot(X[\"Speed\"]/X[\"Speed\"].max())\n</code></pre> <p></p>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#saving-model","title":"Saving model","text":"<p>When you are confident with the results, save the model into a file.</p> <pre><code>pickle.dump(decision_tree, open('./decision_tree_5_depth.sav', 'wb'))\n</code></pre> <p>Tip</p> <p>Pickle file will be located in folder where jupyter notebook command was executed</p>"},{"location":"platform/tutorials/train-and-deploy-ml/train-ml-model/#mlflow","title":"MLflow","text":"<p>To help you with experiments management, you can review experiments in MLflow.</p> <p>Warning</p> <p>MLflow works only on MacOS, Linux or Windows linux subsystem (WSL).</p> <p>Tip</p> <p>To have some meaningful data, run the experiment with 3 different <code>max_depth</code> parameter.</p> <p>Let\u2019s leave Jupyter notebook for now and go back to command line and run MLflow server:</p> <pre><code>mlflow ui\n</code></pre> <p>Select experiments to compare:</p> <p></p> <p>Plot metrics from experiments:</p> <p></p>"},{"location":"client-library/app-management/","title":"App management","text":"<p>In order to reduce the amount of boilerplate code added to each of our samples, we developed the <code>App.run()</code> feature.</p> <p><code>App.run</code> takes care of several small but important tasks in managing your Python apps. These include:</p> <ul> <li>Start subscribing to topics</li> <li>Handle termination</li> <li>Close streams and dispose topics</li> <li>Keep alive</li> </ul> <p>Each of these is described in the following sections.</p>"},{"location":"client-library/app-management/#imports","title":"Imports","text":"<p>To use <code>App.run()</code> in your code, you will need this import:</p> <pre><code>from quixstreams import App\nApp.run()\n</code></pre>"},{"location":"client-library/app-management/#start-subscribing","title":"Start subscribing","text":"<p>In order to start receiving from a topic you need to make a call to the <code>subscribe()</code> method. Your Python code won\u2019t be able to receive any data from the broker if you have missed this step. It makes sense to add this call near the end of the code, just before your 'keep busy' <code>while</code> loop.</p> <p>Your code might look something like this:</p> <pre><code>from quixstreams import TopicConsumer, StreamConsumer\ndef on_stream_received_handler(stream_received: StreamConsumer):\nbuffer = stream_received.timeseries.create_buffer()\nbuffer.on_dataframe_released = on_dataframe_released_handler\ndef on_dataframe_released_handler(stream: StreamConsumer, df: pd.DataFrame):\nprint(df.to_string())\n# ... some code setting up topic_consumer\ntopic_consumer.on_stream_received = on_stream_received_handler\ntopic_consumer.subscribe()\nwhile(True):  # or other blocking call\nprint('running')\n</code></pre> <p>However, using <code>App.run()</code> you no longer need the call to <code>subscribe()</code>, because it is called for you. </p> <p>So your code would look like this instead:</p> <pre><code>from quixstreams import App, TopicConsumer, StreamConsumer\nimport pandas as pd\ndef on_stream_received_handler(stream_received: StreamConsumer):\nbuffer = stream_received.timeseries.create_buffer()\nbuffer.on_dataframe_released = on_dataframe_released_handler\ndef on_dataframe_released_handler(stream: StreamConsumer, df: pd.DataFrame):\nprint(df.to_string())\n# ... some code setting up topic_consumer\ntopic_consumer.on_stream_received = on_stream_received_handler\nApp.run()\n</code></pre>"},{"location":"client-library/app-management/#termination","title":"Termination","text":"<p>Termination signals such as <code>SIGINT</code>, <code>SIGTERM</code> and <code>SIGQUIT</code> could be used to break out of the <code>while</code> loop used to keep your code listening for data. In order to listen for these you\u2019d need to add some code like this:</p> <pre><code>import threading\nimport signal\nimport time\n# Handle graceful exit\nevent = threading.Event() \ndef signal_handler(sig, frame):\n# set the termination flag\nprint('Setting termination flag')\nevent.set()\nsignal.signal(signal.SIGINT, signal_handler)\nsignal.signal(signal.SIGTERM, signal_handler)\nwhile not event.is_set():\ntime.sleep(1)\nprint('Exiting')\n</code></pre> <p>In this case, when the code runs, it will subscribe to and handle <code>SIGINT</code> (an interrupt signal) and <code>SIGTERM</code> (a termination signal). If either of these signals is observed, the code will call <code>signal_handler</code>, and the <code>while</code> loop will terminate allowing the code execution to come to an end.</p> <p><code>App.run</code> has this termination signal handling built in. It works on all popular platforms too.</p> <p>Using <code>App.run</code> the above code becomes much simpler:</p> <pre><code>from quixstreams import App\nApp.run()\nprint('Exiting')\n</code></pre>"},{"location":"client-library/app-management/#close-streams","title":"Close streams","text":"<p>Ideally, when your code terminates it will close the streams it opened and tidy up any other resources it was using.</p> <p>You can ensure the streams are closed by calling the following code just before your process terminates:</p> <pre><code># dispose the topic(s) and close the stream(s)\nprint('Closing streams...')\ntopic_consumer.dispose()  # Note the order. Producer should be closed after consumer\ntopic_producer.dispose()  # to avoid receiving data - possibly committing - but not being able to write.\nprint('Closed streams...')\n# or using the with statement, if your code structure is suited for it\nwith topic_producer, topic_consumer:  # disposed in reverse order\npass  # do great stuff\n# disposed when going out of scope\nprint('Closed streams...')\n</code></pre> <p>Here, you dispose of the topic consumer, which stops new data from being received, and then dispose of the topic producer. This code is easy and straightforward, however it is just more boilerplate that you have to remember.</p> <p>Once again, <code>App.run()</code> encapsulates this and handles this for you, so the code above becomes:</p> <pre><code>from quixstreams import App\nApp.run()\n</code></pre>"},{"location":"client-library/app-management/#keep-alive","title":"Keep alive","text":"<p>Unless you add an infinite loop or similar code, a Python code file will run each code statement sequentially until the end of the file, and then exit.  </p> <p>In order to continuously handle data in your Python code, you need to prevent the code from terminating. There are several ways this could be achieved. For example, you could use an infinite <code>while</code> loop to allow your code to run continuously. The following code will continuously print \"running\" until the <code>end_condition</code> has been satisfied:</p> <pre><code>run = True\nwhile(run):\nprint('running')\nif(end_condition = True):\nrun = False\nprint('ending')\n</code></pre> <p>We used code similar to this in our Quix Library items for a while. However, once we'd seen the same pattern being used repeatedly, we decided to build this functionality into Quix Streams.</p> <p>This is how you can use it in your code:</p> <pre><code>from quixstreams import App\nApp.run()\n</code></pre>"},{"location":"client-library/app-management/#bring-it-all-together","title":"Bring it all together","text":"<p>You have seen how you could start subscribing to streams, handle termination signals, dispose of topics consumers and producers, and how to keep your code running. </p> <p>To recap, here is an example of your code without using <code>App.run</code>, all in one snippet:</p> <pre><code>import threading\nimport signal\nimport time\n# ... some code setting up topic_consumer and topic_producer\ntopic_consumer.subscribe()  # initiate read\n# Hook up to termination signal (for docker image) and CTRL-C\nprint('Listening to streams. Press CTRL-C to exit.')\n# Below code is to handle graceful exit of the model.\nevent = threading.Event() \ndef signal_handler(sig, frame):\n# dispose the topic(s) and close the stream(s)\nprint('Closing streams...')\ntopic_consumer.dispose()\ntopic_producer.dispose()\nprint('Setting termination flag')\nevent.set()\nsignal.signal(signal.SIGINT, signal_handler)\nsignal.signal(signal.SIGTERM, signal_handler)\nwhile not event.is_set():\ntime.sleep(1)\nprint('Exiting')\n</code></pre> <p>If you use <code>App.run()</code>, this is greatly simplified into the following much smaller snippet:</p> <pre><code>from quixstreams import App\n# run a while loop\n# Subscribe to the input topics\n# listen for termination\n# close topic consumers and producers\nApp.run()\nprint('Exiting')\n</code></pre>"},{"location":"client-library/app-management/#before-shutdown","title":"Before shutdown","text":"<p>It's good practice to make sure that your code cleans up during the shutdown phase. Cleanup includes disposing of any resources you might have used, or indicating to external systems that they need to close or deallocate resources.</p> <p>If you choose to implement the cleanup of other resources, or simply need to log something immediately before the code ends, you can configure <code>App.run()</code> to call a function before it does all of the other built-in actions. <code>App.run</code> provides the <code>before_shutdown</code> hook to enable this facility. The following code provides an example of this:</p> <pre><code>from quixstreams import App\ndef before_shutdown():\nprint('before shutdown')\nApp.run(before_shutdown=before_shutdown)\n</code></pre> <p>In this snippet, <code>before_shutdown</code> is called before the app shuts down. That is before the <code>while</code> loop inside <code>App.run</code> comes to an end. This allows you to close connections, tidy up, and log your last messages before the app terminates.</p>"},{"location":"client-library/app-management/#triggering-shutdown-from-code","title":"Triggering shutdown from code","text":"<p>In some cases you might want to trigger shutdown from code. You can do it using <code>CancellationTokenSource</code>.</p> <pre><code>from quixstreams import App, CancellationTokenSource\nimport threading\nimport time\ncts = CancellationTokenSource()  # used for interrupting the App\n# setup shutdown after 5 seconds\ndef timeout_callback():\ntime.sleep(5)\ncts.cancel()\ntimeout_thread = threading.Thread(target=timeout_callback)\ntimeout_thread.start()\n# Setup some prints and pass the token to App.run\ndef before_shutdown():\nprint('before shutdown')\nprint('Waiting 5 seconds')\nApp.run(cts.token, before_shutdown=before_shutdown)\nprint('exiting')\n</code></pre>"},{"location":"client-library/connect/","title":"Connecting to a broker","text":"<p>It is possible to connect Quix Streams to brokers such as:</p> <ul> <li>Kafka</li> <li>Quix Platform (which also uses Kafka)</li> </ul> <p>Connecting Quix Streams to these brokers is discussed in the following sections.</p> <p>For more information on how Quix Streams works with Kafka, read Kafka and Quix Streams.</p>"},{"location":"client-library/connect/#connecting-to-kafka","title":"Connecting to Kafka","text":"<p>Connect to Kafka using the <code>KafkaStreamingClient</code> class provided by the library. This is a Kafka specific client implementation that requires some explicit configuration but allows you to connect to any Kafka cluster even outside Quix Platform.</p> <p>When your broker requires no authentication, you can use the following code to create a client to connect to it:</p> PythonC# <pre><code>from quixstreams import KafkaStreamingClient\nclient = KafkaStreamingClient('127.0.0.1:9092')\n</code></pre> <pre><code>var client = new QuixStreams.Streaming.KafkaStreamingClient(\"127.0.0.1:9092\");\n</code></pre> <p>If your broker is secured, the library provides easy authentication when using username and password with an optional certificate to validate server identity. </p> <p>The following code shows you how to set up the <code>SecurityOptions</code> for your connection and how to create a <code>KafkaStreamingClient</code> instance to start subscribing to topics and publishing time series data:</p> PythonC# <pre><code>from quixstreams import SecurityOptions, KafkaStreamingClient\nsecurity = SecurityOptions(CERTIFICATES_FOLDER, QUIX_USER, QUIX_PASSWORD)\nclient = KafkaStreamingClient('127.0.0.1:9093', security)  # additional details can be set using `properties=`\n</code></pre> <pre><code>var security = new SecurityOptions(CERTIFICATES_FOLDER, QUIX_USER, QUIX_PASSWORD);\nvar client = new QuixStreams.Streaming.KafkaStreamingClient(\"127.0.0.1:9093\", security);\n</code></pre>"},{"location":"client-library/connect/#connecting-to-quix","title":"Connecting to Quix","text":"<p>Quix Streams comes with a streaming client that enables you to connect to Quix SaaS topics easily. The streaming client manages the connections between your application and Quix and makes sure that the data is delivered reliably to and from your application. You can still use <code>KafkaStreamingClient</code> and manually set details, but <code>QuixStreamingClient</code> is a much easier way to connect.</p> <p><code>QuixStreamingClient</code> handles the cumbersome part of setting up your streaming credentials using the Quix API. </p>"},{"location":"client-library/connect/#code-running-in-quix-platform","title":"Code running in Quix Platform","text":"<p>When you\u2019re running the app in our online IDE or as a Quix deployment, the following code connects to Quix:</p> PythonC# <pre><code>from quixstreams import QuixStreamingClient\nclient = QuixStreamingClient()\n</code></pre> <pre><code>var client = new QuixStreams.Streaming.QuixStreamingClient();\n</code></pre>"},{"location":"client-library/connect/#code-running-locally","title":"Code running locally","text":"<p>If you wish to connect to Quix with your code running locally, you\u2019ll have to provide an OAuth2.0 bearer token. Quix have created a token for this, called <code>SDK token</code>. </p> <p>Once you have obtained the token you will have to provide it as an argument to <code>QuixStreamingClient</code> or set <code>Quix__Sdk__Token</code> environment variable. This is shown in the following code:</p> PythonC# <pre><code>from quixstreams import QuixStreamingClient\nclient = QuixStreamingClient('your_token')\n</code></pre> <pre><code>var client = new QuixStreams.Streaming.QuixStreamingClient(\"your_token\");\n</code></pre>"},{"location":"client-library/introduction/","title":"Introduction","text":"<p>Quix Streams makes it quick and easy to develop streaming applications. It\u2019s designed to be used for high-performance telemetry services where you need to process high volumes of data with quick response time in a scalable way.</p> <p>Quix Streams is available for Python and C#.</p> <p>Using Quix Streams, you can:</p> <ul> <li> <p>Publish time-series data to a Kafka topic</p> </li> <li> <p>Subscribe to time-series data from a Kafka topic</p> </li> <li> <p>Process data by subscribing to it from one topic and publishing the results to another one or elsewhere.</p> </li> </ul> <p>To support these operations, Quix Streams provides several useful features and solves common problems you may face when developing real-time streaming applications:</p>"},{"location":"client-library/introduction/#streaming-context","title":"Streaming context","text":"<p>Quix Streams handles stream contexts for you, so all the data from one data source is bundled in the same scope. This allows you to attach metadata to streams.</p> <p>Stream context simplifies processing streams by providing callbacks on the subscribing side. You can keep working with each context (stream) separately or together, depending on your needs.</p> <p>Refer to the Streaming context section of this documentation for more information.</p>"},{"location":"client-library/introduction/#built-in-buffers","title":"Built-in buffers","text":"<p>If you\u2019re sending data at high frequency, processing each message can be costly. Quix Streams provides a built-in buffers features for reading and writing to give you freedom in balancing between latency and cost.</p> <p>Refer to the Built-in buffers section of this documentation for more information.</p>"},{"location":"client-library/introduction/#support-for-data-frames","title":"Support for data frames","text":"<p>In many use cases, multiple parameters are emitted at the same time, so they share one timestamp. Publishing these parameters independently is wasteful. Quix Streams uses a rows system and can work with pandas DataFrame natively. Each row has a timestamp and user-defined tags as indexes.</p> <p>Refer to the Support for Data Frames section of this documentation for more information.</p>"},{"location":"client-library/introduction/#message-splitting","title":"Message splitting","text":"<p>Quix Streams automatically handles large messages on the producer side, splitting them up when required. On the consumer side, those messages are automatically merged back and processed as one.</p> <p>Refer to the Message splitting section of this documentation for more information.</p>"},{"location":"client-library/introduction/#data-serdes","title":"Data ser/des","text":"<p>Quix Streams automatically serializes data from native types in your language. You can work with familiar data types, such as pandas DataFrame, without worrying about conversion. Serialization can be difficult, especially if it is done with performance in mind. We serialize native types using our codecs so you don\u2019t have to implement that.</p> <p>Refer to the Data serialization section of this documentation for more information.</p>"},{"location":"client-library/introduction/#multiple-data-types","title":"Multiple data types","text":"<p>Quix Streams allows you to attach different types of data to your timestamps, such as numbers, string and binary data. This gives Quix Streams the ability to adapt to any streaming application use case.</p> <p>Refer to the Multiple data types section of this documentation for more information.</p>"},{"location":"client-library/introduction/#checkpointing","title":"Checkpointing","text":"<p>Quix Streams allows you to do manual checkpointing when you subscribe to data from a topic. This provides the ability to inform the message broker that you have already processed messages up to one point, called a checkpoint.</p> <p>This is a very important concept when you are developing high-performance streaming applications.</p> <p>Refer to the Checkpointing section of this documentation for more information.</p>"},{"location":"client-library/introduction/#horizontal-scaling","title":"Horizontal scaling","text":"<p>Quix Streams provides horizontal scaling using the streaming context feature. Data scientists or engineers do not have to implement horizontal scaling for stream processing themselves. You can scale the processing models, from one replica to many or back to ensure that your data is distributed between your model replicas.</p> <p>Refer to the Horizontal scaling section of this documentation for more information.</p>"},{"location":"client-library/introduction/#raw-messages","title":"Raw messages","text":"<p>Quix Streams uses an internal protocol which is both data and speed optimized so we do encourage you to use it, but you need to use Quix Streams on both producer and consumer sides as of today. We have plans to support most common formats in near future, but custom formats will always need to be handled manually.</p> <p>For this, we have created a way to publish and subscribe to the raw, unformatted messages and work with them as bytes. This gives you the ability to implement the protocol as needed and convert between formats.</p>"},{"location":"client-library/kafka/","title":"Kafka and Quix Streams","text":"<p>Quix Streams helps you to leverage Kafka\u2019s powerful features with ease.</p>"},{"location":"client-library/kafka/#why-this-is-important","title":"Why this is important","text":"<p>Kafka is a powerful but complex technology to master. Using Quix Streams, you can leverage the power of Kafka without worrying about mastering it. There are just a couple of important concepts to grasp, the rest is handled in the background by Quix Streams.</p>"},{"location":"client-library/kafka/#concepts","title":"Concepts","text":""},{"location":"client-library/kafka/#topic-replica","title":"Topic replica","text":"<p>Each topic can be set to replicate over the Kafka cluster for increased resilience, so a failure of a node will not cause downtime of your processing pipeline. For example, if you set replica to 2, every message you send to the topic will be replicated twice in the cluster.</p>"},{"location":"client-library/kafka/#topic-retention","title":"Topic retention","text":"<p>Each topic has temporary storage. Every message sent to the topic will live in Kafka for a configured amount of time or size. That means that a consumer can join the topic later and still consume messages. If your processing pipeline has downtime, no data is lost.</p>"},{"location":"client-library/kafka/#topic-partitions","title":"Topic partitions","text":"<p>Each Kafka topic is created with a number of partitions. You can add more partitions later, but you can\u2019t remove them. Each partition is an independent queue that preserves the order of messages. Quix Streams restricts all messages inside one stream to the same single partition. That means that inside one stream, a consumer can rely on the order of messages. Partitions are spread across your Kafka cluster, over different Kafka nodes, for improved performance.</p>"},{"location":"client-library/kafka/#redistribution-of-load","title":"Redistribution of load","text":"<p>Streams are redistributed over available partitions. With an increasing number of streams, each partition will end up with approximately the same number of streams.</p> <p>Warning</p> <p>The number of partitions sets the limit for how many replicas of one model can process the topic. For example: A topic with three partitions can be processed with up to 3 instances of a model. The fourth instance will remain idle.</p>"},{"location":"client-library/kafka/#consumer-group","title":"Consumer group","text":"<p>The Consumer group is a concept of how to horizontally scale topic processing. Each consumer group has an ID, which you set when opening a connection to the topic:</p> <pre><code>topic_consumer = client.get_topic_consumer(\"{topic}\",\"{your-consumer-group-id}\")\n</code></pre> <p>If you deploy this model with 3 instances, the partitions will be shared across the three instances to even out the load.</p> <ul> <li>If you increase the instance count, some partitions will get reassigned to new instances of the model.</li> <li>If you decrease the instance count, partitions left by leaving instances get reassigned to remaining instances in the consumer group.</li> </ul>"},{"location":"client-library/kafka/#checkpointing","title":"Checkpointing","text":"<p>We can think of Kafka temporary storage as a processing queue for each partition. Consumer groups read from this queue and regularly commit offsets to track which messages were already processed. By default, this is done by Quix Streams automatically, but you can override that by manually committing an offset when you are done processing a set of rows.</p> <pre><code>from quixstreams import CommitMode\ntopic_consumer = client.get_topic_consumer(\"{topic}\",\"{your-consumer-group-id}\", commit_settings=CommitMode.Manual)\ntopic_consumer.commit()\n</code></pre> <p>The consumer group is playing an important role here as offset commits are associated with the consumer group ID. That means that if you connect to the same topic with a different consumer group ID, the model will start reading from the offset specified, which is latest by default.</p> <p>Note</p> <p>When you open a topic you can also choose where to start reading data from. Either read all the data from the start or only read the new data as it arrives. Read more here.</p> <p>Note</p> <p>You can use Kafka without consumer group. If not using consumer group, you must specify offset like Latest (default), Earliest in order to read from Kafka, as Kafka won't have any previously stored value available. Commit and checkpointing is only available with consumer groups.</p> <p>Tip</p> <p>If you want to consume data from the topic locally for debugging purposes, and the model is running elsewhere at the same time, make sure that you change consumer group ID to prevent clashing with the the other deployment.</p>"},{"location":"client-library/kafka/#data-grouping","title":"Data grouping","text":"<p>Topics group data streams from a single type of source. The golden rule for maximum performance is to always maintain one schema per topic.</p> <p>For example:</p> <ul> <li>For connected car data you could create individual topics to group data from different systems like the engine, transmission, electronics, chassis, infotainment systems.</li> <li>For games you might create individual topics to separate player, game and machine data.</li> <li>For consumer apps you could create a topic each source i.e one for your IOS app, one for your Android app, and one for your web app.</li> <li>For live ML pipelines you\u2019ll want to create a topic for each stage of the pipeline ie raw-data-topic \u2192 cleaning model \u2192 clean-data-topic \u2192 ML model \u2192 results topic.</li> </ul>"},{"location":"client-library/kafka/#data-governance","title":"Data governance","text":"<p>Topics are key to good data governance. Use them to organize your data by:</p> <ul> <li>Group data streams by type or source.</li> <li>Use separate topics for raw, clean or processed data.</li> <li>Create prototyping topics to publish results of models in development.</li> </ul>"},{"location":"client-library/process/","title":"Processing data","text":"<p>Quix Streams is specifically designed to make real-time data processing easy. We provide high-performance technology, powered by our experience in F1, in a way that anybody with basic development skills can understand and use it very quickly.</p> <p>Tip</p> <p>The Quix Portal offers you easy-to-use, auto-generated examples for reading, writing, and processing data based on our open source library. These examples work directly with your workspace topics. You can deploy these examples in our serverless environment with just a few clicks. For a quick test of the capabilities of the library, we recommend starting with those auto-generated examples.</p> <p>Other streaming platforms are tied to a narrow set of functions, queries, or syntax to set up a data processor or model. This limited approach is often only suitable for some use cases, and tends to have a steep learning curve. The feature limitations and time investment required form a barrier to entry for inexperienced developers and data scientists alike.</p> <p>Our approach is simpler and far more powerful than other streaming solutions. So much so that we can\u2019t show you any Quix Streams related functions here because you literally don\u2019t need them if you use Quix Streams.</p> <p>With Quix Streams, you are not tied to complicated functions, lambdas, maps or query libraries to be able to deploy and process data in real time. You just need to know how to subscribe and publish data with Quix Streams \u2014 that\u2019s it, the rest is up to you and your imagination.</p> <p>Let\u2019s see some examples of how to subscribe to and publish data using Quix Streams. We just subscribe to data from the message broker, process it, and publish it back to the message broker.</p> Python - Data FramePython - PlainC# <pre><code>from quixstreams import TopicConsumer, StreamConsumer\nusing pandas as pd\n# Callback triggered for each new data frame\ndef on_dataframe_received_handler(stream: StreamConsumer, df: pd.DataFrame):\noutput_df = pd.DataFrame()\n# you can use 'time', 'timestamp', 'datetime' for your own dataframes, but\n# when you are given a dataframe by this call or data.to_dataframe\n# 'timestamp' will be used as the column name\noutput_df[\"time\"] = df[\"timestamp\"]\noutput_df[\"TAG__LapNumber\"] = df[\"TAG__LapNumber\"]\n# If braking force applied is more than 50%, we mark HardBraking with True\noutput_df[\"HardBraking\"] = df.apply(lambda row: \"True\" if row.Brake &gt; 0.5 else \"False\", axis=1)\nstream_producer.timeseries.publish(output_df)  # Send data to the output stream\nstream_consumer.timeseries.on_dataframe_received = on_dataframe_received_handler\n</code></pre> <pre><code>from quixstreams import TopicConsumer, StreamConsumer, TimeseriesData\n# Callback triggered for each new data frame\ndef on_data_received_handler(stream: StreamConsumer, data: TimeseriesData):\nwith data:\nfor row in data.timestamps:\n# If braking force applied is more than 50%, we mark HardBraking with True\nhard_braking = row.parameters[\"Brake\"].numeric_value &gt; 0.5\nstream_producer.timeseries \\\n                .add_timestamp(row.timestamp) \\\n                .add_tag(\"LapNumber\", row.tags[\"LapNumber\"]) \\\n                .add_value(\"HardBraking\", hard_braking) \\\n                .publish()\nstream_consumer.timeseries.on_data_received = on_dataframe_received_handler\n</code></pre> <pre><code>streamConsumer.timeseries.OnDataReceived += (stream, args) =&gt;\n{\nvar outputData = new TimeseriesData();\n// We calculate mean value for each second of data to effectively down-sample source topic to 1Hz.\noutputData.AddTimestamp(args.Data.Timestamps.First().Timestamp)\n.AddValue(\"ParameterA 10Hz\", args.Data.Timestamps.Average(s =&gt; s.Parameters[\"ParameterA\"].NumericValue.GetValueOrDefault()))\n.AddValue(\"ParameterA source frequency\", args.Data.Timestamps.Count);\n// Send data back to the stream\nstreamProducer.Timeseries.Publish(outputData);\n};\n</code></pre> <p>So, because you are not tied to any narrow processing architecture, you can use any methods, classes or libraries that you are already familiar with to implement your model or data processor.</p> <p>Check out more samples using our open source library samples.</p>"},{"location":"client-library/publish/","title":"Publishing time-series data","text":"<p>Quix Streams allows you to publish data using stream context to your topic. It lets you create new streams, append data to existing streams, organize streams in folders, and add context to the streams.</p>"},{"location":"client-library/publish/#connect-to-quix","title":"Connect to Quix","text":"<p>In order to start publishing data to Quix you need an instance of <code>KafkaStreamingClient</code>. This is the entry point where you begin publishing to the topics. To create an instance, use the following code:</p> PythonC# <pre><code>from quixstreams import KafkaStreamingClient\nclient = KafkaStreamingClient('127.0.0.1:9092')\n</code></pre> <pre><code>var client = new QuixStreams.Streaming.KafkaStreamingClient(\"127.0.0.1:9092\");\n</code></pre> <p>You can find other ways to connect to your message broker in the Connect section.</p>"},{"location":"client-library/publish/#create-a-topic-producer","title":"Create a topic producer","text":"<p>In order to publish data to a topic you need an instance of <code>TopicProducer</code>. This instance allow you to publish data and additional context for streams using the provided topic. You can create an instance using the client\u2019s <code>get_topic_producer</code> method, passing the <code>TOPIC</code> as the parameter.</p> PythonC# <pre><code>topic_producer = client.get_topic_producer(TOPIC)\n</code></pre> <pre><code>var topicProducer = client.GetTopicProducer(TOPIC);\n</code></pre>"},{"location":"client-library/publish/#create-reopen-a-stream","title":"Create / reopen a stream","text":"<p>Streams are the central context of data in Quix Streams. Streams make it easy to manage, discover, and work with your data. You can create as many streams as you want using the <code>create_stream</code> method of your <code>TopicProducer</code> instance:</p> PythonC# <pre><code>stream = topic_producer.create_stream()\n</code></pre> <pre><code>var stream = topicProducer.CreateStream();\n</code></pre> <p>A stream ID is auto-generated, but you can also pass a <code>StreamId</code> to the method to append data to an existing stream. This is also useful when you simply wish to have consistent streamid for to be able to continue the same stream in the future.</p> PythonC# <pre><code>stream = topic_producer.create_stream(\"existing-stream-id\")\n</code></pre> <pre><code>var stream = topicProducer.CreateStream(\"existing-stream-id\");\n</code></pre>"},{"location":"client-library/publish/#stream-properties","title":"Stream properties","text":"<p>You can add optional context to your streams by adding a name, some metadata or a default location.</p> <p>You can add using the <code>Properties</code> options of the generated <code>stream</code> instance:</p> PythonC# <pre><code>stream.properties.name = \"Hello World Python stream\"\nstream.properties.location = \"/test/location\"\nstream.properties.metadata[\"metakey\"] = \"value\"\nstream.properties.metadata[\"metakey2\"] = \"value2\"\n</code></pre> <pre><code>stream.Properties.Name = \"Hello World C# stream\";\nstream.Properties.Location = \"/test/location\";\nstream.Properties.Metadata[\"metakey\"] = \"value1\";\nstream.Properties.Metadata[\"metakey2\"] = \"value2\";\n</code></pre>"},{"location":"client-library/publish/#stream-name","title":"Stream name","text":"<p>When using Quix SaaS, the stream name is the display name of your stream in the platform. If you specify one, Quix SaaS will use it instead of the Stream Id to represent your stream inside the platform. For example, the following name:</p> PythonC# <pre><code>stream.properties.name = \"Hello World my first stream\"\n</code></pre> <pre><code>stream.Properties.Name = \"Hello World my first stream\";\n</code></pre> <p>Would result in this visualization in the list of streams of your workspace:</p> <p></p>"},{"location":"client-library/publish/#stream-location","title":"Stream location","text":"<p>The stream location property defines a default folder for the stream in the folder structure of your Persisted steams.</p> <p>For example, the following location:</p> PythonC# <pre><code>stream.properties.location = \"/Game/Codemasters/F1-2019/{track}\"\n</code></pre> <pre><code>stream.Properties.Location = $\"/Game/Codemasters/F1-2019/{track}\"\n</code></pre> <p>Would result in this hierarchy:</p> <p></p> <p>Any streams sent without a location property will be located under the \"Root\" level by default.</p>"},{"location":"client-library/publish/#close-a-stream","title":"Close a stream","text":"<p>Streams can be left open 24/7 if you aren\u2019t sure when the next data will arrive, but they can and should be closed when you know that you won't be publishing any more data to signal consumers the stream is over.</p> <p>However, sometimes a stream can be closed for other reasons, such as if an error occurs in the publisher code, or something unexpected happens.</p> <p>These snippets show you how to close a stream and how to specify the <code>StreamEndType</code>:</p> PythonC# <pre><code>stream.close()  # same as when used with StreamEndType.Closed\nstream.close(StreamEndType.Closed)\nstream.close(StreamEndType.Aborted)\nstream.close(StreamEndType.Terminated)\n</code></pre> <pre><code>stream.Close(); // same as when used with StreamEndType.Closed\nstream.Close(StreamEndType.Closed);\nstream.Close(StreamEndType.Aborted);\nstream.Close(StreamEndType.Terminated);\n</code></pre> <p>The <code>StreamEndType</code> can be one of the following possible end types:</p> StreamEndType Description Closed The stream was closed normally Aborted The stream was aborted by your code for your own reasons Terminated The stream was terminated unexpectedly while data was being written <p>Note</p> <p>Terminated state is also used when subscribing to a stream but the <code>StreamConsumer</code> is closed before the stream concluded for other reasons. This can happen when closing the <code>TopicConsumer</code> for example.</p>"},{"location":"client-library/publish/#publishing-time-series-data_1","title":"Publishing time-series data","text":"<p>You can now start writing data to your stream. TimeseriesData is the formal class in Quix Streams which represents a time-series data packet in memory. TimeseriesData is meant to be used for time-series data coming from sources that generate data at a regular time basis and with a fixed number of Parameters.</p> <p>Tip</p> <p>If your data source generates data at irregular time intervals and you don\u2019t have a defined list of regular Parameters, the EventData format is probably a better fit for your time-series data.</p>"},{"location":"client-library/publish/#timeseriesdata-format","title":"TimeseriesData format","text":"<p>TimeseriesData is the formal class in Quix Streams which represents a time series data packet in memory. The format consists of a list of Timestamps with their corresponding parameter names and values for each timestamp.</p> <p>You should imagine a TimeseriesData as a table where the Timestamp is the first column of that table and where the parameters are the columns for the values of that table. </p> <p>The following table shows an example:</p> Timestamp Speed Gear 1 120 3 2 123 3 3 125 3 6 110 2 <p>Tip</p> <p>The Timestamp column plus the Tags assigned to it work as the index of the table. If you add values for the same Timestamp and Tags combination, only the last Values will be sent to the stream.</p> <p>The following code would generate the previous <code>TimeseriesData</code> and publish it to the stream:</p> PythonC# <pre><code>from quixstreams import TimeseriesData\ndata = TimeseriesData()\ndata.add_timestamp_nanoseconds(1) \\\n    .add_value(\"Speed\", 120) \\\n    .add_value(\"Gear\", 3)\ndata.add_timestamp_nanoseconds(2) \\\n    .add_value(\"Speed\", 123) \\\n    .add_value(\"Gear\", 3)\ndata.add_timestamp_nanoseconds(3) \\\n    .add_value(\"Speed\", 125) \\\n    .add_value(\"Gear\", 3)\ndata.add_timestamp_nanoseconds(6) \\\n    .add_value(\"Speed\", 110) \\\n    .add_value(\"Gear\", 2)\nstream.timeseries.publish(data)\n</code></pre> <pre><code>var data = new TimeseriesData();\ndata.AddTimestampNanoseconds(1)\n.AddValue(\"Speed\", 120)\n.AddValue(\"Gear\", 3);\ndata.AddTimestampNanoseconds(2)\n.AddValue(\"Speed\", 123)\n.AddValue(\"Gear\", 3);\ndata.AddTimestampNanoseconds(3)\n.AddValue(\"Speed\", 125)\n.AddValue(\"Gear\", 3);\ndata.AddTimestampNanoseconds(6)\n.AddValue(\"Speed\", 110)\n.AddValue(\"Gear\", 2);\nstream.Timeseries.Publish(data);\n</code></pre> <p>Although Quix Streams allows you to publish <code>TimeseriesData</code> to a stream directly, without any buffering, we recommend you use the built-in Buffer feature to achieve high throughput speeds. The following code would publish the same <code>TimeseriesData</code> through a buffer:</p> PythonC# <pre><code>stream.timeseries.buffer.publish(data)\n</code></pre> <pre><code>stream.Timeseries.Buffer.Publish(data);\n</code></pre> <p>Visit the Buffer section of this documentation to find out more about the built-in buffer feature.</p> <p>Quix Streams allows you to attach numbers, strings, or binary data to your timestamps. The code below will attach one of each to the same timestamp:</p> PythonC# <pre><code>from quixstreams import TimeseriesData\nfrom datetime import datetime\ndata = TimeseriesData()\ndata.add_timestamp(datetime.utcnow()) \\\n    .add_value(\"ParameterA\", 10) \\\n    .add_value(\"ParameterB\", \"hello\") \\\n    .add_value(\"ParameterC\", bytearray(\"hello, Quix!\", 'utf-8'))  # use bytearray to publish binary data to a stream.\n</code></pre> <pre><code>var data = new TimeseriesData();\ndata.AddTimestamp(DateTime.UtcNow)\n.AddValue(\"ParameterA\", 10)\n.AddValue(\"ParameterB\", \"hello\")\n.AddValue(\"ParameterC\", Encoding.ASCII.GetBytes(\"Hello Quix!\")); // Publish binary data as a byte array.\n</code></pre>"},{"location":"client-library/publish/#pandas-dataframe-format","title":"pandas DataFrame format","text":"<p>If you use the Python version of Quix Streams you can use pandas DataFrame for writing time-series data. You just need to use the <code>publish</code> methods of the <code>stream.timeseries</code> or <code>stream.timeseries.buffer</code>, passing the Data Frame instead of a TimeseriesData:</p> <pre><code>df = data.to_dataframe()\nstream.timeseries.buffer.publish(df)\n</code></pre> <p>Alternatively, you can convert a pandas DataFrame to a TimeseriesData using the method <code>from_dataframe</code>:</p> <pre><code>with (data := TimeseriesData.from_dataframe(df)):\nstream.timeseries.buffer.publish(data)\n</code></pre> <p>Tip</p> <p>The conversions from pandas DataFrame to TimeseriesData have an intrinsic cost overhead. For high-performance models using pandas DataFrame, you should use pandas DataFrame methods provided by Quix Streams that are optimized for doing as few conversions as possible.</p>"},{"location":"client-library/publish/#timestamps","title":"Timestamps","text":"<p>Quix Streams supports common date and time formats for timestamps when adding data to a stream.</p> <p>There are several helper functions to add new timestamps to <code>Buffer</code>, <code>TimeseriesData</code>, and <code>EventData</code> instances with several types of date/time formats.</p> <p>These are all the common helper functions:</p> PythonC# <ul> <li><code>add_timestamp(datetime: datetime)</code> : Add a new timestamp in <code>datetime</code> format. Default <code>epoch</code> will never be added to this.    </li> <li><code>add_timestamp(time: timedelta)</code> : Add a new timestamp in <code>timedelta</code> format since the default <code>epoch</code> determined in the stream.    </li> <li><code>add_timestamp_milliseconds(milliseconds: int)</code> : Add a new timestamp in milliseconds since the default <code>epoch</code> determined in the stream.    </li> <li><code>add_timestamp_nanoseconds(nanoseconds: int)</code> : Add a new timestamp in nanoseconds since the default <code>epoch</code> determined in the stream.</li> </ul> <ul> <li><code>AddTimestamp(DateTime dateTime)</code> : Add a new timestamp in <code>DateTime</code> format. Default <code>Epoch</code> will never be added to this.    </li> <li><code>AddTimestamp(TimeSpan timeSpan)</code> : Add a new timestamp in <code>TimeSpan</code> format since the default <code>Epoch</code> determined in the stream.    </li> <li><code>AddTimestampMilliseconds(long timeMilliseconds)</code> : Add a new timestamp in milliseconds since the default <code>Epoch</code> determined in the stream.    </li> <li><code>AddTimestampNanoseconds(long timeNanoseconds)</code> : Add a new timestamp in nanoseconds since the default <code>Epoch</code> determined in the stream.</li> </ul>"},{"location":"client-library/publish/#epoch","title":"Epoch","text":"<p>There is a stream property called <code>Epoch</code> (set to 0 by default, meaning 00:00:00 on 1 January 1970) that is added to every timestamp (except for datetime formats) when it\u2019s added to the stream. You can use any value you like to act as a base, from which point timestamps will be relative to.</p> <p>The following code indicates to Quix Streams to set the current date as epoch and add it to each timestamp added to the stream:</p> PythonC# <pre><code>from datetime import date\nstream.epoch = date.today()\n</code></pre> <pre><code>stream.Epoch = DateTime.Today;\n</code></pre> <p>Adding data without using Epoch property:</p> PythonC# <pre><code>stream.timeseries.buffer \\\n    .add_timestamp(datetime.datetime.utcnow()) \\\n    .add_value(\"ParameterA\", 10) \\\n    .add_value(\"ParameterB\", \"hello\") \\\n    .publish()\n</code></pre> <pre><code>stream.Timeseries.Buffer\n.AddTimestamp(DateTime.UtcNow)\n.AddValue(\"ParameterA\", 10)\n.AddValue(\"ParameterB\", \"hello\")\n.Publish();\n</code></pre> <p>Or we can add a timestamp 1000ms from the epoch \"Today\":</p> PythonC# <pre><code>stream.epoch = date.today()\nstream.timeseries.buffer \\\n    .add_timestamp_milliseconds(1000) \\\n    .add_value(\"ParameterA\", 10) \\\n    .add_value(\"ParameterB\", \"hello\") \\\n    .publish()\n</code></pre> <pre><code>stream.Epoch = DateTime.Today;\nstream.Timeseries.Buffer\n.AddTimestampInMilliseconds(1000)\n.AddValue(\"ParameterA\", 10)\n.AddValue(\"ParameterB\", \"hello\")\n.Publish();\n</code></pre>"},{"location":"client-library/publish/#using-a-buffer","title":"Using a Buffer","text":"<p>Quix Streams provides you with an optional programmable buffer which you can tailor to your needs. Using buffers to publish data allows you to achieve better compression and higher throughput. </p> PythonC# <p>You can use the <code>buffer</code> property embedded in the <code>parameters</code> property of your <code>stream</code>: <pre><code>stream.timeseries.buffer.packet_size = 100\n</code></pre></p> <p>You can use the <code>Buffer</code> property embedded in the <code>Parameters</code> property of your <code>stream</code>: <pre><code>stream.Timeseries.Buffer.PacketSize = 100;\n</code></pre></p> <p>The code above configures the buffer to publish a packet when the size of the buffer reaches 100 timestamps.</p> PythonC# <p>Writing a TimeseriesData to that buffer is as simple as using the <code>publish</code> method of that built-in <code>buffer</code>: <pre><code>stream.timeseries.buffer.publish(data)\n</code></pre></p> <p>Writing a TimeseriesData to that buffer is as simple as using the <code>Publish</code> method of that built-in <code>Buffer</code>: <pre><code>stream.Timeseries.Buffer.Publish(data);\n</code></pre></p> <p>Quix Streams also allows you to publish data to the buffer without creating a <code>TimeseriesData</code> instance explicitly. To do so, you can use the same helper methods that are supported by the <code>TimeseriesData</code> class like <code>add_timestamp</code>, <code>add_value</code> or <code>add_tag</code>. Then use the <code>publish</code> method to publish that timestamp to the buffer.</p> PythonC# <pre><code>stream.timeseries.buffer \\\n    .add_timestamp(datetime.utcnow()) \\\n    .add_value(\"ParameterA\", 10) \\\n    .add_value(\"ParameterB\", \"hello\") \\\n    .add_value(\"ParameterC\", bytearray(\"hello, Quix!\", 'utf-8')) \\\n    .publish()\n</code></pre> <pre><code>stream.Timeseries.Buffer\n.AddTimestamp(DateTime.UtcNow)\n.AddValue(\"ParameterA\", 10)\n.AddValue(\"ParameterB\", \"hello\")\n.AddValue(\"ParameterC\", Encoding.ASCII.GetBytes(\"Hello Quix!\")) // Publish binary data as a byte array.\n.Publish();\n</code></pre> <p>You can configure multiple conditions to determine when the Buffer has to release data. If any of these conditions become true, the buffer will release a new packet of data and that data is cleared from the buffer:</p> PythonC# <ul> <li><code>buffer.buffer_timeout</code>: The maximum duration in milliseconds for which the buffer will be held before releasing the data. A packet of data is released when the configured timeout value has elapsed from the last data received in the buffer.    </li> <li><code>buffer.packet_size</code>: The maximum packet size in terms of number of timestamps. Each time the buffer has this number of timestamps, the packet of data is released.    </li> <li><code>buffer.time_span_in_nanoseconds</code>: The maximum time between timestamps in nanoseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released.    </li> <li><code>buffer.time_span_in_milliseconds</code>: The maximum time between timestamps in milliseconds. When the difference between the earliest and latest buffered timestamp surpasses this number the packet of data is released. Note: This is a millisecond converter on top of <code>time_span_in_nanoseconds</code>. They both work with the same underlying value.    </li> <li><code>buffer.custom_trigger_before_enqueue</code>: A custom function which is invoked before adding a new timestamp to the buffer. If it returns true, the packet of data is released before adding the timestamp to it.    </li> <li><code>buffer.custom_trigger</code>: A custom function which is invoked after adding a new timestamp to the buffer. If it returns true, the packet of data is released with the entire buffer content.    </li> <li><code>buffer.filter</code>: A custom function to filter the incoming data before adding it to the buffer. If it returns true, data is added, otherwise it isn\u2019t.</li> </ul> <ul> <li><code>Buffer.BufferTimeout</code>: The maximum duration in milliseconds for which the buffer will be held before releasing the data. A packet of data is released when the configured timeout value has elapsed from the last data received in the buffer.    </li> <li><code>Buffer.PacketSize</code>: The maximum packet size in terms of number of timestamps. Each time the buffer has this number of timestamps, the packet of data is released.    </li> <li><code>Buffer.TimeSpanInNanoseconds</code>: The maximum time between timestamps in nanoseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released.    </li> <li><code>Buffer.TimeSpanInMilliseconds</code>: The maximum time between timestamps in milliseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. Note: This is a millisecond converter on top of <code>TimeSpanInNanoseconds</code>. They both work with the same underlying value.    </li> <li><code>Buffer.CustomTriggerBeforeEnqueue</code>: A custom function which is invoked before adding a new timestamp to the buffer. If it returns true, the packet of data is released before adding the timestamp to it.    </li> <li><code>Buffer.CustomTrigger</code>: A custom function which is invoked after adding a new timestamp to the buffer. If it returns true, the packet of data is released with the entire buffer content.    </li> <li><code>Buffer.Filter</code>: A custom function to filter the incoming data before adding it to the buffer. If it returns true, data is added, otherwise it isn\u2019t.</li> </ul>"},{"location":"client-library/publish/#examples","title":"Examples","text":"<p>The following buffer configuration will publish data every 100ms or, if no data is buffered in the 1 second timeout period, it will flush and empty the buffer anyway:</p> PythonC# <pre><code>stream.timeseries.buffer.time_span_in_milliseconds = 100\nstream.timeseries.buffer.buffer_timeout = 1000\n</code></pre> <pre><code>stream.Timeseries.Buffer.TimeSpanInMilliseconds = 100;\nstream.Timeseries.Buffer.BufferTimeout = 1000;\n</code></pre> <p>The following buffer configuration will publish data every 100ms window or if critical data is added to it:</p> PythonC# <pre><code>stream.timeseries.buffer.time_span_in_milliseconds = 100\nstream.timeseries.buffer.custom_trigger = lambda data: data.timestamps[0].tags[\"is_critical\"] == 'True'\n</code></pre> <pre><code>stream.Timeseries.Buffer.TimeSpanInMilliseconds = 100;\nstream.Timeseries.Buffer.CustomTrigger = data =&gt; data.Timestamps[0].Tags[\"is_critical\"] == \"True\";\n</code></pre>"},{"location":"client-library/publish/#parameter-definitions","title":"Parameter definitions","text":"<p>Quix Streams allows you to define metadata for parameters and events to describe them. You can define things like human readable names, descriptions, acceptable ranges of values, etc. Quix SaaS uses some of this configuration when visualizing data on the platform, but you can also use them in your own models, bridges, or visualization implementations.</p> PythonC# <p>We call this parameter metadata <code>ParameterDefinitions</code>, and all you need to do is to use the <code>add_definition</code> helper function of the <code>stream.timeseries</code> property:</p> <pre><code>stream.timeseries.add_definition('ParameterIdForCode', 'DisplayNameForHumans', 'Additional Description')\n</code></pre> <p>We call this parameter metadata <code>ParameterDefinitions</code>, and all you need to do is to use the <code>AddDefinition</code> helper function of the <code>stream.timeseries</code> property:</p> <pre><code>stream.Timeseries.AddDefinition(\"ParameterIdForCode\", \"DisplayNameForHumans\", \"Additional Description\")\n</code></pre> <p>Once you have added a new definition, you can attach some additional properties to it. This is the list of visualization and metadata options you can attach to a <code>ParameterDefinition</code>:</p> PythonC# <ul> <li><code>set_range(minimum_value: float, maximum_value: float)</code> : Set the minimum and maximum range of the parameter.    </li> <li><code>set_unit(unit: str)</code> : Set the unit of the parameter.    </li> <li><code>set_format(format: str)</code> : Set the format of the parameter.    </li> <li><code>set_custom_properties(custom_properties: str)</code> : Set the custom properties of the parameter for your own needs.</li> </ul> <p>Example:</p> <pre><code>stream.timeseries \\\n    .add_definition(\"vehicle-speed\", \"Vehicle speed\", \"Current vehicle speed measured using wheel sensor\") \\\n    .set_unit(\"kmh\") \\\n    .set_range(0, 400)\n</code></pre> <ul> <li><code>SetRange(double minimumValue, double maximumValue)</code> : Set the minimum and maximum range of the parameter.    </li> <li><code>SetUnit(string unit)</code> : Set the unit of the parameter.    </li> <li><code>SetFormat(string format)</code> : Set the format of the parameter.    </li> <li><code>SetCustomProperties(string customProperties)</code> : Set the custom properties of the parameter for your own needs</li> </ul> <p>Example:</p> <pre><code>stream.Timeseries\n.AddDefinition(\"vehicle-speed\", \"Vehicle speed\", \"Current vehicle speed measured using wheel sensor\")\n.SetUnit(\"kmh\")\n.SetRange(0, 400);\n</code></pre> <p>The Min and Max range definition sets the Y axis range in the waveform visualisation view in the Quix SaaS. This definition:</p> PythonC# <pre><code>.add_definition(\"Speed\").set_range(0, 400)\n</code></pre> <pre><code>.AddDefinition(\"Speed\").SetRange(0, 400)\n</code></pre> <p>Will set up this view in Data explorer:</p> <p></p> <p>Adding additional <code>Definitions</code> for each parameter allows you to see data with different ranges on the same waveform view:</p> <p></p> <p>You can also define a <code>Location</code> before adding parameter and event definitions. Locations are used to organize the parameters and events in hierarchy groups in the data catalogue. To add a Location you should use the <code>add_location</code> method before adding the definitions you want to include in that group.</p> <p>For example, setting this parameter location:</p> PythonC# <pre><code>stream.timeseries \\\n    .add_location(\"/Player/Motion/Car\") \\\n    .add_definition(\"Pitch\") \\\n    .add_definition(\"Roll\") \\\n    .add_definition(\"Yaw\")\n</code></pre> <pre><code>stream.Timeseries\n.AddLocation(\"/Player/Motion/Car\")\n.AddDefinition(\"Pitch\")\n.AddDefinition(\"Roll\")\n.AddDefinition(\"Yaw\");\n</code></pre> <p>Will result in this parameter hierarchy in the parameter selection dialogs.</p>"},{"location":"client-library/publish/#writing-events","title":"Writing events","text":"<p><code>EventData</code> is the formal class in Quix Streams which represents an Event data packet in memory. <code>EventData</code> is meant to be used when the data is intended to be consumed only as single unit, such as json payload where properties can't be converted to individual parameters. EventData can also be better for non-standard changes such as a machine shutting down might publish an event named ShutDown.</p> <p>Tip</p> <p>If your data source generates data at regular time intervals, or the information can be organized in a fixed list of Parameters, the TimeseriesData format is a better fit for your time-series data.</p>"},{"location":"client-library/publish/#eventdata-format","title":"EventData format","text":"<p><code>EventData</code> consists of a record with a <code>Timestamp</code>, an <code>EventId</code> and an <code>EventValue</code>.</p> <p>You should imagine a list of <code>EventData</code> instances as a simple table of three columns where the <code>Timestamp</code> is the first column of that table and the <code>EventId</code> and <code>EventValue</code> are the second and third columns, as shown in the following table:</p> Timestamp EventId EventValue 1 failure23 Gearbox has a failure 2 box-event2 Car has entered to the box 3 motor-off Motor has stopped 6 race-event3 Race has finished <p>The following code would generate the list of <code>EventData</code> shown in the previous example and publish it to the stream:</p> PythonC# <pre><code>from quixstreams import EventData\nstream.events.publish(EventData(\"failure23\", 1, \"Gearbox has a failure\"))\nstream.events.publish(EventData(\"box-event2\", 2, \"Car has entered to the box\"))\nstream.events.publish(EventData(\"motor-off\", 3, \"Motor has stopped\"))\nstream.events.publish(EventData(\"race-event3\", 6, \"Race has finished\"))\n</code></pre> <pre><code>stream.Events.Publish(new EventData(\"failure23\", 1, \"Gearbox has a failure\"));\nstream.Events.Publish(new EventData(\"box-event2\", 2, \"Car has entered to the box\"));\nstream.Events.Publish(new EventData(\"motor-off\", 3, \"Motor has stopped\"));\nstream.Events.Publish(new EventData(\"race-event3\", 6, \"Race has finished\"));\n</code></pre> <p>Quix Streams lets you publish Events without creating <code>EventData</code> instances explicitly. To do so, you can use similar helpers to those present in TimeseriesData format such as <code>add_timestamp</code>, <code>add_value</code> or <code>add_tag</code>. Then use the <code>publish</code> method to publish that timestamp to the stream.</p> PythonC# <pre><code>stream.events \\\n    .add_timestamp(1) \\\n    .add_value(\"failure23\", \"Gearbox has a failure\") \\\n    .publish()\nstream.events \\\n    .add_timestamp(2) \\\n    .add_value(\"box-event2\", \"Car has entered to the box\") \\\n    .publish()\nstream.events \\\n    .add_timestamp(3) \\\n    .add_value(\"motor-off\", \"Motor has stopped\") \\\n    .publish()\nstream.events \\\n    .add_timestamp(6) \\\n    .add_value(\"race-event3\", \"Race has finished\") \\\n    .publish()\n</code></pre> <pre><code>stream.Events\n.AddTimestamp(1)\n.AddValue(\"failure23\", \"Gearbox has a failure\")\n.Publish();\nstream.Events\n.AddTimestamp(2)\n.AddValue(\"box-event2\", \"Car has entered to the box\")\n.Publish();\nstream.Events\n.AddTimestamp(3)\n.AddValue(\"motor-off\", \"Motor has stopped\")\n.Publish();\nstream.Events\n.AddTimestamp(6)\n.AddValue(\"race-event3\", \"Race has finished\")\n.Publish();\n</code></pre>"},{"location":"client-library/publish/#event-definitions","title":"Event definitions","text":"<p>As with parameters, you can attach <code>Definitions</code> to each event.</p> <p>This is the whole list of visualization and metadata options we can attach to a <code>EventDefinition</code>:</p> <ul> <li><code>set_level(level: EventLevel)</code> : Set severity level of the event.</li> <li><code>set_custom_properties(custom_properties: str)</code> : Set the custom properties of the event for your own needs.</li> </ul> <p>For example, the following code defines a human readable name and a Severity level for the <code>EventA</code>:</p> PythonC# <pre><code>from quixstreams import EventLevel\nstream.events \\\n    .add_definition(\"EventA\", \"The Event A\") \\\n    .set_level(EventLevel.Critical) \\\n    .set_custom_properties(\"{this could be a json}\")\n</code></pre> <pre><code>stream.Events.AddDefinition(\"EventA\", \"The Event A\").SetLevel(EventLevel.Critical).SetCustomProperties(\"{this could be a json}\");\n</code></pre>"},{"location":"client-library/publish/#tags","title":"Tags","text":"<p>The library allows you to tag data for <code>TimeseriesData</code> and <code>EventData</code> packets. Using tags alongside parameters and events helps when indexing persisted data in the database. Tags allow you to filter and group data with fast queries.</p> <p>Tags work as a part of the primary key inside <code>TimeseriesData</code> and <code>EventData</code>, in combination with the default Timestamp key. If you add data values with the same Timestamps, but a different combination of Tags, the timestamp will be treated as a separate row.</p> <p>For example, the following code:</p> PythonC# <pre><code>from quixstreams import TimeseriesData\ndata = TimeseriesData()\ndata.add_timestamp_nanoseconds(1) \\\n    .add_tag(\"CarId\", \"car1\") \\\n    .add_value(\"Speed\", 120) \\\n    .add_value(\"Gear\", 3)\ndata.add_timestamp_nanoseconds(2) \\\n    .add_tag(\"CarId\", \"car1\") \\\n    .add_value(\"Speed\", 123) \\\n    .add_value(\"Gear\", 3)\ndata.add_timestamp_nanoseconds(3) \\\n    .add_tag(\"CarId\", \"car1\") \\\n    .add_value(\"Speed\", 125) \\\n    .add_value(\"Gear\", 3)\ndata.add_timestamp_nanoseconds(1) \\\n    .add_tag(\"CarId\", \"car2\") \\\n    .add_value(\"Speed\", 95) \\\n    .add_value(\"Gear\", 2)\ndata.add_timestamp_nanoseconds(2) \\\n    .add_tag(\"CarId\", \"car2\") \\\n    .add_value(\"Speed\", 98) \\\n    .add_value(\"Gear\", 2)\ndata.add_timestamp_nanoseconds(3) \\\n    .add_tag(\"CarId\", \"car2\") \\\n    .add_value(\"Speed\", 105) \\\n    .add_value(\"Gear\", 2)\n</code></pre> <pre><code>var data = new TimeseriesData();\ndata.AddTimestampNanoseconds(1)\n.AddTag(\"CarId\", \"car1\")\n.AddValue(\"Speed\", 120)\n.AddValue(\"Gear\", 3);\ndata.AddTimestampNanoseconds(2)\n.AddTag(\"CarId\", \"car1\")\n.AddValue(\"Speed\", 123)\n.AddValue(\"Gear\", 3);\ndata.AddTimestampNanoseconds(3)\n.AddTag(\"CarId\", \"car1\")\n.AddValue(\"Speed\", 125)\n.AddValue(\"Gear\", 3);\ndata.AddTimestampNanoseconds(1)\n.AddTag(\"CarId\", \"car2\")\n.AddValue(\"Speed\", 95)\n.AddValue(\"Gear\", 2);\ndata.AddTimestampNanoseconds(2)\n.AddTag(\"CarId\", \"car2\")\n.AddValue(\"Speed\", 98)\n.AddValue(\"Gear\", 2);\ndata.AddTimestampNanoseconds(3)\n.AddTag(\"CarId\", \"car2\")\n.AddValue(\"Speed\", 105)\n.AddValue(\"Gear\", 2);\n</code></pre> <p>Will generate the following <code>TimeseriesData</code> packet with tagged data:</p> Timestamp CarId Speed Gear 1 car1 120 3 1 car2 95 2 2 car1 123 3 2 car2 98 2 3 car1 125 3 3 car2 105 2 <p>Warning</p> <p>Tags have to be chosen carefully as excessive cardinality leads to performance degradation in the database. You should use tags only for identifiers and not cardinal values.</p> <p>The following example of good tagging practice enables you to query the maximum speed for driver identifier \"Peter\":</p> PythonC# <pre><code>stream.timeseries.buffer \\\n    .add_timestamp(datetime.datetime.utcnow()) \\\n    .add_tag(\"vehicle-plate\", \"SL96 XCX\") \\\n    .add_tag(\"driver-id\", \"Peter\") \\\n    .add_value(\"Speed\", 53) \\\n    .add_value(\"Gear\", 4) \\\n    .publish()\n</code></pre> <pre><code>stream.Timeseries.Buffer\n.AddTimestamp(DateTime.UtcNow)\n.AddTag(\"vehicle-plate\", \"SL96 XCX\")\n.AddTag(\"driver-id\", \"Peter\")\n.AddValue(\"Speed\", 53)\n.AddValue(\"Gear\", 4)\n.Publish();\n</code></pre> <p>The following example of bad tagging practice will lead to excessive cardinality as there will be a large number of different values for the specified tag, Speed:</p> PythonC# <pre><code>stream.timeseries.buffer \\\n    .add_timestamp(datetime.datetime.utcnow()) \\\n    .add_tag(\"Speed\", 53) \\\n    .add_value(\"Gear\", 4) \\\n    .publish()\n</code></pre> <pre><code>stream.Timeseries.Buffer\n.AddTimestamp(DateTime.UtcNow)\n.AddTag(\"Speed\", 53)\n.AddValue(\"Gear\", 4)\n.Publish();\n</code></pre>"},{"location":"client-library/publish/#minimal-example","title":"Minimal example","text":"<p>This is a minimal code example you can use to publish data to a topic using Quix Streams:</p> PythonC# <pre><code>import time\nimport datetime\nfrom quixstreams import *\nclient = KafkaStreamingClient('127.0.0.1:9092')\nwith (topic_producer := client.get_topic_producer(TOPIC_ID)):\nstream = topic_producer.create_stream()\nstream.properties.name = \"Hello World python stream\"\nfor index in range(0, 3000):\nstream.timeseries \\\n            .buffer \\\n            .add_timestamp(datetime.datetime.utcnow()) \\\n            .add_value(\"ParameterA\", index) \\\n            .publish()\ntime.sleep(0.01)\nprint(\"Closing stream\")\nstream.close()\n</code></pre> <pre><code>using System;\nusing System.Threading;\nnamespace WriteHelloWorld\n{\nclass Program\n{\n/// &lt;summary&gt;\n/// Main will be invoked when you run the application\n/// &lt;/summary&gt;\nstatic void Main()\n{\n// Create a client which holds generic details for creating input and output topics\nvar client = new QuixStreams.Streaming.QuixStreamingClient();\nusing var topicProducer = client.GetTopicProducer(TOPIC_ID);\nvar stream = topicProducer.CreateStream();\nstream.Properties.Name = \"Hello World stream\";\nConsole.WriteLine(\"Publishing values for 30 seconds\");\nfor (var index = 0; index &lt; 3000; index++)\n{\nstream.Timeseries.Buffer\n.AddTimestamp(DateTime.UtcNow)\n.AddValue(\"ParameterA\", index)\n.Publish();\nThread.Sleep(10);\n}\nConsole.WriteLine(\"Closing stream\");\nstream.Close();\nConsole.WriteLine(\"Done!\");\n}\n}\n}\n</code></pre>"},{"location":"client-library/publish/#publish-raw-kafka-messages","title":"Publish raw Kafka messages","text":"<p>Quix Streams uses an internal protocol which is both data and speed optimized so we do encourage you to use it, but you need to use Quix Streams on both producer and consumer sides as of today. We have plans to support most common formats in near future, but custom formats will always need to be handled manually.</p> <p>For this, we have created a way to publish and subscribe to the raw, unformatted messages and work with them as bytes. This gives you the ability to implement the protocol as needed and convert between formats.</p> <p>You can publish messages with or without a key. The following example demonstrates how to publish two messages to Kafka, one message with a key, and one without:</p> PythonC# <pre><code>with (producer := client.create_raw_topic_producer(TOPIC_ID)):    \ndata = bytearray(bytes(\"TEXT CONVERTED TO BYTES\",'utf-8'))\n#publish value with KEY to kafka\nmessage = RawMessage(data)\nmessage.key = MESSAGE_KEY\nproducer.publish(message)\n#publish value without key into kafka\nproducer.publish(data)\n</code></pre> <pre><code>using var producer = client.GetRawTopicProducer(TOPIC_ID);\nvar data = new byte[]{1,3,5,7,1,43};\n//Publish value with KEY to kafka\nproducer.Publish(new Streaming.Raw.RawMessage(\nMESSAGE_KEY,\ndata\n));\n//Publish value withhout key to kafka\nproducer.Publish(new Streaming.Raw.RawMessage(\ndata\n));\n</code></pre>"},{"location":"client-library/quickstart/","title":"Quickstart","text":"<p>Quix Streams provides you with a library for developing real-time streaming applications focused on time-series data.</p> <p>If you would like to know more about Quix Streams, you can view the Quix Streams GitHub repository. Quix Streams is open source under the Apache 2.0 license.</p> <p>In this quickstart guide you will learn how to start using Quix Streams as quickly as possible. This guide covers how to:</p> <ul> <li>Create a consumer</li> <li>Create a producer</li> <li>Create a producer/consumer transform </li> <li>Connect to the Quix Platform</li> </ul> <p>The typical stream processing pipline you create with Quix Streams involves producers, consumers, and transforms. Producers publish information into a topic, consumers subscribe to read information from a topic. Transforms typically consume data, process it in some way, and then publish the transformed data to a topic, or stream within a topic.</p> <p>In this guide you'll learn how to create a producer that publishes data to a topic, a consumer that reads data from a topic, and a simple transform that consumes data from a topic, transforms it, and then publishes the new data to a topic.</p> <p>Initially you will work with your local Kafka installation, and then you'll learn how you can connect to Quix Platform. In Quix Platform you can build your stream processing pipelines graphically.</p>"},{"location":"client-library/quickstart/#prerequisites","title":"Prerequisites","text":"<p>The prerequisites for this guide are as follows:</p> <ul> <li>Python 3.x.</li> <li>You have a local installation of Kafka up and running. You use this to test your code.</li> </ul> <p>Optionally:</p> <ul> <li>Sign up for a free Quix account. You may just want to connect to your own Kafka installation, but if you'd like to connect to the Quix Platform you'll need a free account.</li> </ul>"},{"location":"client-library/quickstart/#getting-help","title":"Getting help","text":"<p>If you need help with this guide, then please join our public Slack community <code>The Stream</code>, and ask any questions you have there.</p>"},{"location":"client-library/quickstart/#install","title":"Install","text":"<p>Make sure you have Python 3.x installed by running:</p> <pre><code>python --version\n</code></pre> <p>Install Quix Streams for Python locally:</p> <pre><code>pip install quixstreams\n</code></pre> <p>You can read information about installation for your platform on the PyPi page for Quix Streams.</p> <p>The README for Quix Streams also has the latest information on installation.</p> <p>Note</p> <p>The following sections assume you have a local installation of Kafka running. </p>"},{"location":"client-library/quickstart/#create-a-consumer","title":"Create a Consumer","text":"<p>To create a simple consumer, follow these steps:</p> <ol> <li> <p>Create a directory for your project, you can call it anything you want, and change into the project directory. </p> </li> <li> <p>Create a file called <code>consumer.py</code> that contains the following code:</p> <pre><code>import quixstreams as qx\nimport pandas as pd\n# Client connecting to Kafka instance locally without authentication. \nclient = qx.KafkaStreamingClient('127.0.0.1:9092')\n# Open the input topic where to consume data from.\n# For testing purposes we remove consumer group and always read from latest data.\ntopic_consumer = client.get_topic_consumer(\"quickstart-topic\", consumer_group=None, auto_offset_reset=qx.AutoOffsetReset.Latest)\n# consume streams\ndef on_stream_received_handler(stream_received: qx.StreamConsumer):\nstream_received.timeseries.on_dataframe_received = on_dataframe_received_handler\n# consume data (as Pandas DataFrame)\ndef on_dataframe_received_handler(stream: qx.StreamConsumer, df: pd.DataFrame):\nprint(df.to_string())\n# Hook up events before initiating read to avoid losing out on any data\ntopic_consumer.on_stream_received = on_stream_received_handler\nprint(\"Listening to streams. Press CTRL-C to exit.\")\n# Handle graceful exit\nqx.App.run()\n</code></pre> </li> <li> <p>Run the code:</p> <pre><code>python consumer.py\n</code></pre> </li> </ol> <p>The code will wait for published messages and then print information about any messages received to the console. You'll next build a suitable producer than can publish messages to the example topic.</p> Understand the code <p>Click on the annotations to understand the consumer code:</p> <pre><code>import quixstreams as qx\nimport pandas as pd # (1)\n# Client connecting to Kafka instance locally without authentication. \nclient = qx.KafkaStreamingClient('127.0.0.1:9092') # (2)\n# Open the input topic where to consume data from.\n# For testing purposes we remove consumer group and always read from latest data.\ninput_topic = client.get_topic_consumer(\"quickstart-topic\", consumer_group=None, auto_offset_reset=qx.AutoOffsetReset.Latest) # (3)\n# consume streams\ndef on_stream_received_handler(stream_received: qx.StreamConsumer): # (4)\nstream_received.timeseries.on_dataframe_received = on_dataframe_received_handler # (5)\n# consume data (as Pandas DataFrame)\ndef on_dataframe_received_handler(stream: qx.StreamConsumer, df: pd.DataFrame): # (6)\nprint(df.to_string()) # (7)\n# Hook up events before initiating read to avoid losing out on any data\ninput_topic.on_stream_received = on_stream_received_handler # (8)\nprint(\"Listening to streams. Press CTRL-C to exit.\")\n# Handle graceful exit\nqx.App.run() # (9)\n</code></pre> <ol> <li>Imports the Pandas library can be used to handle tabular data in Quix Streams. This library is supported because it is widely used.</li> <li>Connects to a Kafka server. In this case the Kafka server is running locally.</li> <li>Opens the specified topic for reading.</li> <li>A function definition for the stream callback. This stream event handler will be called for all stream events across all streams.</li> <li>Registers the Pandas data reader callback. This is registered for data events within a stream, not globally for all streams. This is efficient as you might not need to use this handler on many streams.</li> <li>This function defines a Pandas data event callback.</li> <li>The function simply prints a Pandas data frame in this example.</li> <li>Registers the stream callback.</li> <li>Runs the application, and registers code to monitor termination signals. On shutdown the code performs tasks such as closing open file handles, flushing buffers, shutting down threads, and freeing up allocated memory. It also closes input and output streams in the correct order, and creates topics that don't exist on startup.</li> </ol>"},{"location":"client-library/quickstart/#create-a-producer","title":"Create a Producer","text":"<p>To create a simple producer follow these steps:</p> <ol> <li> <p>Start a new terminal tab.</p> </li> <li> <p>In your project directory, create a file called <code>producer.py</code> that contains the following code:</p> <pre><code>import quixstreams as qx\nimport time\nimport datetime\nimport math\n# Client connecting to Kafka instance locally without authentication. \nclient = qx.KafkaStreamingClient('127.0.0.1:9092')\n# Open the output topic where to produce data to.\ntopic_producer = client.get_topic_producer(\"quickstart-topic\")\nstream = topic_producer.create_stream()\nstream.properties.name = \"Hello World python stream\"\nstream.properties.metadata[\"my-metadata\"] = \"my-metadata-value\"\nstream.timeseries.buffer.time_span_in_milliseconds = 100   # Send data in 100 ms chunks\nprint(\"Sending values for 30 seconds.\")\nfor index in range(0, 3000):\nstream.timeseries \\\n        .buffer \\\n        .add_timestamp(datetime.datetime.utcnow()) \\\n        .add_value(\"ParameterA\", math.sin(index / 200.0)) \\\n        .add_value(\"ParameterB\", \"string value: \" + str(index)) \\\n        .add_value(\"ParameterC\", bytearray.fromhex(\"51 55 49 58\")) \\\n        .publish()\ntime.sleep(0.01)\nprint(\"Closing stream\")\nstream.close()\n</code></pre> </li> <li> <p>Run the code:</p> <pre><code>python producer.py\n</code></pre> </li> </ol> <p>The code will publish a series of messages to the specified topic.</p> <ol> <li> <p>Switch to the consumer terminal tab and view the messages being displayed. The following shows an example data frame:</p> <pre><code>                time  ParameterA          ParameterB ParameterC\n0  1675695013706982000    0.687444  string value: 2990    b'QUIX'\n1  1675695013719422000    0.683804  string value: 2991    b'QUIX'\n2  1675695013730504000    0.680147  string value: 2992    b'QUIX'\n3  1675695013745346000    0.676473  string value: 2993    b'QUIX'\n4  1675695013756586000    0.672782  string value: 2994    b'QUIX'\n5  1675695013769315000    0.669075  string value: 2995    b'QUIX'\n6  1675695013782740000    0.665351  string value: 2996    b'QUIX'\n7  1675695013796677000    0.661610  string value: 2997    b'QUIX'\n</code></pre> </li> </ol> <p>You've now created and tested both a producer and consumer that uses Quix Streams.</p> Understand the code <p>Click on the annotations to understand the producer code:</p> <pre><code>import quixstreams as qx\nimport time\nimport datetime\nimport math\n# Client connecting to Kafka instance locally without authentication. \nclient = qx.KafkaStreamingClient('127.0.0.1:9092') # (1)\n# Open the output topic where to produce data to.\ntopic_producer = client.get_topic_producer(\"quickstart-topic\") # (2)\nstream = topic_producer.create_stream() # (3)\nstream.properties.name = \"Quixstart Python stream\" # (4)\nstream.properties.metadata[\"my-metadata\"] = \"my-metadata-value\" # (5)\nstream.timeseries.buffer.time_span_in_milliseconds = 100   # (6)\nprint(\"Sending values for 30 seconds.\")\nfor index in range(0, 3000):\nstream.timeseries \\\n        .buffer \\\n        .add_timestamp(datetime.datetime.utcnow()) \\\n        .add_value(\"ParameterA\", math.sin(index / 200.0)) \\\n        .add_value(\"ParameterB\", \"string value: \" + str(index)) \\\n        .add_value(\"ParameterC\", bytearray.fromhex(\"51 55 49 58\")) \\\n        .publish() # (7)\ntime.sleep(0.01)\nprint(\"Closing stream\")\nstream.close() # (8)\n</code></pre> <ol> <li>Opens a connection to the Kafka server.</li> <li>Opens a topic top write parameter data to.</li> <li>Creates the stream to write to.</li> <li>Sets a stream property, in this case <code>name</code>.</li> <li>Sets application-specific key-value metadata.</li> <li>Sets a stream buffer property. In this case <code>time_span_in_milliseconds</code> is set to 100. The data is then sent in 100ms chunks.</li> <li>Writes parameter data to the stream buffer. A time stamp is added. Also, data of different data types can be added, such as numbers, strings, and binary data.</li> <li>Closes the stream.</li> </ol>"},{"location":"client-library/quickstart/#consumer-producer-transform","title":"Consumer-producer transform","text":"<p>Typically a transform block in Quix will receive some data on an input topic, perform some processing on the data, and then publish data to an output topic. Example code that does this is shown here:</p> <pre><code>import quixstreams as qx\nimport pandas as pd\nclient = qx.KafkaStreamingClient('127.0.0.1:9092')\nprint(\"Opening consumer and producer topics\")\ntopic_consumer = client.get_topic_consumer(\"quickstart-topic\")\ntopic_producer = client.get_topic_producer(\"output-topic\")\ndef on_dataframe_received_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\nprint(df) \nprint('Data transformed') # Transform your data here\n# write data to output topic\ntopic_producer.get_or_create_stream(stream_consumer.stream_id).timeseries.publish(df)\n# read streams\ndef on_stream_received_handler(stream_consumer: qx.StreamConsumer):\nstream_consumer.timeseries.on_dataframe_received = on_dataframe_received_handler\ntopic_consumer.on_stream_received = on_stream_received_handler\n# Hook up to termination signal (for docker image) and CTRL-C\nprint(\"Listening to streams. Press CTRL-C to exit.\")\n# Handle graceful exit\nqx.App.run()\n</code></pre> <p>This example reads data in from the <code>quickstart-topic</code> topic, and then writes the transformed data out to the <code>output-topic</code> topic. The approach is to use callbacks to make the code event driven. You register a callback to handle data on a stream, and then when data is received, the callback to handle data frames is registered and invoked.</p> <p>This approach of consuming, transforming, and producing data is a fundamental of building data processing pipelines in Quix.</p> Understand the code <p>Click on the annotations to understand the producer/consumer code:</p> <pre><code>import quixstreams as qx\nimport pandas as pd\nclient = qx.KafkaStreamingClient('127.0.0.1:9092') # (1)\nprint(\"Opening consumer and producer topics\")\ntopic_consumer = client.get_topic_consumer(\"quickstart-topic\") # (2)\ntopic_producer = client.get_topic_producer(\"output-topic\") # (3)\ndef on_dataframe_received_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame): # (4)\nprint(df) \nprint('Data transformed') # Transform your data here\n# write data to output topic\ntopic_producer.get_or_create_stream(stream_consumer.stream_id).timeseries.publish(df) # (5)\n# read streams\ndef on_stream_received_handler(stream_consumer: qx.StreamConsumer): # (6)\nstream_consumer.timeseries.on_dataframe_received = on_dataframe_received_handler # (7)\ntopic_consumer.on_stream_received = on_stream_received_handler # (8)\n# Hook up to termination signal (for docker image) and CTRL-C\nprint(\"Listening to streams. Press CTRL-C to exit.\")\n# Handle graceful exit\nqx.App.run() # (9)\n</code></pre> <ol> <li>Opens a connection to the Kafka server.</li> <li>Opens the consumer topic, the topic is created if it does not exist.</li> <li>Opens the producer topic, the topic is created if it does not exist.</li> <li>Defines the data frame handler function. In this case it publishes data to the consumer stream.</li> <li>Gets a stream on the producer topic, creating the stream if it does not exist, and then publishes data to this stream.</li> <li>Defines the consumer stream data handler. It simply regsiters the data frame handler in this example.</li> <li>Registers the data frame handler for the consumer stream.</li> <li>Registers the consumer stream data handler.</li> <li>Runs the application, and registers code to monitor termination signals. On shutdown the code performs tasks such as closing open file handles, flushing buffers, shutting down threads, and freeing up allocated memory. It also closes input and output streams in the correct order, and creates topics that don't exist on startup.</li> </ol>"},{"location":"client-library/quickstart/#connecting-to-quix-platform","title":"Connecting to Quix Platform","text":"<p>As well as being able to connect directly to a Kafka installation, either locally (for development purposes), on premise, or in the cloud, you can also connect to the Quix Platform, the SaaS for building real-time stream processing applications. Quix Platform provides the ability to build stream processing applications in a graphical environment, and deploy the applications to the Quix-hosted infrastructure.</p>"},{"location":"client-library/quickstart/#obtaining-a-token","title":"Obtaining a token","text":"<p>To connect to the Quix Platform using Quix Streams, you will need to provide a token for authentication.</p> <ol> <li> <p>Sign up for a free Quix account, and log in.</p> </li> <li> <p>In the Quix Platform, click on <code>Topics</code> on the left-hand navigation. </p> </li> <li> <p>Click on the gear icon. The <code>Broker Settings</code> dialog is displayed. </p> </li> <li> <p>Copy <code>token 1</code> to the clipboard. You will use that in the code that connects to the Quix platform.</p> </li> </ol>"},{"location":"client-library/quickstart/#code-to-connect-to-quix-platform","title":"Code to connect to Quix Platform","text":"<p>The following code snippet shows you how to connect to the Quix Platform:</p> <pre><code>import quixstreams as qx\n# connect to Quix platform with token\nclient = qx.QuixStreamingClient('&lt;your-token&gt;') # Token 1 from Topics in portal\n</code></pre> <p>This connects to the Quix Platform, rather than your local Kafka installation, which is the code you saw previously in this guide.</p> <p>A further example is to rewrite the consumer-producer program you created earlier in this Quickstart, to work with Quix Platform:</p> <pre><code>import quixstreams as qx\nimport pandas as pd\nclient = qx.QuixStreamingClient('&lt;your_sdk_token&gt;')\nprint(\"Opening consumer and producer topics\")\ntopic_consumer = client.get_topic_consumer(\"quickstart-topic\")\ntopic_producer = client.get_topic_producer(\"output-topic\")\ndef on_dataframe_received_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\nprint(df) \n# Transform your data here.\nprint('transformed')\n# write data to output topic\ntopic_producer.get_or_create_stream(stream_consumer.stream_id).timeseries.publish(df)\n# read streams\ndef on_stream_received_handler(stream_consumer: qx.StreamConsumer):\nstream_consumer.timeseries.on_dataframe_received = on_dataframe_received_handler\ntopic_consumer.on_stream_received = on_stream_received_handler\n# Hook up to termination signal (for docker image) and CTRL-C\nprint(\"Listening to streams. Press CTRL-C to exit.\")\n# Handle graceful exit\nqx.App.run()\n</code></pre>"},{"location":"client-library/quickstart/#next-steps","title":"Next steps","text":"<p>Try one of the following resources to continue your Quix learning journey:</p> <ul> <li> <p>Get a free Quix account</p> </li> <li> <p>Quix Streams GitHub</p> </li> <li> <p>Quix definitions</p> </li> <li> <p>The Stream community on Slack</p> </li> <li> <p>Stream processing glossary</p> </li> <li> <p>Sentiment analysis tutorial</p> </li> <li> <p>Kafka setup blog post</p> </li> </ul>"},{"location":"client-library/state-management/","title":"State management","text":"<p>Your code may get restarted multiple times. An user intervention (like manually stopping and starting) or runtime error could cause your application to terminate. </p> <p>Note</p> <p>When using the Quix SaaS, the platform automatically detects the problem and restarts the underlying service in an attempt to recover from the fault.</p> <p>Due to the code being run in memory, each time a deployment restarts, internal variables will be reset. For example, if you were to calculate the count of the elements in the stream, this counter would get reset on each restart. The counter would then start at the default value not knowing what was the last known value in the state of the previous run before program terminated.</p> <p>Quix Streams has state management built in to allow values to be used and persisted across restarts of a given deployment. Quix Streams persists your state using your filesystem at the moment. We have plans to bring you different type of state stores in the future.</p> <p>Note</p> <p>When using the Quix Saas, the platform provides your replicas with a shared state store when enabled.</p>"},{"location":"client-library/state-management/#usage","title":"Usage","text":"<p>To use the library\u2019s state management feature create an instance of <code>LocalFileStorage</code>, then use the available methods on the instance to manipulate the state as needed.</p> PythonC# <pre><code>from quixstreams import LocalFileStorage\nstorage = LocalFileStorage()\n#clear storage ( remove all keys )\nstorage.clear()\n#storage class supports handling of\n#   `str`, `int`, `float`, `bool`, `bytes`, `bytearray` types.\n#set value\nstorage.set(\"KEY1\", 12.51)\nstorage.set(\"KEY2\", \"str\")\nstorage.set(\"KEY3\", True)\nstorage.set(\"KEY4\", False)\n#check if the storage contains key\nstorage.contains_key(\"KEY1\")\n#get value\nvalue = storage.get(\"KEY1\")\n</code></pre> <p>C# supports two ways to call the Storage API.</p> <ul> <li> <p>Synchronous</p> </li> <li> <p>Asynchronous ( methods are with Async suffix )</p> </li> </ul> <p>The Synchronous API. During a call to these synchronous methods, the program thread execution is blocked.</p> <pre><code>var storage = new LocalFileStorage();\n//clear storage ( remove all keys )\nawait storage.Clear();\n//set value to specific key\nawait storage.Set(\"KEY1\", 123);  //long\nawait storage.Set(\"KEY2\", 1.23); //double\nawait storage.Set(\"KEY3\", \"1.23\"); //string\nawait storage.Set(\"KEY4\", new byte[]{12,53,23}); //binary\nawait storage.Set(\"KEY5\", false); //boolean\n//check if the key exists\nawait storage.ContainsKey(\"KEY1\");\n//retrieve value from key\nawait storage.GetLong(\"KEY1\");\nawait storage.GetDouble(\"KEY2\");\nawait storage.GetString(\"KEY3\");\nawait storage.GetBinary(\"KEY4\");\nawait storage.GetBinary(\"KEY5\");\n//list all keys in the storage\nawait storage.GetAllKeys();\n</code></pre> <p>The asynchronous API in which methods do contain Async suffix. These methods use the Task-Based Asynchronous Pattern (TAP) and return Tasks. TAP allows us to use async / await and avoid blocking the main thread on longer-running operations. In this case internal I/O.</p> <pre><code>var storage = new LocalFileStorage();\n//clear storage ( remove all keys )\nawait storage.ClearAsync();\n//set value to specific key\nawait storage.SetAsync(\"KEY1\", 123);  //long\nawait storage.SetAsync(\"KEY2\", 1.23); //double\nawait storage.SetAsync(\"KEY3\", \"1.23\"); //string\nawait storage.SetAsync(\"KEY4\", new byte[]{12,53,23}); //binary\nawait storage.SetAsync(\"KEY5\", false); //boolean\n//check if the key exists\nawait storage.ContainsKeyAsync(\"KEY1\");\n//retrieve value from key\nawait storage.GetLongAsync(\"KEY1\");\nawait storage.GetDoubleAsync(\"KEY2\");\nawait storage.GetStringAsync(\"KEY3\");\nawait storage.GetBinaryAsync(\"KEY4\");\nawait storage.GetBinaryAsync(\"KEY5\");\n//list all keys in the storage\nawait storage.GetAllKeysAsync();\n</code></pre>"},{"location":"client-library/state-management/#in-memory-storage","title":"In memory storage","text":"<p>In Python there is another storage available as an experimental feature called <code>InMemoryStorage</code>. It works identical to <code>LocalFileStorage</code> and also supports dictionary operations such as <code>del</code> or iteration.</p> <p>InMemoryStorage can be used on its own using code below: <pre><code>from quixstreams import InMemoryStorage\nstorage = InMemoryStorage()\nstorage.clear()\nstorage.set(\"floatval\", 12.51)\nstorage.set(\"stringval\", \"str\")\nstorage.set(\"boolval\", True)\nstorage.set(\"objval\", {\"dic\": \"tionary\"})\n</code></pre></p> <p>Alternative can also be used with a backing storage: <pre><code>from quixstreams import InMemoryStorage, LocalFileStorage\nstorage = InMemoryStorage(LocalFileStorage(\"state/test\"))\nstorage.clear()\nstorage.set(\"floatval\", 12.51)\nstorage.set(\"stringval\", \"str\")\nstorage.set(\"boolval\", True)\nstorage.set(\"objval\", {\"dic\": \"tionary\"})\nstorage.flush()  # to write to backing storage\n# can be useful to hook it to consumer.on_committing\ntopic_consumer.on_committing = storage.flush\n# or \ndef on_committing_handler(topic_consumer: qx.TopicConsumer):\nprint(\"Committing!\")\nstorage.flush()  # to write to backing storage\ntopic_consumer.on_committing = on_committing_handler\n</code></pre></p>"},{"location":"client-library/subscribe/","title":"Subscribing to data","text":"<p>Quix Streams allows you to subscribe to data in real time from your topics. In this section, we explain more in-depth how to do it using the library.</p>"},{"location":"client-library/subscribe/#connect-to-quix","title":"Connect to Quix","text":"<p>In order to start subscribing to data from your kafka topics, you need an instance of <code>KafkaStreamingClient</code>. This is the entry point where you begin subscribing to the topics. To create an instance, use the following code:</p> PythonC# <pre><code>from quixstreams import KafkaStreamingClient\nclient = KafkaStreamingClient('127.0.0.1:9092')\n</code></pre> <pre><code>var client = new QuixStreams.Streaming.KafkaStreamingClient(\"127.0.0.1:9092\");\n</code></pre> <p>You can find other ways to connect to your message broker in the Connect section.</p>"},{"location":"client-library/subscribe/#create-a-topic-consumer","title":"Create a topic consumer","text":"<p>Topics are at the center for stream processing operations. In order to subscribe to data in a topic you need an instance of <code>TopicConsumer</code>. This instance allow you to receive all the incoming streams on the specified topic. You can create an instance using the client\u2019s <code>get_topic_consumer</code> method, passing the <code>TOPIC</code> as the parameter.</p> PythonC# <pre><code>topic_consumer = client.get_topic_consumer(TOPIC)\n</code></pre> <pre><code>var topicConsumer = client.GetTopicConsumer(TOPIC);\n</code></pre>"},{"location":"client-library/subscribe/#consumer-group","title":"Consumer group","text":"<p>The Consumer group is a concept used when you want to scale horizontally. Each consumer group is identified using an ID, which you set optionally when opening a connection to the topic for reading:</p> PythonC# <pre><code>topic_consumer = client.get_topic_consumer(\"{topic}\",\"{your-consumer-group-id}\")\n</code></pre> <pre><code>var topicConsumer = client.GetTopicConsumer(\"{topic}\",\"{your-consumer-group-id}\");\n</code></pre> <p>This is how the message broker knows that all the replicas of your process want to share the load of the incoming streams between replicas. Each replica will receive only a subset of the streams incoming to the Input Topic.</p> <p>Warning</p> <p>If you want to consume data from the topic locally for debugging purposes, and the model is also deployed elsewhere, make sure that you change the consumer group ID to prevent clashing with the other deployment. If the clash happens, only one instance will be able to receive data for a partition at the same time.</p>"},{"location":"client-library/subscribe/#subscribing-to-streams","title":"Subscribing to streams","text":"PythonC# <p>Once you have the <code>TopicConsumer</code> instance you can start receiving streams. For each stream received from the specified topic, <code>TopicConsumer</code> will execute the callback <code>on_stream_received</code>. This callback will be invoked every time you receive a new Stream. For example, the following code prints the StreamId for each stream received on that topic:</p> <pre><code>from quixstreams import TopicConsumer, StreamConsumer\ndef on_stream_received_handler(stream_received: StreamConsumer):\nprint(\"Stream received:\" + stream_received.stream_id)\ntopic_consumer.on_stream_received = on_stream_received_handler\ntopic_consumer.subscribe()\n</code></pre> <p>Note</p> <p><code>subscribe()</code> method indicates to moment to start consuming streams and data from your Topic. This should normally happen after you\u2019ve registered callbacks for all the events you want to listen to. <code>App.run()</code> can also be used for this and provides other benefits. Find out more about App.run()</p> <p>Once you have the <code>TopicConsumer</code> instance you can start consuming streams. For each stream received to the specified topic, <code>TopicConsumer</code> will execute the event <code>OnStreamReceived</code>. You can attach a callback to this event to execute code that reacts when you receive a new stream. For example the following code prints the StreamId for each stream received on that Topic:</p> <pre><code>topicConsumer.OnStreamReceived += (topic, newStream) =&gt;\n{\nConsole.WriteLine($\"New stream read: {newStream.StreamId}\");\n};\ntopicConsumer.Subscribe();\n</code></pre> <p>Tip</p> <p>The <code>Subscribe</code> method indicates to moment to start consuming streams and data from your Topic. This should normally happen after you\u2019ve registered callbacks for all the events you want to listen to. <code>App.run()</code> can also be used for this and provides other benefits. Find out more about App.run()</p>"},{"location":"client-library/subscribe/#subscribing-to-time-series-data","title":"Subscribing to time-series data","text":""},{"location":"client-library/subscribe/#timeseriesdata-format","title":"TimeseriesData format","text":"<p>TimeseriesData is the formal class in Quix Streams which represents a time series data packet in memory. The format consists of a list of Timestamps with their corresponding parameter names and values for each timestamp.</p> <p>You should imagine a TimeseriesData as a table where the Timestamp is the first column of that table and where the parameters are the columns for the values of that table. </p> <p>The following table shows an example:</p> Timestamp Speed Gear 1 120 3 2 123 3 3 125 3 6 110 2 PythonC# <p>You can subscribe to time-series data from streams using the <code>on_data_received</code> callback of the <code>StreamConsumer</code> instance. For instance, in the following example we consume and print the first timestamp and value of the parameter <code>ParameterA</code> received in the TimeseriesData packet:</p> <pre><code>from quixstreams import TopicConsumer, StreamConsumer, TimeseriesData\ndef on_stream_received_handler(stream_received: StreamConsumer):\nstream_received.timeseries.on_data_received = on_timeseries_data_received_handler\ndef on_timeseries_data_received_handler(stream: StreamConsumer, data: TimeseriesData):\nwith data:\ntimestamp = data.timestamps[0].timestamp\nnum_value = data.timestamps[0].parameters['ParameterA'].numeric_value\nprint(\"ParameterA - \" + str(timestamp) + \": \" + str(num_value))\ntopic_consumer.on_stream_received = on_stream_received_handler\ntopic_consumer.subscribe()\n</code></pre> <p>Note</p> <p><code>subscribe()</code> starts consuming from the topic however, <code>App.run()</code> can also be used for this and provides other benefits.</p> <p>Find out more about App.run()</p> <p>You can subscribe to time-series data from streams using the <code>OnDataReceived</code> event of the <code>StreamConsumer</code> instance. For instance, in the following example we consume and print the first timestamp and value of the parameter <code>ParameterA</code> received in the TimeseriesData packet:</p> <pre><code>topicConsumer.OnStreamReceived += (topic, streamConsumer) =&gt;\n{\nstreamConsumer.Timeseries.OnDataReceived += (sender, args) =&gt;\n{\nvar timestamp = args.Data.Timestamps[0].Timestamp;\nvar numValue = args.Data.Timestamps[0].Parameters[\"ParameterA\"].NumericValue;\nConsole.WriteLine($\"ParameterA - {timestamp}: {numValue}\");\n};\n};\ntopicConsumer.Subscribe();\n</code></pre> <p>Note</p> <p><code>subscribe()</code> starts consuming from the topic however, <code>App.run()</code> can also be used for this and provides other benefits.</p> <p>Find out more about App.run()</p> <p>Quix Streams supports numeric, string, and binary value types. You should use the proper property depending of the value type of your parameter:</p> PythonC# <ul> <li><code>numeric_value</code>: Returns the numeric value of the parameter, represented as a <code>float</code> type.    </li> <li><code>string_value</code>: Returns the string value of the parameter, represented as a <code>string</code> type.    </li> <li><code>binary_value</code>: Returns the binary value of the parameter, represented as a <code>bytearray</code> type.</li> </ul> <ul> <li><code>NumericValue</code>: Returns the numeric value of the parameter, represented as a <code>double</code> type.    </li> <li><code>StringValue</code>: Returns the string value of the parameter, represented as a <code>string</code> type.    </li> <li><code>BinaryValue</code>: Returns the binary value of the parameter, represented as an array of <code>byte</code>.</li> </ul> <p>This is a simple example showing how to consume <code>Speed</code> values of the <code>TimeseriesData</code> used in the previous example:</p> PythonC# <pre><code>for ts in data.timestamps:\ntimestamp = ts.timestamp_nanoseconds\nnumValue = ts.parameters['Speed'].numeric_value\nprint(\"Speed - \" + str(timestamp) \": \" + str(numValue))\n</code></pre> <pre><code>foreach (var timestamp in data.Timestamps)\n{\nvar timestamp = timestamp.TimestampNanoseconds;\nvar numValue = timestamp.Parameters[\"Speed\"].NumericValue;\nConsole.WriteLine($\"Speed - {timestamp}: {numValue}\");\n}\n</code></pre> <p>output:</p> <pre><code>Speed - 1: 120\nSpeed - 2: 123\nSpeed - 3: 125\nSpeed - 6: 110\n</code></pre>"},{"location":"client-library/subscribe/#pandas-dataframe-format","title":"pandas DataFrame format","text":"<p>If you use the Python version of Quix Streams you can use pandas DataFrame for consuming and publishing time-series data. Use the callback <code>on_dataframe_received</code> instead of <code>on_data_received</code> when consuming from a stream:</p> <pre><code>from quixstreams import TopicConsumer, StreamConsumer\ndef on_stream_received_handler(stream_received: StreamConsumer):\nstream_received.timeseries.on_dataframe_received = on_dataframe_received_handler\ndef on_dataframe_received_handler(stream: StreamConsumer, df: pd.DataFrame):\nprint(df.to_string())\ntopic_consumer.on_stream_received = on_stream_received_handler\ntopic_consumer.subscribe()\n</code></pre> <p>Alternatively, you can always convert a TimeseriesData to a pandas DataFrame using the method <code>to_dataframe</code>:</p> <pre><code>from quixstreams import TopicConsumer, StreamConsumer, TimeseriesData\ndef on_stream_received_handler(stream_received: StreamConsumer):\nstream_received.timeseries.on_data_received = on_timeseries_data_received_handler\ndef on_timeseries_data_received_handler(stream: StreamConsumer, data: TimeseriesData):\nwith data:\n# consume from input stream\ndf = data.to_dataframe()\nprint(df.to_string())\ntopic_consumer.on_stream_received = on_stream_received_handler\ntopic_consumer.subscribe()\n</code></pre> <p>Tip</p> <p>The conversions from TimeseriesData to pandas DataFrame have an intrinsic cost overhead. For high-performance models using pandas DataFrame, you should use the <code>on_dataframe_received</code> callback provided by the library, which is optimized to do as few conversions as possible.</p>"},{"location":"client-library/subscribe/#using-a-buffer","title":"Using a Buffer","text":"<p>Quix Streams provides you with an optional programmable buffer which you can tailor to your needs. Using buffers to consume data allows you to process data in batches according to your needs. The buffer also helps you to develop models with a high-performance throughput.</p> PythonC# <p>You can use the <code>buffer</code> property embedded in the <code>timeseries</code> property of your <code>stream</code>, or create a separate instance of that buffer using the <code>create_buffer</code> method:</p> <pre><code>buffer = newStream.timeseries.create_buffer()\n</code></pre> <p>You can use the <code>Buffer</code> property embedded in the <code>Timeseries</code> property of your <code>stream</code>, or create a separate instance of that buffer using the <code>CreateBuffer</code> method:</p> <pre><code>var buffer = newStream.Timeseries.CreateBuffer();\n</code></pre> <p>You can configure a buffer\u2019s input requirements using built-in properties. For example, the following configuration means that the Buffer will release a packet when the time span between first and last timestamp inside the buffer reaches 100 milliseconds:</p> PythonC# <pre><code>buffer.time_span_in_milliseconds = 100\n</code></pre> <pre><code>buffer.TimeSpanInMilliseconds = 100;\n</code></pre> <p>Consuming data from that buffer is as simple as using its callback (Python) or events (C#). The buffer uses the same callbacks and events as when consuming without the buffer. For example, the following code prints the ParameterA value of the first timestamp of each packet released from the buffer:</p> PythonC# <pre><code>from quixstreams import TopicConsumer, StreamConsumer, TimeseriesData\ndef on_data_released_handler(stream: StreamConsumer, data: TimeseriesData):\nwith data:\ntimestamp = data.timestamps[0].timestamp\nnum_value = data.timestamps[0].parameters['ParameterA'].numeric_value\nprint(\"ParameterA - \" + str(timestamp) + \": \" + str(num_value))\nbuffer.on_data_released = on_data_released_handler\n# buffer.on_dataframe_released and other callbacks are also available, check consuming without buffer for more info\n</code></pre> <pre><code>buffer.OnDataReleased += (sender, args) =&gt;\n{\nvar timestamp = ags.Data.Timestamps[0].Timestamp;\nvar numValue = ags.Data.Timestamps[0].Parameters[\"ParameterA\"].NumericValue;\nConsole.WriteLine($\"ParameterA - {timestamp}: {numValue}\");\n};\n</code></pre> <p>You can configure multiple conditions to determine when the buffer has to release data, if any of these conditions become true, the buffer will release a new packet of data and that data is cleared from the buffer:</p> PythonC# <ul> <li><code>buffer.buffer_timeout</code>: The maximum duration in milliseconds for which the buffer will be held before releasing the data. A packet of data is released when the configured timeout value has elapsed from the last data received in the buffer.    </li> <li><code>buffer.packet_size</code>: The maximum packet size in terms of number of timestamps. Each time the buffer has this amount of timestamps, the packet of data is released.    </li> <li><code>buffer.time_span_in_nanoseconds</code>: The maximum time between timestamps in nanoseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released.    </li> <li><code>buffer.time_span_in_milliseconds</code>: The maximum time between timestamps in milliseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. Note: This is a millisecond converter on top of <code>time_span_in_nanoseconds</code>. They both work with the same underlying value.    </li> <li><code>buffer.custom_trigger_before_enqueue</code>: A custom function which is invoked before adding a new timestamp to the buffer. If it returns true, the packet of data is released before adding the timestamp to it.    </li> <li><code>buffer.custom_trigger</code>: A custom function which is invoked after adding a new timestamp to the buffer. If it returns true, the packet of data is released with the entire buffer content.    </li> <li><code>buffer.filter</code>: A custom function to filter the incoming data before adding it to the buffer. If it returns true, data is added, otherwise it isn\u2019t.</li> </ul> <ul> <li><code>Buffer.BufferTimeout</code>: The maximum duration in milliseconds for which the buffer will be held before releasing the data. A packet of data is released when the configured timeout value has elapsed from the last data received in the buffer.    </li> <li><code>Buffer.PacketSize</code>: The maximum packet size in terms of number of timestamps. Each time the buffer has this amount of timestamps, the packet of data is released.    </li> <li><code>Buffer.TimeSpanInNanoseconds</code>: The maximum time between timestamps in nanoseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released.    </li> <li><code>Buffer.TimeSpanInMilliseconds</code>: The maximum time between timestamps in milliseconds. When the difference between the earliest and latest buffered timestamp surpasses this number, the packet of data is released. Note: This is a millisecond converter on top of <code>TimeSpanInNanoseconds</code>. They both work with the same underlying value.    </li> <li><code>Buffer.CustomTriggerBeforeEnqueue</code>: A custom function which is invoked before adding a new timestamp to the buffer. If it returns true, the packet of data is released before adding the timestamp to it.    </li> <li><code>Buffer.CustomTrigger</code>: A custom function which is invoked after adding a new timestamp to the buffer. If it returns true, the packet of data is released with the entire buffer content.    </li> <li><code>Buffer.Filter</code>: A custom function to filter the incoming data before adding it to the buffer. If it returns true, data is added, otherwise it isn\u2019t.</li> </ul>"},{"location":"client-library/subscribe/#examples","title":"Examples","text":"<p>The following buffer configuration will send data every 100ms or, if no data is buffered in the 1 second timeout period, it will empty the buffer and send the pending data anyway:</p> PythonC# <pre><code>stream.timeseries.buffer.packet_size = 100\nstream.timeseries.buffer.buffer_timeout = 1000\n</code></pre> <pre><code>stream.Timeseries.Buffer.PacketSize = 100;\nstream.Timeseries.Buffer.BufferTimeout = 1000;\n</code></pre> <p>The following buffer configuration will send data every 100ms window or if critical data arrives:</p> PythonC# <pre><code>buffer.time_span_in_milliseconds = 100\nbuffer.custom_trigger = lambda data: data.timestamps[0].tags[\"is_critical\"] == 'True'\n</code></pre> <pre><code>stream.Timeseries.Buffer.TimeSpanInMilliseconds = 100;\nstream.Timeseries.Buffer.CustomTrigger = data =&gt; data.Timestamps[0].Tags[\"is_critical\"] == \"True\";\n</code></pre>"},{"location":"client-library/subscribe/#subscribing-to-events","title":"Subscribing to events","text":"<p><code>EventData</code> is the formal class in Quix Streams which represents an Event data packet in memory. <code>EventData</code> is meant to be used when the data is intended to be consumed only as single unit, such as json payload where properties can't be converted to individual parameters. EventData can also be better for non-standard changes such as a machine shutting down sending event named ShutDown.</p> <p>Tip</p> <p>If your data source generates data at regular time intervals, or the information can be organized in a fixed list of Parameters, the TimeseriesData format is a better fit for your time-series data.</p>"},{"location":"client-library/subscribe/#eventdata-format","title":"EventData format","text":"<p><code>EventData</code> consists of a record with a <code>Timestamp</code>, an <code>EventId</code> and an <code>EventValue</code>.</p> <p>You should imagine a list of <code>EventData</code> instances as a simple table of three columns where the <code>Timestamp</code> is the first column of that table and the <code>EventId</code> and <code>EventValue</code> are the second and third columns, as shown in the following table:</p> Timestamp EventId EventValue 1 failure23 Gearbox has a failure 2 box-event2 Car has entered to the box 3 motor-off Motor has stopped 6 race-event3 Race has finished <p>Consuming events from a stream is as easy as consuming timeseries data. In this case, the library does not use a buffer, but the way we consume Event Data from a stream is similar.</p> PythonC# <p>from quixstreams import TopicConsumer, StreamConsumer, EventData</p> <pre><code>def on_event_data_received_handler(stream: StreamConsumer, data: EventData):\nwith data:\nprint(\"Event consumed for stream. Event Id: \" + data.Id)\nstream_received.events.on_data_received = on_event_data_received_handler\n</code></pre> <pre><code>newStream.Events.OnDataReceived += (stream, args) =&gt;\n{\nConsole.WriteLine($\"Event received for stream. Event Id: {args.Data.Id}\");\n};\n</code></pre> <p>output:</p> <pre><code>Event consumed for stream. Event Id: failure23\nEvent consumed for stream. Event Id: box-event2\nEvent consumed for stream. Event Id: motor-off\nEvent consumed for stream. Event Id: race-event3\n</code></pre>"},{"location":"client-library/subscribe/#committing-checkpointing","title":"Committing / checkpointing","text":"<p>It is important to be aware of the commit concept when working with a broker. Committing allows you to mark how far data has been processed, also known as creating a checkpoint. In the event of a restart or rebalance, the client only processes messages from the last committed position. Commits are done for each consumer group, so if you have several consumer groups in use, they do not affect each another when committing.</p> <p>Tip</p> <p>Commits are done at a partition level when you use Kafka as a message broker. Streams that belong to the same partition are committed at the same time using the same position. Quix Streams currently does not expose the option to subscribe to only specific partitions of a topic, but commits will only ever affect partitions that are currently assigned to your client.</p> <p>Partitions and the Kafka rebalancing protocol are internal details of the Kafka implementation of Quix Streams. You can consume more about the Streaming Context feature of the library here.</p>"},{"location":"client-library/subscribe/#automatic-committing","title":"Automatic committing","text":"<p>By default, Quix Streams automatically commits processed messages at a regular default interval, which is every 5 seconds or 5,000 messages, whichever happens sooner. However this is subject to change.</p> <p>If you wish to use different automatic commit intervals, use the following code:</p> PythonC# <pre><code>from quixstreams import CommitOptions\ncommit_settings = CommitOptions()\ncommit_settings.commit_every = 100 # note, you can set this to None\ncommit_settings.commit_interval = 500 # note, you can set this to None\ncommit_settings.auto_commit_enabled = True\ntopic_consumer = client.get_topic_consumer('yourtopic', commit_settings=commit_settings)\n</code></pre> <pre><code>var topicConsumer = client.GetTopicConsumer(topic, consumerGroup, new CommitOptions()\n{\nCommitEvery = 100,\nCommitInterval = 500,\nAutoCommitEnabled = true // optional, defaults to true\n});\n</code></pre> <p>The code above will commit every 100 processed messages or 500 ms, whichever is sooner.</p>"},{"location":"client-library/subscribe/#manual-committing","title":"Manual committing","text":"<p>Some use cases need manual committing to mark completion of work, for example when you wish to batch process data, so the frequency of commit depends on the data. This can be achieved by first enabling manual commit for the topic:</p> PythonC# <pre><code>from quixstreams import CommitMode\ntopic_consumer = client.get_topic_consumer('yourtopic', commit_settings=CommitMode.Manual)\n</code></pre> <pre><code>client.GetTopicConsumer(topic, consumerGroup, CommitMode.Manual);\n</code></pre> <p>Then, whenever your commit condition fulfils, call:</p> PythonC# <pre><code>topic_consumer.commit()\n</code></pre> <pre><code>topicConsumer.Commit();\n</code></pre> <p>The piece of code above will commit anything \u2013 like parameter, event or metadata - consumed and served to you from the topic you subscribed to up to this point.</p>"},{"location":"client-library/subscribe/#commit-callback","title":"Commit callback","text":"Python <p>Whenever a commit occurs, a callback is raised to let you know. This callback is invoked for both manual and automatic commits. You can set the callback using the following code:</p> <pre><code>``` python\nfrom quixstreams import TopicConsumer\n\ndef on_committed_handler(topic_consumer: TopicConsumer):\n    # your code doing something when committed to broker\n\ntopic_consumer.on_committed = on_committed_handler\n```\n</code></pre> C# <p>Whenever a commit occurs, an event is raised to let you know. This event is raised for both manual and automatic commits. You can subscribe to this event using the following code:</p> <pre><code>``` cs\ntopicConsumer.OnCommitted += (sender, args) =&gt;\n{\n    //... your code \u2026\n};\n```\n</code></pre>"},{"location":"client-library/subscribe/#auto-offset-reset","title":"Auto offset reset","text":"<p>You can control the offset that data is received from by optionally specifying <code>AutoOffsetReset</code> when you open the topic.</p> <p>When setting the <code>AutoOffsetReset</code> you can specify one of three options:</p> Option Description Latest Receive only the latest data as it arrives, dont include older data Earliest Receive from the beginning, i.e. as much as possible Error Throws exception if no previous offset is found <p>By default, Latest is used.</p> PythonC# <pre><code>topic_consumer = client.get_topic_consumer(test_topic, auto_offset_reset=AutoOffsetReset.Latest)\nor\ntopic_consumer = client.get_topic_consumer(test_topic, auto_offset_reset=AutoOffsetReset.Earliest)\n</code></pre> <pre><code>var topicConsumer = client.GetTopicConsumer(\"MyTopic\", autoOffset: AutoOffsetReset.Latest);\nor\nvar topicConsumer = client.GetTopicConsumer(\"MyTopic\", autoOffset: AutoOffsetReset.Earliest);\n</code></pre>"},{"location":"client-library/subscribe/#revocation","title":"Revocation","text":"<p>When working with a broker, you have a certain number of topic streams assigned to your consumer. Over the course of the client\u2019s lifetime, there may be several events causing a stream to be revoked, like another client joining or leaving the consumer group, so your application should be prepared to handle these scenarios in order to avoid data loss and/or avoidable reprocessing of messages.</p> <p>Tip</p> <p>Kafka revokes entire partitions, but Quix Streams makes it easy to determine which streams are affected by providing two events you can listen to.</p> <p>Partitions and the Kafka rebalancing protocol are internal details of the Kafka implementation of Quix Streams. You mainly don\u2019t even need to worry about it because everything is abstracted within the Streaming Context feature of the library.</p>"},{"location":"client-library/subscribe/#streams-revoking","title":"Streams revoking","text":"<p>One or more streams are about to be revoked from your client, but you have a limited time frame \u2013 according to your broker configuration \u2013 to react to this and optionally commit to the broker:</p> PythonC# <pre><code>def on_revoking_handler(topic_consumer: TopicConsumer):\n# your code\ntopic_consumer.on_revoking = on_revoking_handler\n</code></pre> <pre><code>topicConsumer.OnRevoking += (sender, args) =&gt;\n{\n// ... your code ...\n};\n</code></pre>"},{"location":"client-library/subscribe/#streams-revoked","title":"Streams revoked","text":"<p>One or more streams are revoked from your client. You can no longer commit to these streams, you can only handle the revocation in your client.</p> PythonC# <pre><code>from quixstreams import StreamConsumer\ndef on_streams_revoked_handler(topic_consumer: TopicConsumer, streams: [StreamConsumer]):\nfor stream in streams:\nprint(\"Stream \" + stream.stream_id + \" got revoked\")\ntopic_consumer.on_streams_revoked = on_streams_revoked_handler\n</code></pre> <pre><code>topicConsumer.OnStreamsRevoked += (sender, revokedStreams) =&gt;\n{\n// revoked streams are provided to the handler\n};\n</code></pre>"},{"location":"client-library/subscribe/#stream-closure","title":"Stream closure","text":"Python <p>You can detect stream closure with the <code>on_stream_closed</code> callback which has the stream and the StreamEndType to help determine the closure reason if required.</p> <pre><code>``` python\ndef on_stream_closed_handler(stream: StreamConsumer, end_type: StreamEndType):\n        print(\"Stream closed with {}\".format(end_type))\n\nstream_received.on_stream_closed = on_stream_closed_handler\n```\n</code></pre> C# <p>You can detect stream closure with the stream closed event which has the sender and the StreamEndType to help determine the closure reason if required.</p> <pre><code>``` cs\ntopicConsumer.OnStreamReceived += (topic, streamConsumer) =&gt;\n{\n        streamConsumer.OnStreamClosed += (reader, args) =&gt;\n        {\n                Console.WriteLine(\"Stream closed with {0}\", args.EndType);\n        };\n};\n```\n</code></pre> <p>The <code>StreamEndType</code> can be one of:</p> StreamEndType Description Closed The stream was closed normally Aborted The stream was aborted by your code for your own reasons Terminated The stream was terminated unexpectedly while data was being written"},{"location":"client-library/subscribe/#minimal-example","title":"Minimal example","text":"<p>This is a minimal code example you can use to receive data from a topic using Quix Streams:</p> PythonC# <pre><code>from quixstreams import *\nclient = KafkaStreamingClient('127.0.0.1:9092')\ntopic_consumer = client.get_topic_consumer(TOPIC_ID)\n# Consume streams\ndef on_stream_received_handler(stream_received: StreamConsumer):    \nbuffer = stream_received.timeseries.create_buffer()\nbuffer.on_data_released = on_data_released_handler\ndef on_data_released_handler(stream: StreamConsumer, data: TimeseriesData):\nwith data:\ndf = data.to_dataframe()\nprint(df.to_string())    \n# Hook up events before subscribing to avoid losing out on any data\ntopic_consumer.on_stream_received = on_stream_received_handler\n# Hook up to termination signal (for docker image) and CTRL-C\nprint(\"Listening to streams. Press CTRL-C to exit.\")\n# Handle graceful exit\nApp.run()\n</code></pre> <p>Find out more about App.run()</p> <pre><code>using System;\nusing System.Linq;\nusing System.Threading;\nusing QuixStreams.Streaming;\nusing QuixStreams.Streaming.Configuration;\nusing QuixStreams.Streaming.Models;\nnamespace ReadHelloWorld\n{\nclass Program\n{\n/// &lt;summary&gt;\n/// Main will be invoked when you run the application\n/// &lt;/summary&gt;\nstatic void Main()\n{\n// Create a client which holds generic details for creating input and output topics\nvar client = new KafkaStreamingClient(\"127.0.0.1:9092\")\nusing var topicConsumer = client.GetTopicConsumer(TOPIC_ID);\n// Hook up events before subscribing to avoid losing out on any data\ntopicConsumer.OnStreamReceived += (topic, streamConsumer) =&gt;\n{\nConsole.WriteLine($\"New stream received: {streamConsumer.StreamId}\");\nvar buffer = streamConsumer.Timeseries.CreateBuffer();\nbuffer.OnDataReleased += (sender, args) =&gt;\n{\nConsole.WriteLine($\"ParameterA - {ags.Data.Timestamps[0].Timestamp}: {ags.Data.Timestamps.Average(a =&gt; a.Parameters[\"ParameterA\"].NumericValue)}\");\n};\n};\nConsole.WriteLine(\"Listening for streams\");\n// Hook up to termination signal (for docker image) and CTRL-C and open streams\nApp.Run();\nConsole.WriteLine(\"Exiting\");\n}\n}\n}\n</code></pre>"},{"location":"client-library/subscribe/#subscribe-raw-kafka-messages","title":"Subscribe raw Kafka messages","text":"<p>Quix Streams uses an internal protocol which is both data and speed optimized so we do encourage you to use it, but you need to use Quix Streams on both producer and consumer sides as of today. We have plans to support most common formats in near future, but custom formats will always need to be handled manually.</p> <p>For this, we have created a way to publish and subscribe to the raw, unformatted messages and work with them as bytes. This gives you the ability to implement the protocol as needed and convert between formats.</p> PythonC# <pre><code>from quixstreams import RawTopicConsumer, RawMessage\nraw_consumer = client.create_raw_topic_consumer(TOPIC_ID)\ndef on_message_received_handler(topic: RawTopicConsumer, msg: RawMessage):\n#bytearray containing bytes received from kafka\ndata = msg.value\n#broker metadata as dict\nmeta = msg.metadata\nraw_consumer.on_message_received = on_message_received_handler\nraw_consumer.subscribe()  # or use App.run()\n</code></pre> <pre><code>var rawConsumer = client.GetRawTopicConsumer(TOPIC_ID)\nrawConsumer.OnMessageRead += (sender, message) =&gt;\n{\nvar data = (byte[])message.Value;\n};\nrawConsumer.Subscribe()  // or use App.Run()\n</code></pre>"},{"location":"client-library/using/","title":"Using Quix Streams","text":"<p>In this topic you will learn how to use Quix Streams to perform two types of data processing:</p> <ol> <li>One message at a time processing - Here the message received contains all required data for processing. No state needs to be preserved between messages, or between replicas. The data from the message is used to calculate a new value, which is then typically published to the output stream.</li> <li>Stateful processing - This is where you need to keep track of data between messages, such as keeping a running total of a variable. This is more complicated as state needs to be preserved between messages, and potentially between replicas, where multiple replicas are deployed. In addition, state may need to be preserved in the event of the failure of a deployment - Quix Streams supports checkpointing as a way to enable this.</li> </ol> <p>The following sections will explore these methods of data processing in more detail.</p>"},{"location":"client-library/using/#topics-streams-partitions-replicas-and-consumer-groups","title":"Topics, streams, partitions, replicas, and consumer groups","text":"<p>The main structure used for data organization in Quix is the topic. For example, the topic might be <code>iot-telemetry</code>. To allow for horizontal scaling, a topic is typically divided into multiple streams. You may have multiple devices, or sources, writing data into a topic, so to ensure scaling and message ordering, each source writes into its own stream. Device 1 would write to stream 1, and device 2 to stream 2 and so on. This is the idea of stream context. </p> <p>Quix Streams ensures that stream context is preserved, that is, messages inside one stream are always published to the same single partition. This means that inside one stream, a consumer can rely on the order of messages. A partition can contain multiple streams, but a stream is always confined to one partition.</p> <p>It is possible to organize the code that processes the streams in a topic using the idea of a consumer group. This indicates to the broker that you will process the topic with all available replicas.</p> <p>Horizontal scaling occurs automatically, because when you deploy multiple replicas, a stream is assigned to a replica. For example, if there are three streams and three replicas, each replica will process a single stream. If you had only one replica, it would need to process all streams in that topic. If you have three streams and two replicas, one replica would process two streams, and the other replica a single stream.</p> <p>When you create the consumer you specify the consumer group as follows:</p> <pre><code>topic_consumer = client.get_topic_consumer(os.environ[\"input\"], consumer_group = \"empty-transformation\")\n</code></pre> <p>Note</p> <p>If you don't specify a consumer group, then all messages in all streams in a topic will be processed by all replicas in the microservice deployment.</p>"},{"location":"client-library/using/#stream-data-formats","title":"Stream data formats","text":"<p>There are two main formats of stream data: </p> <ol> <li>Event data - in Quix Streams this is represented with the <code>qx.EventData</code> class.</li> <li>Time-series - in Quix Streams this is represented with the <code>qx.TimeseriesData</code> class (and two other classes: one for Pandas data frame format, and one for raw Kafka data).</li> </ol> <p>Event data refers to data that is independent, whereas time-series data is a variable that changes over time. An example of event data is a financial transaction. It contains all data for the invoice, with a timestamp (the time of the transaction), but a financial transaction itself is not a variable you'd track over time. The invoice may itself contain time-series data though, such as the customer's account balance. </p> <p>Time-series data is a variable that is tracked over time, such as temperature from a sensor, or the g-forces in a racing car.</p> <p>Time-series data has three formats in Quix Streams:</p> <ol> <li>Data (represented by the <code>qx.TimeseriesData</code> class)</li> <li>Pandas Data Frame (represented by the <code>pd.DataFrame</code> class)</li> <li>DataRaw (represented by the <code>qx.TimeseriesDataRaw</code> class)</li> </ol> <p>In this topic you'll learn about the <code>TimeseriesData</code> and <code>pd.DataFrame</code> formats.</p>"},{"location":"client-library/using/#registering-a-callback-for-stream-data","title":"Registering a callback for stream data","text":"<p>You can register a stream callback that is invoked when data is first received on a stream. </p> <pre><code>topic_consumer.on_stream_received = on_stream_received_handler\n</code></pre> <p>The <code>on_stream_received_handler</code> is typically written to handle a specific data format on that stream. This is explained in the next section.</p> <p>Note</p> <p>This callback is invoked for each stream in a topic. This means you will have multiple instances of this callback invoked, if there are multiple streams. </p>"},{"location":"client-library/using/#registering-callbacks-to-handle-data-formats","title":"Registering callbacks to handle data formats","text":"<p>Specific callbacks are registered to handle each type of stream data.</p> <p>The following table documents which callbacks to register, depending on the type of stream data you need to handle:</p> Stream data format Callback to register Event data <code>stream_consumer.events.on_data_received = on_event_data_received_handler</code> Time-series data <code>stream_consumer.timeseries.on_data_received = on_data_received_handler</code> Time-series raw data <code>stream_consumer.timeseries.on_raw_received = on_raw_received_handler</code> Time-series data frame <code>stream_consumer.timeseries.on_dataframe_received = on_dataframe_received_handler</code> <p>Note</p> <p>You can have multiple callbacks registered at the same time, but usually you would work with the data format most suited to your use case. For example, if the source was providing only event data, it only makes sense to register the event data callback.</p>"},{"location":"client-library/using/#example-of-callback-registration","title":"Example of callback registration","text":"<p>The following code sample demonstrates how to register a callback to handle data in the data frame format: </p> <pre><code>def on_stream_received_handler(stream_consumer: qx.StreamConsumer):\nstream_consumer.timeseries.on_dataframe_received = on_dataframe_received_handler\n# subscribe to new streams being received. \n# callback will only be registered for an active stream\ntopic_consumer.on_stream_received = on_stream_received_handler\n</code></pre> <p>In this example, when a stream becomes active, it registers a callback to handle time-series data in the data frame format.</p> <p>Note</p> <p>The callback is registered only for the specified stream, and only if that stream is active.</p>"},{"location":"client-library/using/#converting-time-series-data","title":"Converting time-series data","text":"<p>Sometimes you need to convert time-series data into Panda data frames format for processing. That can be done using <code>to_dataframe</code>:</p> <pre><code>df = ts.to_dataframe()\n</code></pre>"},{"location":"client-library/using/#one-message-at-a-time-processing","title":"\"One message at a time\" processing","text":"<p>Now that you have learned about stream data formats and callbacks, the following example shows a simple data processor. </p> <p>This processor receives (consumes) data, processes it (transforms), and then publishes generated data (produces) on an output topic. This encapsulates the typical processing pipeline which consists of:</p> <ol> <li>Consumer (reads data)</li> <li>Transformer (processes data)</li> <li>Producer (writes data)</li> </ol> <p>The example code demonstrates this:</p> <pre><code>import quixstreams as qx\nimport pandas as pd\nclient = qx.KafkaStreamingClient('127.0.0.1:9092')\nprint(\"Opening consumer and producer topics\")\ntopic_consumer = client.get_topic_consumer(\"quickstart-topic\")\ntopic_producer = client.get_topic_producer(\"output-topic\")\ndef on_dataframe_received_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\nprint(df)\n# Calculate gForceTotal, the sum of vector absolute values \ndf[\"gForceTotal\"] = df[\"gForceX\"].abs() + df[\"gForceY\"].abs() + df[\"gForceZ\"].abs() \n# write result data to output topic\ntopic_producer.get_or_create_stream(stream_consumer.stream_id).timeseries.publish(df)\n# read streams\ndef on_stream_received_handler(stream_consumer: qx.StreamConsumer):\nstream_consumer.timeseries.on_dataframe_received = on_dataframe_received_handler\ntopic_consumer.on_stream_received = on_stream_received_handler\n# Hook up to termination signal (for docker image) and CTRL-C\nprint(\"Listening to streams. Press CTRL-C to exit.\")\n# Handle graceful exit\nqx.App.run()\n</code></pre> <p>In this example the stream data is inbound in Pandas <code>DataFrame</code> format. </p> <p>Note that all information required to calculate <code>gForceTotal</code> is contained in the inbound data frame (the X, Y, and Z components of g-force). This is an example of \"one message at a time\" processing: no state needs to be preserved between messages. </p> <p>Further, if multiple replicas were used here, it would require no changes to your code, as each replica, running its own instance of the callback for the target stream, would simply calculate a value for <code>gForceTotal</code> based on the data in the data frame it received.</p>"},{"location":"client-library/using/#stateful-processing","title":"Stateful processing","text":"<p>With stateful processing, additional complexity is introduced, as data now needs to be preserved between messages, streams, and potentially replicas (where multiple replicas are deployed to handle multiple streams). </p>"},{"location":"client-library/using/#the-problem-of-using-global-variables-to-track-state","title":"The problem of using global variables to track state","text":"<p>There are problems with using global variables in your code to track state. The first is that callbacks are registered per-stream. This means that if you modify a global variable in a callback, it will be modified by all streams. </p> <p>For example, consider the following problematic code:</p> <pre><code>...\ngForceRunningTotal = 0.0\ndef on_dataframe_received_handler(stream_consumer: qx.StreamConsumer, df: pd.DataFrame):\nprint(df)\n# Calculate gForceTotal, the sum of vector absolute values \ndf[\"gForceTotal\"] = df[\"gForceX\"].abs() + df[\"gForceY\"].abs() + df[\"gForceZ\"].abs() \n# Track running total of all g-forces\nglobal gForceRunningTotal\ngForceRunningTotal += df[\"gForceTotal\"]\n# write result data to output topic\ntopic_producer.get_or_create_stream(stream_consumer.stream_id).timeseries.publish(df)\n...\n</code></pre> <p>You might think this would give you the running total for a stream, but because the callback is registered for each stream, you'd actually get all streams modifying the global.</p> <p>If you were running across multiple replicas, you'd get a running total for each replica, because each replica would have its own instance of the global variable. Again, the results would not be as you might expect.</p> <p>Let's say there were three streams and two replicas, you'd get the running total of two streams for one replica, and the running total for the third stream in the other replica.</p> <p>In most practical scenarios you'd want to track a running total per stream (say, total g-forces per race car), or perhaps for some variables a running total across all streams. Each of these scenarios is described in the followng sections.</p>"},{"location":"client-library/using/#tracking-running-totals-per-stream","title":"Tracking running totals per stream","text":"<p>Sometimes you might want to calculate a running total of a variable for a stream. For example, the total g-force a racing car is exposed to. If you use a global variable you'll lose the stream context. All streams will add to the value potentially, and each replica will also have its own instance of the global, further confusing matters.</p> <p>The solution is to use the stream context to preserve a running total for that stream only. To do this you can use the <code>stream_id</code> of a stream to identify its data. Consider the following example:</p> <pre><code>...\ng_running_total_per_stream = {}\ndef callback_handler (stream_consumer: qx.StreamConsumer, data: qx.TimeseriesData):\nif stream_consumer.stream_id not in g_running_total_per_stream:\ng_running_total_per_stream[stream_consumer.stream_id] = 0\n...\ng_running_total_per_stream[stream_consumer.stream_id] += some_value\n...\n</code></pre> <p>The key point here is that data is tracked per stream context. You keep running totals on a per-stream basis by using the stream ID, <code>stream_consumer.stream_id</code> to index a dictionary containing running totals for each stream.</p> <p>Note</p> <p>A stream will only ever be processed by one replica.</p>"},{"location":"client-library/using/#tracking-running-totals-across-multiple-streams","title":"Tracking running totals across multiple streams","text":"<p>Sometimes you want to track a running total across all streams in a topic. The problem is that when you scale using replicas, there is no way to share data between all replicas in a consumer group. </p> <p>The solution is to write the running total per stream (with stream ID) to an output topic. You can then have another processor in the pipeline to calculate total values from inbound messages.</p> <pre><code>...\ng_running_total_per_stream = {}\ndef callback_handler (stream_consumer: qx.StreamConsumer, data: qx.TimeseriesData):\nif stream_consumer.stream_id not in g_running_total_per_stream:\ng_running_total_per_stream[stream_consumer.stream_id] = 0\n...\ng_running_total_per_stream[stream_consumer.stream_id] += some_value\ndata.add_value(\"RunningTotal\", g_running_total_per_stream[stream_consumer.stream_id])\ntopic_producer.get_or_create_stream(stream_consumer.stream_id).timeseries.publish(data)\n...\n</code></pre> <p>In this case the running total is published to its own stream in the output topic. The next service in the data processing pipeline would be able to sum all running totals across all streams in the output topic.</p>"},{"location":"client-library/using/#handling-system-restarts-and-crashes","title":"Handling system restarts and crashes","text":"<p>One issue you may run into is that in-memory data is not persisted across instance restarts, shutdowns, and instance crashes. This can be mitigated by using the Quix Streams <code>LocalFileStorage</code> facility. This will ensure that specified variables are persisted on permanent storage, and this data is preserved across restarts, shutdowns, and system crashes.</p> <p>The following example code demonstrates a simple use of <code>LocalFile Storage</code>:</p> <pre><code>my_var = qx.InMemoryStorage(qx.LocalFileStorage())\n...\ntopic_consumer.on_stream_received = on_stream_received_handler\ntopic_consumer.on_committed = my_var.flush\n...\n</code></pre> <p>This ensures that the variable <code>my_var</code> is persisted, as periodically (default is 20 seconds) it is flushed to local file storage.</p> <p>If the system crashes (or is restarted), Kafka resumes message processing from the last committed message. This facility is built into Kafka.</p> <p>Tip</p> <p>For this facility to work in Quix Platform you need to enable the State Management feature. You can enable it in the <code>Deployment</code> dialog, where you can also specify the size of storage required. When using Quix Streams with a third-party broker such as Kafka, no configuration is required, and data is stored on the local file system.</p>"},{"location":"client-library/using/#conclusion","title":"Conclusion","text":"<p>In this topic you have learned:</p> <ul> <li>How to perform simple \"one message at a time\" processing.</li> <li>How to handle the situation where state needs to be preserved, and problems that can arise in naive code.</li> <li>How to persist state, so that your data is preserved in the event of restarts or crashes.</li> </ul>"},{"location":"client-library/using/#next-steps","title":"Next steps","text":"<p>Continue your Quix Streams learning journey by reading the following more in-depth topics:</p> <ul> <li>Publishing data</li> <li>Subscribing to data</li> <li>Processing data</li> <li>State management</li> </ul>"},{"location":"client-library/versionmigration/","title":"Migrating from previous versions","text":"<p>Our goal is to minimize the occurrence of breaking changes, and if we do need to make them, we'll do so simultaneously. In cases where such changes are necessary, we'll provide migration steps from previous versions to assist in the transition. To prevent undue verbosity, we'll only show one difference unless it's language-specific, such as naming conventions (casing vs underscore).</p>"},{"location":"client-library/versionmigration/#04-050","title":"0.4.* -&gt; 0.5.0","text":""},{"location":"client-library/versionmigration/#the-library-is-renamed","title":"The library is renamed","text":"<p>For Python, the library is renamed to <code>quixstreams</code> from <code>quixstreaming</code>, while for C# the packages will be available under <code>QuixStreams.*</code> rather than <code>Quix.Sdk.*</code>. The latter also resulted in namespace changes.</p>"},{"location":"client-library/versionmigration/#library-availability","title":"Library availability","text":"<p>Previously, the library was not open source and was distributed via our public feed.</p> <p>For Python it was done by using:</p> <pre><code>pip install quixstreaming --extra-index-url https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/pypi/simple/\n</code></pre> <p>Now it should be installed from the official PyPi feed using:</p> <pre><code>pip install quixstreams\n</code></pre> <p>Quix currently publishes in-development versions to test PyPi, you can try these using:</p> <pre><code>pip install quixstreams --extra-index-url https://test.pypi.org/simple/\n</code></pre> <p>Note: The original feed will be maintained for some time, but should be treated as deprecated.</p> <p>We are currently using our feed for C#, but we're in the process of developing our public NuGet packages, which will be made available shortly.</p> <pre><code>https://pkgs.dev.azure.com/quix-analytics/53f7fe95-59fe-4307-b479-2473b96de6d1/_packaging/public/nuget/v3/index.json\n</code></pre>"},{"location":"client-library/versionmigration/#streamingclient-renamed-to-kafkastreamingclient","title":"StreamingClient renamed to KafkaStreamingClient","text":"<p>We renamed the <code>StreamingClient</code> to be more specific to the technology it works with.</p>"},{"location":"client-library/versionmigration/#outputtopic-and-inputtopic-classes-renamed-to-topicproducer-and-topicconsumer","title":"OutputTopic and InputTopic classes renamed to TopicProducer and TopicConsumer","text":"<p>This also brought several other changes to the code, see them below:</p> Python beforePython after <pre><code>output_topic = client.open_output_topic(TOPIC)\ninput_topic = client.open_input_topic(TOPIC)\n# !!! There was a significant modification made to `open_input_topic`,\n# which changed its default behavior from using a consumer group named 'Default'\n# with the `Earliest` offset to no consumer group and `Latest` offset.\n</code></pre> <pre><code>topic_producer = client.get_topic_producer(TOPIC)\ntopic_consumer = client.get_topic_consumer(TOPIC)\n# !!! There was a significant modification made to `get_topic_consumer`,\n# which changed its default behavior from using a consumer group named 'Default'\n# with the `Earliest` offset to no consumer group and `Latest` offset.\n</code></pre>"},{"location":"client-library/versionmigration/#readers-and-writers-renamed-to-consumers-and-producers","title":"Readers and Writers renamed to Consumers and Producers","text":"<p>The modifications will have the most significant impact on Python code that includes type hints in functions or C# code that subscribes events using a method with a particular signature rather than a lambda expression. The alterations are as follows:</p> <ul> <li><code>StreamReader|Writer</code> -&gt; <code>StreamConsumer|Producer</code></li> <li><code>StreamPropertiesReader|Writer</code> -&gt; <code>StreamPropertiesConsumer|Producer</code></li> <li><code>StreamParametersReader|writer</code> -&gt; <code>StreamTimeseriesConsumer|Producer</code>  (see section below about <code>Parameters</code>-&gt;<code>TimeSeries</code> rename)</li> <li><code>StreamEventsReader|Writer</code> -&gt; <code>StreamEventsConsumer|Producer</code></li> <li><code>ParametersBufferReader|Writer</code> -&gt; <code>TimeseriesConsumer|Producer</code> (see section below about <code>Parameters</code>-&gt;<code>TimeSeries</code> rename)</li> </ul>"},{"location":"client-library/versionmigration/#parameterdata-renamed-to-timeseriesdata","title":"ParameterData renamed to TimeseriesData","text":"<p>The class itself is renamed, see below for changes:</p> Python beforePython after <pre><code>from quixstreams import ParameterData\ndata = ParameterData()\ndata.add_timestamp_nanoseconds(1) \\\n    .add_value(\"Speed\", 120) \\\n    .add_value(\"Gear\", 3)\n</code></pre> <pre><code>from quixstreams import TimeseriesData\ndata = TimeseriesData()\ndata.add_timestamp_nanoseconds(1) \\\n    .add_value(\"Speed\", 120) \\\n    .add_value(\"Gear\", 3)\n</code></pre> <p>And the property on streams is also renamed:</p> Python beforePython after <pre><code>stream.parameters.\u2026\n</code></pre> <pre><code>stream.timeseries.\u2026\n</code></pre>"},{"location":"client-library/versionmigration/#pandas-dataframe-changes","title":"pandas DataFrame changes","text":"<p>All pandas DataFrames provided to you by callbacks or methods will expose the timestamp as 'timestamp' instead of 'time'.</p> <p>In addition <code>from|to_panda_frame</code> has been renamed to <code>from|to_dataframe</code>:</p> Python beforePython after <pre><code>data = ParameterData()\ndata.add_timestamp_nanoseconds(1) \\\n    .add_value(\"Speed\", 120) \\\n    .add_value(\"Gear\", 3)\ndf = data.to_panda_frame()\ntime_col = df['time']\n</code></pre> <pre><code>data = TimeseriesData()\ndata.add_timestamp_nanoseconds(1) \\\n    .add_value(\"Speed\", 120) \\\n    .add_value(\"Gear\", 3)\ndf = data.to_dataframe()\ntimestamp_col = df['timestamp']\n</code></pre>"},{"location":"client-library/versionmigration/#write-renamed-to-publish-to-be-in-sync-with-producer","title":".Write renamed to .Publish to be in sync with Producer","text":"Python beforePython after <pre><code>stream.parameters.write(\u2026)\nstream.parameters.buffer \\\n    .add_timestamp(datetime.utcnow()) \\\n    .add_value(\"parameter\", 10) \\\n    .write()\nstream.events.write(\u2026)\nraw_output_topic.write(\u2026)\n</code></pre> <pre><code>stream.timeseries.publish(\u2026)\nstream.timeseries.buffer \\\n    .add_timestamp(datetime.utcnow()) \\\n    .add_value(\"parameter\", 10) \\\n    .publish()\nstream.events.publish(\u2026)\nraw_topic_producer.publish(\u2026)\n</code></pre>"},{"location":"client-library/versionmigration/#startreading-renamed-to-subscribe-to-be-in-sync-with-producer","title":".StartReading renamed to .Subscribe to be in sync with Producer","text":"Python beforePython after <pre><code>input_topic.start_reading()\n</code></pre> <pre><code>topic_consumer.subscribe()\n</code></pre>"},{"location":"client-library/versionmigration/#event-changes","title":"Event changes","text":"<p>Certain callbacks have altered signatures, either in the name or number of arguments. In the case of C#, detecting these changes will be straightforward, and thus, the specifics will be omitted.</p> <p>Furthermore, in Python, event subscriptions (+=, -=) have been replaced with callback assignments.</p> Python beforePython after <pre><code>\u2026 the rest of your code, such as client and input/output creation\ndef on_stream_received_handler(stream_read : StreamReader):\nbuffer = stream_read.parameters.create_buffer() # or stream_read.parameters.buffer if don't want separate buffer with different filters and buffer condition\n# data event subscriptions\ndef on_parameter_pandas_dataframe_handler(data: pandas.DataFrame):\npass\nstream_read.parameters.on_read_pandas += on_parameters_pandas_dataframe_handler\nbuffer.on_read_pandas += on_parameters_pandas_dataframe_handler\ndef on_parameter_data_handler(data: ParameterData):\npass\nstream_read.parameters.on_read += on_parameter_data_handler\nbuffer.on_read += on_parameter_data_handler\ndef on_parameter_raw_data_handler(data: ParameterDataRaw):\npass\nstream_read.parameters.on_read_raw += on_parameter_raw_data_handler\nbuffer.on_read_raw += on_parameter_raw_data_handler\ndef on_event_data_handler(data: EventData):\npass\nnew_stream.events.on_read += on_event_data_handler\n# metadata event subscriptions\ndef on_stream_closed_handler(end_type: StreamEndType):\npass \nstream_read.on_stream_closed += on_stream_closed_handler\ndef on_stream_properties_changed_handler():\npass\nstream_read.properties.on_changed += on_stream_properties_changed_handler\ndef on_parameter_definitions_changed_handler():\npass\nnew_stream.parameters.on_definitions_changed += on_parameter_definitions_changed_handler\ndef on_event_definitions_changed_handler():\npass\nnew_stream.events.on_definitions_changed += on_event_definitions_changed_handler\ndef on_package_received_handler(stream: StreamReader, package: StreamPackage):\npass\nnew_stream.on_package_received += on_package_received_handler\ninput_topic.on_stream_received += on_stream_received_handler\n\u2026 the rest of your code\n</code></pre> <pre><code>\u2026 the rest of your code, such as client and consumer/producer creation\n# Please note that in the new version, you'll have access to all the required scopes within the callback,\n# eliminating the need to rely on the on_stream_received_handler's scope. This makes it simpler to define\n# callbacks in other locations. Another example will be provided in a separate section, but here we'll\n# maintain the previous structure for easier comprehension of the changes.\ndef on_stream_received_handler(stream_received : StreamConsumer):\nbuffer = stream_received.timeseries.create_buffer() # or stream_received.timeseries.buffer if don't want separate buffer with different filters and buffer condition\n# data callback assignments\ndef on_dataframe_received_handler(stream: StreamConsumer, data: pandas.DataFrame):  # Note the stream being available\npass\nstream_received.timeseries.on_dataframe_received = on_dataframe_received_handler  # note the rename and it is no longer +=\nbuffer.on_dataframe_released = on_dataframe_received_handler  # note the rename and it is no longer +=\ndef on_data_releasedorreceived_handler(stream: StreamConsumer, data: TimeseriesData):  # Note the stream being available\npass\nstream_received.timeseries.on_data_received = on_data_releasedorreceived_handler  # note the rename and it is no longer +=\nbuffer.on_data_released = on_data_releasedorreceived_handler  # note the rename and it is no longer +=\ndef on_rawdata_releasedorreceived_handler(stream: StreamConsumer, data: TimeseriesDataRaw):  # Note the stream being available\npass\nstream_received.timeseries.on_raw_received = on_rawdata_releasedorreceived_handler  # note the rename and it is no longer +=\nbuffer.on_raw_released = on_rawdata_releasedorreceived_handler  # note the rename and it is no longer +=\ndef on_event_data_handler(stream: StreamConsumer, data: EventData):  # Note the stream being available\npass\nnew_stream.events.on_data_received = on_event_data_handler  # note the rename and it is no longer +=\n# metadata callback assignments\ndef on_stream_closed_handler(stream: StreamConsumer, end_type: StreamEndType):  # Note the stream being available\npass \nstream_received.on_stream_closed = on_stream_closed_handler  # note it is no longer +=\ndef on_stream_properties_changed_handler(stream: StreamConsumer):  # Note the stream being available\npass\nstream_received.properties.on_changed = on_stream_properties_changed_handler  # note it is no longer +=\ndef on_parameter_definitions_changed_handler(stream: StreamConsumer):  # Note the stream being available\npass\nnew_stream.timeseries.on_definitions_changed = on_parameter_definitions_changed_handler  # note it is no longer +=\ndef on_event_definitions_changed_handler(stream: StreamConsumer):  # Note the stream being available\npass\nnew_stream.events.on_definitions_changed = on_event_definitions_changed_handler  # note it is no longer +=\ndef on_package_received_handler(stream: StreamConsumer, package: StreamPackage):\npass\nnew_stream.on_package_received = on_package_received_handler  # note it is no longer +=\ninput_topic.on_stream_received = on_stream_received_handler  # note it is no longer +=\n\u2026 the rest of your code\n</code></pre>"},{"location":"client-library/versionmigration/#in-python-topic-is-now-available-for-the-stream","title":"In Python topic is now available for the stream","text":"<p>This, paired with the event changes (read above), enables you improve your callback setup. The code above can now be expressed as follows:</p> <pre><code>\u2026 the rest of your code, such as client and consumer/producer creation\ndef on_stream_received_handler(stream_received : StreamConsumer):\nbuffer = stream_received.timeseries.create_buffer() # or stream_received.timeseries.buffer if don't want separate buffer with different filters and buffer condition\n# data callback assignments.\nstream_received.timeseries.on_dataframe_received = on_dataframe_received_handler\nbuffer.on_dataframe_released = on_dataframe_received_handler\nstream_received.timeseries.on_data_received = on_data_releasedorreceived_handler\nbuffer.on_data_released = on_data_releasedorreceived_handler\nstream_received.timeseries.on_raw_received = on_rawdata_releasedorreceived_handler\nbuffer.on_raw_released = on_rawdata_releasedorreceived_handler\nnew_stream.events.on_data_received = on_event_data_handler\n# metadata callback assignments\nstream_received.on_stream_closed = on_stream_closed_handler\nstream_received.properties.on_changed = on_stream_properties_changed_handler\nnew_stream.timeseries.on_definitions_changed = on_parameter_definitions_changed_handler\nnew_stream.events.on_definitions_changed = on_event_definitions_changed_handler\nnew_stream.on_package_received = on_package_received_handler\ninput_topic.on_stream_received = on_stream_received_handler\n# Note that these could be in a different file completely, defined by other classes, having access to all context of the stream and topic it is for\ndef on_dataframe_received_handler(stream: StreamConsumer, data: pandas.DataFrame):\n# do great things\nstream.topic.commit() # or any other topic method/property\ndef on_data_releasedorreceived_handler(stream: StreamConsumer, data: TimeseriesData):\npass\ndef on_rawdata_releasedorreceived_handler(stream: StreamConsumer, data: TimeseriesDataRaw):\npass\ndef on_event_data_handler(stream: StreamConsumer, data: EventData):\npass\ndef on_stream_closed_handler(stream: StreamConsumer, end_type: StreamEndType):\npass \ndef on_stream_properties_changed_handler(stream: StreamConsumer):\npass\ndef on_parameter_definitions_changed_handler(stream: StreamConsumer):\npass\ndef on_event_definitions_changed_handler(stream: StreamConsumer):\npass\ndef on_package_received_handler(stream: StreamConsumer, package: StreamPackage):\npass\n\u2026 the rest of your code\n</code></pre>"},{"location":"client-library/versionmigration/#with-statement-should-be-used-with-some-classes-in-python","title":"'with' statement should be used with some classes in python","text":"<p>Certain classes now use unmanaged resources, and to prevent memory leaks, we have incorporated the Python 'with' syntax for resource management.</p> <p>These are:</p> <ul> <li><code>EventData</code>: important to be disposed whenever manually created or received in callbacks.</li> <li><code>TimeseriesData</code>: important to be disposed whenever manually created or received in callbacks.</li> <li><code>TimeseriesDataRaw</code>: important to be disposed whenever manually created or received in callbacks.</li> <li><code>StreamPackage</code>: important to be disposed whenever manually created or received in callbacks.</li> <li><code>StreamConsumer</code>: also supports <code>dispose()</code> and automatically disposes when stream is closed.</li> <li><code>StreamProducer</code>: also supports <code>dispose()</code> and automatically disposes when stream is closed.</li> <li><code>TopicConsumer</code>: unless you're frequently subscribing to topics, this is not something you have to be too concerned about.</li> <li><code>TopicProducer</code>: unless you're frequently subscribing to topics, this is not something you have to be too concerned about.</li> </ul> <p>Example code:</p> <pre><code>\u2026 the rest of your code, such as client and consumer/producer creation\ndef on_stream_received_handler(stream_received : StreamConsumer):\n# data callback assignments.\nstream_received.timeseries.on_dataframe_received = on_dataframe_received_handler\nstream_received.timeseries.on_data_received = on_data_releasedorreceived_handler\nstream_received.timeseries.on_raw_received = on_rawdata_releasedorreceived_handler\nnew_stream.events.on_data_received = on_event_data_handler\n# metadata callback assignments\nstream_received.on_stream_closed = on_stream_closed_handler\nstream_received.properties.on_changed = on_stream_properties_changed_handler\nnew_stream.timeseries.on_definitions_changed = on_parameter_definitions_changed_handler\nnew_stream.events.on_definitions_changed = on_event_definitions_changed_handler\nnew_stream.on_package_received = on_package_received_handler\ninput_topic.on_stream_received = on_stream_received_handler\n# Please note that these could be defined in a separate file by other classes with access to the context of the stream and the associated topic.\ndef on_dataframe_received_handler(stream: StreamConsumer, data: pandas.DataFrame):\npfdata = TimeseriesData.from_panda_dataframe(data)\nwith pfdata:  # should be used because TimeseriesData needs it\npass\ndef on_data_releasedorreceived_handler(stream: StreamConsumer, data: TimeseriesData):\nwith data:\npass\ndef on_rawdata_releasedorreceived_handler(stream: StreamConsumer, data: TimeseriesDataRaw):\nwith data:\npass\ndef on_event_data_handler(stream: StreamConsumer, data: EventData):\nwith data:\npass\n\u2026 the rest of your code\n</code></pre>"},{"location":"client-library/features/builtin-buffers/","title":"Built-in buffers","text":"<p>If you\u2019re sending data at a high frequency, processing time-series without any buffering involved can be very costly. With smaller messages, data compression cannot be implemented as efficiently. The time spent in serializing and deserializing messages can be improved by sending more values per message.</p> <p>On the other hand, an incorrect buffer strategy can introduce unnecessary latency which may not be acceptable in some use cases.</p> <p>Quix Streams provides you with a very high performance, low-latency buffer, combined with easy-to-use configurations to subscribe and publish to give you freedom in balancing between latency and cost.</p> <p>Buffers in the library work at the timestamp level. A buffer accumulates timestamps until a release condition is met. A packet is then published containing those timestamps and values as a TimeseriesData package.</p> <p></p> <p>The logic is simple in theory but gets more complicated when trying to maintain high performance and easy interface. The buffer can be used to subscribe to and publish time-series data.</p> <p>Our buffer implementation uses short memory allocations and minimizes conversions between raw transport packages and TimeseriesData format to achieve low CPU and memory consumption with high throughput. We are happy to claim that our implementation has all of these, \u2014 simplicity, low resource consumption, and high performance \u2014, therefore you don\u2019t need to implement buffering because it is provided in the library.</p>"},{"location":"client-library/features/checkpointing/","title":"Checkpointing","text":"<p>Quix Streams allows you to do manual checkpointing when you read data from a Topic. This gives you the ability to inform the message broker that you have already processed messages up to a point, usually called a checkpoint.</p> <p>This is a very important concept when you are developing high-performance, streaming applications, processing lots of data in memory. You don\u2019t want to persist a state for each message received because it would cause an unaffordable processing cost, slowing down your streaming speeds and performance.</p> <p>Checkpointing lets you do some of this costly processing at a lower frequency, without having to worry about losing data. If, for some reason, your process is restarted or crashes and you haven\u2019t saved all the in-memory data you are processing, the message broker will resend all the messages from the last Checkpoint when you reconnect to the topic.</p> <p>Refer to the Committing / checkpointing section of this documentation to find out how to do Checkpointing when reading data with the library.</p>"},{"location":"client-library/features/checkpointing/#checkpointing-example","title":"Checkpointing example","text":"<p>Let\u2019s explain the checkpointing concept and its benefits with an easy example.</p> <p>One process is reading and processing data, without saving its state after each message received. This allows good performance and high throughtput of the service but, without checkpointing, risks data loss in the case of failure.</p> <ol> <li> <p>The process reads the first four messages, keeping its state in memory.</p> <p></p> </li> <li> <p>The process commits the messages of the topic (checkpointing) just after reading the first four and saves the in-memory state to the database.</p> <p></p> </li> <li> <p>The process reads the next four messages, but it crashes just after that, without time to commit the messages. This will not result in data loss because it will begin from the last checkpoint after it restarts.</p> <p></p> </li> <li> <p>The process restarts and reopens the input topic. It will start reading messages from the last checkpoint resulting in no data loss from the previous crash.</p> <p></p> </li> <li> <p>The process resumes reading the next five messages, keeping its state in memory.</p> <p></p> </li> <li> <p>The process commits the messages of the topic just after reading the previous five messages and saves the in-memory state to the database.</p> <p></p> </li> </ol>"},{"location":"client-library/features/data-frames/","title":"Support for Data Frames","text":"<p>Quix Streams supports pandas DataFrame for subscribing and publishing to topics. The library still uses the common TimeseriesData internally, but handles the conversion seamlessly for you.</p> <p>For example, the following TimeseriesData:</p> Timestamp CarId (tag) Speed Gear 1 car-1 120 3 1 car-2 123 3 3 car-1 125 3 6 car-2 110 2 <p>Is represented as the following pandas DataFrame:</p> time TAG__CarId Speed Gear 1 car-1 120 3 1 car-2 123 3 3 car-1 125 3 6 car-2 110 2 <p>Quix Streams provides multiple methods and events that work directly with pandas DataFrame.</p> <p>Please refer to the sections Using Data Frames for subscribing and Using Data Frames for publishing for extended information.</p>"},{"location":"client-library/features/data-serialization/","title":"Data serialization","text":"<p>Serialization can be difficult, especially if it\u2019s done with performance in mind. We serialize and deserialize native TimeseriesData transport objects, created specifically to be efficient with time series data. On top of that we use codecs like Protobuf that improve overall performance of the serialization and deserialization process by some orders of magnitude.</p> <p></p> <p>Quix Streams automatically serializes data from native types in your language. You can work with familiar types, such as pandas DataFrame, or use our own TimeseriesData without worrying about the conversions that are done for you by the library.</p>"},{"location":"client-library/features/horizontal-scaling/","title":"Horizontal scaling","text":"<p>Quix Streams provides out of box horizontal scaling using streaming context for automatic partitioning together with the underlying broker technology, such as kafka.</p> <p>Imagine the following example:</p> <p></p> <p>Each car produces one stream with its own time-series data, and each stream is processed by a replica of the deployment, labelled \"Process\". By default the message broker will assign each stream to one replica via the RangeAssignor strategy.</p> <p>When the purple replicas crashes \"stream 4\" is assigned automatically to the blue replica.</p> <p></p> <p>This situation will trigger an event on topic consumer in the blue replica indicating that \"stream 4\" has been received:</p> PythonC# <pre><code>def on_stream_received_handler(topic_consumer: TopicConsumer, stream_received: StreamConsumer):\nprint(\"Stream received:\" + stream_received.stream_id)\ntopic_consumer.on_stream_received = on_stream_received_handler\n</code></pre> <pre><code>topicConsumer.OnStreamReceived += (topic, newStream) =&gt;\n{\nConsole.WriteLine($\"New stream received: {newStream.StreamId}\");\n};\n</code></pre> <p>output on blue replica:</p> <pre><code>New stream received: stream 4\n</code></pre> <p>When the purple replica has restarted and becomes available again, it signals to the broker and takes control of \"stream 4\". </p> <p></p> <p>This will trigger two events, one in the blue replica indicating that \"stream 4\" has been revoked, and one in the purple replica indicating that \"stream 4\" has been assigned again:</p> PythonC# <pre><code>def on_stream_received_handler(stream_received: StreamConsumer):\nprint(\"Stream received:\" + stream_received.stream_id)\ndef on_streams_revoked_handler(topic_consumer: TopicConsumer, streams_revoked: [StreamConsumer]):\nfor stream in streams_revoked:\nprint(\"Stream revoked:\" + stream.stream_id)\ntopic_consumer.on_stream_received = on_stream_received_handler\ntopic_consumer.on_streams_revoked = on_streams_revoked_handler\n</code></pre> <pre><code>topicConsumer.OnStreamReceived += (topic, newStream) =&gt;\n{\nConsole.WriteLine($\"New stream received: {newStream.StreamId}\");\n};\ntopicConsumer.OnStreamsRevoked += (topic, streamsRevoked) =&gt;\n{\nforeach (var stream in streamsRevoked)\n{\nConsole.WriteLine($\"Stream revoked: {stream.StreamId}\");\n}\n};\n</code></pre> <p>Output on the blue replica:</p> <pre><code>Stream revoked: stream 4\n</code></pre> <p>Output on the purple replica:</p> <pre><code>New stream received: stream 4\n</code></pre> <p>The same behavior will happen if we scale the \"Process\" deployment up or down, increasing or decreasing the number of replicas. Kafka will trigger the rebalancing mechanism internally and this will trigger the same events on Quix Streams. Note that this example assumes perfect conditions, but in reality a rebalance event can shift all streams to different processes. In the library we ensured you only get revocation raised for a stream if it is not assigned back to the same consumer, not while it is rebalancing.</p>"},{"location":"client-library/features/horizontal-scaling/#rebalancing-mechanism-and-partitions","title":"Rebalancing mechanism and partitions","text":"<p>Kafka uses partitions and the RangeAssignor strategy to decide which consumers receive which messages. </p> <p>Partitions and the Kafka rebalancing protocol are internal details of the Kafka implementation behind Quix Streams. You don\u2019t need to worry about them because everything is abstracted within the Streaming Context feature of the library. The events described above will remain the same, even if Quix Streams uses another message broker technology or another rebalancing mechanism in the future.</p> <p>Warning</p> <p>Because of how the Kafka rebalancing mechanism works, you should follow one golden rule: you should not have more replicas than the number of partitions the topic you're subscribing to has, as additional replicas will idle.</p>"},{"location":"client-library/features/message-splitting/","title":"Message splitting","text":"<p>Message brokers have message size limitation by design to ensure performance. For example, Kafka has a 1MB limit by default, and while you can increase it, it will have performance implications and has an upper suggested limit of 10 MB. This can be a problem in some use cases where messages published are big such as sound, video binary chunks or just huge volumes of time-series data.</p> <p>Quix Streams automatically handles large messages on the producer side, splitting them up if required and merging them back together at the consumer side, in a totally transparent way.</p> <p></p> <p>This feature gives you currently 255 times the broker message size limit, independent of the message broker used. This can be useful for intermittent messages which exceed the limit of your broker without getting exceptions.</p> <p>Warning</p> <p>While this feature is undeniably useful, effort should be made to stay within your broker's limit to avoid complex commit limitations and increased memory footprint in the consumer. If you consistently have huge messages, should consider uploading to external storage and only sending a reference using the broker.</p>"},{"location":"client-library/features/multiple-data-types/","title":"Multiple data types","text":"<p>Quix Streams lets you attach any type of data \u2014 Numbers, Strings or even Binary data \u2014 to your timestamps.</p> <p>For example, with the library you can send telemetry data from your vehicle or IoT device and attach a picture or video frame to the same timestamp.</p> <p></p> <p>This gives Quix Streams the ability to adapt to any streaming application use case, from plain telemetry data to video streaming, or a mix of both.</p>"},{"location":"client-library/features/streaming-context/","title":"Streaming context","text":"<p>Broker client libraries allow you to send messages that can contain anything, using binary content. This is what they're designed for and Quix Streams is built on such library also. The problem with these messages is that by default they don't have any relation to each other and we found that is something streaming frequently needs.</p> <p>Sending messages using a broker client library would look like this:</p> <p></p> <p>Quix Streams creates a stream context for you to group all data for a source. Among other things, this enables automatic horizontal scaling of your models when you deal with multiple data sources.</p> <p></p> <p>This context simplifies processing streams by providing callbacks on the subscribing side. You can keep working with each context (stream) separately or together, depending on your needs.</p> <p>The library also allows you to attach metadata to streams, like ids, location, references, time or any other type of information related to the data source.</p> <p></p> <p>This metadata can be read by the library real time or browsed in our SaaS, if you choose to persist the topic streams.</p>"}]}